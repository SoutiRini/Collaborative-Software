[{"count": 0, "tags": [], "bug_id": 55144, "attachment_id": null, "text": "I'm using Atmoshphere framework and when I try to resume a resource it locks (can be for hours)\n\nI'm not sure how to reproduce since it's happening on our prod servers.\nUsing \nwindows server 2008 standard x64 R2, \ntomcat 7.0.41, \njava jrockit-jdk1.6.0_37-R28.2.5-4.1.0\n\nThis was happening since version 7.0.30 (that's the one we started using)\n\nsee dump:\n\"CleanResourceTask\" id=79 idx=0x14c tid=228160 prio=5 alive, parked, native_blocked, daemon\n    at jrockit/vm/Locks.park0(J)V(Native Method)\n    at jrockit/vm/Locks.park(Locks.java:2230)[optimized]\n    at jrockit/proxy/sun/misc/Unsafe.park(Unsafe.java:616)[inlined]\n    at java/util/concurrent/locks/LockSupport.parkNanos(LockSupport.java:196)[inlined]\n    at java/util/concurrent/locks/AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1011)[inlined]\n    at java/util/concurrent/locks/AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1303)[inlined]\n    at java/util/concurrent/CountDownLatch.await(CountDownLatch.java:253)[inlined]\n    at org/apache/tomcat/util/net/NioEndpoint$KeyAttachment.awaitLatch(NioEndpoint.java:1577)[inlined]\n    at org/apache/tomcat/util/net/NioEndpoint$KeyAttachment.awaitWriteLatch(NioEndpoint.java:1580)[inlined]\n    at org/apache/tomcat/util/net/NioBlockingSelector.write(NioBlockingSelector.java:109)[optimized]\n    at org/apache/tomcat/util/net/NioSelectorPool.write(NioSelectorPool.java:174)[optimized]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.writeToSocket(InternalNioOutputBuffer.java:163)[inlined]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.flushBuffer(InternalNioOutputBuffer.java:242)[optimized]\n    ^-- Holding lock: org/apache/coyote/http11/InternalNioOutputBuffer@0x0000000222F4BE70[fat lock]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.endRequest(InternalNioOutputBuffer.java:121)\n    at org/apache/coyote/http11/AbstractHttp11Processor.action(AbstractHttp11Processor.java:752)[optimized]\n    at org/apache/coyote/Response.action(Response.java:173)[inlined]\n    at org/apache/coyote/Response.finish(Response.java:279)[inlined]\n    at org/apache/catalina/connector/OutputBuffer.close(OutputBuffer.java:313)[optimized]\n    at org/apache/catalina/connector/Response.finishResponse(Response.java:512)\n    at org/apache/catalina/connector/CometEventImpl.close(CometEventImpl.java:96)\n    at org/atmosphere/container/Tomcat7CometSupport.bz51881(Tomcat7CometSupport.java:178)[optimized]\n    at org/atmosphere/container/Tomcat7CometSupport.action(Tomcat7CometSupport.java:209)\n    at org/atmosphere/container/Tomcat7CometSupport.action(Tomcat7CometSupport.java:47)\n    at org/atmosphere/cpr/AtmosphereResourceImpl.resume(AtmosphereResourceImpl.java:323)[optimized]\n    at com/fx/tasks/CleanResourceTask.resumeOldResources(CleanResourceTask.java:97)[inlined]\n    at com/fx/tasks/CleanResourceTask.access$200(CleanResourceTask.java:26)[optimized]\n    at com/fx/tasks/CleanResourceTask$1.run(CleanResourceTask.java:51)\n    at java/lang/Thread.run(Thread.java:662)\n    at jrockit/vm/RNI.c2java(JJJJJ)V(Native Method)\n    -- end of trace", "id": 168060, "time": "2013-06-26T06:39:56Z", "creator": "pavel.rekun@gmail.com", "creation_time": "2013-06-26T06:39:56Z", "is_private": false}, {"count": 1, "tags": [], "creator": "markt@apache.org", "attachment_id": null, "text": "I'm leaning towards resolving this as invalid. Tomcat is waiting to write some data and is waiting for the configured timeout (not sure if Atmosphere or the app sets the timeout) to send the data. If Tomcat is waiting for hours that is because of the timeout that has been configured.\n\nI don't see any evidence in this report of incorrect behaviour by Tomcat.", "id": 168083, "time": "2013-06-26T20:24:38Z", "bug_id": 55144, "creation_time": "2013-06-26T20:24:38Z", "is_private": false}, {"count": 2, "tags": [], "text": "here is my connection config:\n\n\t<Connector address=\"1.1.1.1\" connectionTimeout=\"20000\" port=\"80\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\" redirectPort=\"8443\"\n\tmaxThreads=\"500\"\n\tacceptCount=\"100\"\n\tacceptorThreadCount=\"5\"\n\tcompression=\"on\"\n  compressionMinSize=\"0\"\n  noCompressionUserAgents=\"gozilla, traviata\"\n  compressableMimeType=\"text/html,text/plain,text/xml,text/css,text/javascript,application/javascript,application/x-javascript,application/json\"\n\t/>\n\nAs you can see it has a timeout of 20 sec - so I'm not sure how it can lock for hours.", "attachment_id": null, "id": 168085, "creator": "pavel.rekun@gmail.com", "time": "2013-06-26T20:39:50Z", "bug_id": 55144, "creation_time": "2013-06-26T20:39:50Z", "is_private": false}, {"count": 3, "tags": [], "creator": "markt@apache.org", "attachment_id": null, "text": "Any way you can debug the instance to see that the timeout has been set to?\n\nI'll have another look at the code and see if I can spot any possible code paths that could lead to a much longer delay.\n\nIs it possible that Atmosphere is changing the timeout?\n\nIs it possible that the associated client has gone away?", "id": 168104, "time": "2013-06-27T20:09:06Z", "bug_id": 55144, "creation_time": "2013-06-27T20:09:06Z", "is_private": false}, {"count": 4, "tags": [], "bug_id": 55144, "attachment_id": null, "id": 168140, "time": "2013-06-30T07:32:10Z", "creator": "pavel.rekun@gmail.com", "creation_time": "2013-06-30T07:32:10Z", "is_private": false, "text": "I debugged and I don't see that Atmosphere rewrites the timeout.\n\nAs to the client going away - how do I check this? That's what I was suspecting at the beginning but I'm not sure how to reproduce. \n\nThe whole point of the \"CleanResourceTask\"  is to release connections that are active for more than 30 minutes, cause for some reason it wasn't able to detect a disconnect event."}, {"count": 5, "tags": [], "bug_id": 55144, "text": "It is good to know that Atmosphere doesn't appear to change the timeout but the question I asked was for one of these blocked threads can you see what the timeout has actually been set to? If the timeout is huge, we need to find out where it is set. If the timeout is normal and the thread blocks for longer than the timeout then that points to a JVM bug.", "id": 168475, "time": "2013-07-10T20:28:49Z", "creator": "markt@apache.org", "creation_time": "2013-07-10T20:28:49Z", "is_private": false, "attachment_id": null}, {"count": 6, "attachment_id": null, "creator": "pavel.rekun@gmail.com", "text": "Missed that question :)\n\nI'm not sure how can I check the timeout for one of the locked threads... (I can't reproduce it locally)\n\nI did check the timeout for atmosphere threads and it looks normal (uses the timeout I've set in tomcat conf)\n\nDo you have a temp workaround in the meanwhile ? It causes tomcat to crash (keeps opening more and more threads)", "id": 168517, "time": "2013-07-11T17:17:50Z", "bug_id": 55144, "creation_time": "2013-07-11T17:17:50Z", "tags": [], "is_private": false}, {"count": 7, "tags": [], "creator": "pavel.rekun@gmail.com", "attachment_id": null, "text": "See this new dump - I'm also seeing gzip in the stacktrace - maybe it's contributing to the problem.\n\n\"CleanResourceTask\" id=81 idx=0x154 tid=220556 prio=5 alive, interrupted, blocked, native_blocked, daemon\n    -- Blocked trying to get lock: org/apache/coyote/http11/InternalNioOutputBuffer@0x00000001D9CAC6E8[thin lock]\n    at jrockit/vm/Threads.sleep(I)V(Native Method)\n    at jrockit/vm/Locks.waitForThinRelease(Locks.java:955)[optimized]\n    at jrockit/vm/Locks.monitorEnterSecondStageHard(Locks.java:1083)[optimized]\n    at jrockit/vm/Locks.monitorEnterSecondStage(Locks.java:1005)[optimized]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.addToBB(InternalNioOutputBuffer.java:208)[inlined]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.access$000(InternalNioOutputBuffer.java:41)[inlined]\n    at org/apache/coyote/http11/InternalNioOutputBuffer$SocketOutputBuffer.doWrite(InternalNioOutputBuffer.java:268)[optimized]\n    at org/apache/coyote/http11/filters/ChunkedOutputFilter.doWrite(ChunkedOutputFilter.java:117)[optimized]\n    at org/apache/coyote/http11/filters/GzipOutputFilter$FakeOutputStream.write(GzipOutputFilter.java:177)[optimized]\n    at java/util/zip/GZIPOutputStream.finish(GZIPOutputStream.java:91)\n    at org/apache/coyote/http11/filters/FlushableGZIPOutputStream.finish(FlushableGZIPOutputStream.java:84)\n    ^-- Holding lock: org/apache/coyote/http11/filters/FlushableGZIPOutputStream@0x00000001D9CF1698[thin lock]\n    at org/apache/coyote/http11/filters/GzipOutputFilter.end(GzipOutputFilter.java:141)\n    at org/apache/coyote/http11/AbstractOutputBuffer.endRequest(AbstractOutputBuffer.java:310)\n    at org/apache/coyote/http11/InternalNioOutputBuffer.endRequest(InternalNioOutputBuffer.java:120)\n    at org/apache/coyote/http11/AbstractHttp11Processor.action(AbstractHttp11Processor.java:752)[optimized]\n    at org/apache/coyote/Response.action(Response.java:173)[inlined]\n    at org/apache/coyote/Response.finish(Response.java:279)[inlined]\n    at org/apache/catalina/connector/OutputBuffer.close(OutputBuffer.java:313)[optimized]\n    at org/apache/catalina/connector/Response.finishResponse(Response.java:512)\n    at org/apache/catalina/connector/CometEventImpl.close(CometEventImpl.java:96)\n    at org/atmosphere/container/Tomcat7CometSupport.bz51881(Tomcat7CometSupport.java:178)[optimized]\n    at org/atmosphere/container/Tomcat7CometSupport.action(Tomcat7CometSupport.java:209)\n    at org/atmosphere/container/Tomcat7CometSupport.action(Tomcat7CometSupport.java:47)\n    at org/atmosphere/cpr/AtmosphereResourceImpl.resume(AtmosphereResourceImpl.java:323)[optimized]\n    at com/fx/tasks/CleanResourceTask.resumeOldResources(CleanResourceTask.java:124)[optimized]\n    at com/fx/tasks/CleanResourceTask.access$400(CleanResourceTask.java:26)\n    at com/fx/tasks/CleanResourceTask$2.run(CleanResourceTask.java:78)\n    at java/lang/Thread.run(Thread.java:662)\n    at jrockit/vm/RNI.c2java(JJJJJ)V(Native Method)\n    -- end of trace\n\n\"Atmosphere-Shared-AsyncOp-593\" id=1499 idx=0x1738 tid=209140 prio=5 alive, parked, native_blocked, daemon\n    at jrockit/vm/Locks.park0(J)V(Native Method)\n    at jrockit/vm/Locks.park(Locks.java:2230)[optimized]\n    at jrockit/proxy/sun/misc/Unsafe.park(Unsafe.java:616)[inlined]\n    at java/util/concurrent/locks/LockSupport.parkNanos(LockSupport.java:196)[inlined]\n    at java/util/concurrent/locks/AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1011)[inlined]\n    at java/util/concurrent/locks/AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1303)[inlined]\n    at java/util/concurrent/CountDownLatch.await(CountDownLatch.java:253)[inlined]\n    at org/apache/tomcat/util/net/NioEndpoint$KeyAttachment.awaitLatch(NioEndpoint.java:1577)[inlined]\n    at org/apache/tomcat/util/net/NioEndpoint$KeyAttachment.awaitWriteLatch(NioEndpoint.java:1580)[inlined]\n    at org/apache/tomcat/util/net/NioBlockingSelector.write(NioBlockingSelector.java:109)[optimized]\n    at org/apache/tomcat/util/net/NioSelectorPool.write(NioSelectorPool.java:174)[optimized]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.writeToSocket(InternalNioOutputBuffer.java:163)[inlined]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.flushBuffer(InternalNioOutputBuffer.java:242)[inlined]\n    at org/apache/coyote/http11/InternalNioOutputBuffer.flush(InternalNioOutputBuffer.java:94)[optimized]\n    ^-- Holding lock: org/apache/coyote/http11/InternalNioOutputBuffer@0x00000001D9CAC6E8[thin lock]\n    at org/apache/coyote/http11/AbstractHttp11Processor.action(AbstractHttp11Processor.java:793)[optimized]\n    at org/apache/coyote/Response.action(Response.java:173)[inlined]\n    at org/apache/catalina/connector/OutputBuffer.doFlush(OutputBuffer.java:359)[inlined]\n    at org/apache/catalina/connector/OutputBuffer.flush(OutputBuffer.java:326)[inlined]\n    at org/apache/catalina/connector/CoyoteOutputStream.flush(CoyoteOutputStream.java:101)[optimized]\n    at org/atmosphere/handler/AbstractReflectorAtmosphereHandler.onStateChange(AbstractReflectorAtmosphereHandler.java:134)[optimized]\n    at org/atmosphere/cpr/DefaultBroadcaster.invokeOnStateChange(DefaultBroadcaster.java:1093)[inlined]\n    at org/atmosphere/cpr/DefaultBroadcaster.prepareInvokeOnStateChange(DefaultBroadcaster.java:1113)[optimized]\n    at org/atmosphere/cpr/DefaultBroadcaster.executeAsyncWrite(DefaultBroadcaster.java:998)[optimized]\n    at org/atmosphere/cpr/DefaultBroadcaster$3.run(DefaultBroadcaster.java:619)[optimized]\n    ^-- Holding lock: org/atmosphere/cpr/AtmosphereResourceImpl@0x00000001D9CA8AA0[thin lock]\n    at java/util/concurrent/Executors$RunnableAdapter.call(Executors.java:439)[optimized]\n    at java/util/concurrent/FutureTask$Sync.innerRun(FutureTask.java:303)[inlined]\n    at java/util/concurrent/FutureTask.run(FutureTask.java:138)[optimized]\n    at java/util/concurrent/ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)[optimized]\n    at java/util/concurrent/ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n    at java/lang/Thread.run(Thread.java:662)[optimized]\n    at jrockit/vm/RNI.c2java(JJJJJ)V(Native Method)\n    -- end of trace", "id": 168521, "time": "2013-07-11T19:04:00Z", "bug_id": 55144, "creation_time": "2013-07-11T19:04:00Z", "is_private": false}, {"count": 8, "attachment_id": null, "creator": "markt@apache.org", "text": "If you can take a thread dump then you should be able to take a heap dump as well. You can then load the heap dump into a profiler and take a look at the current value of the timeout. Knowing the timeout value is key to figuring out where to go next with this.", "id": 168522, "time": "2013-07-11T19:06:38Z", "bug_id": 55144, "creation_time": "2013-07-11T19:06:38Z", "tags": [], "is_private": false}, {"count": 9, "attachment_id": null, "creator": "pavel.rekun@gmail.com", "text": "I did a heap dump - and discovered 2 things:\n\n1. timeout is normal (setting to 30 minutes in atmosphere and it's the same in the locked thread)\n\n2. the nio connection is an HTTPS\n\nI haven't tested https in my local tests - I'll test it and see if I can reproduce.\n\nAny idea why https would cause an issue?", "id": 168541, "time": "2013-07-12T07:19:29Z", "bug_id": 55144, "creation_time": "2013-07-12T07:19:29Z", "tags": [], "is_private": false}, {"count": 10, "tags": [], "bug_id": 55144, "attachment_id": null, "text": "Next question - are those threads blocking for more than 30 minutes after the write attempt? If yes, that is points to a JVM bug. If not, then everything is working as designed.\n\nGiven the above, I don't see a Tomcat bug here so I am resolving this as INVALID. Feel free to continue this discussion on the users list. If your investigation does discover a bug (e.g. Tomcat fails to act on a something (e.g. an exception) that should result in the socket being closed) then feel free to re-open this issue.\n\nI'd suggest that the CleanResourceTask sets the timeout a lot lower (e.g. 20s) before trying to write to the socket. While 30mins is OK for a read timeout it is way too high for a blocking write.", "id": 168550, "time": "2013-07-12T09:05:51Z", "creator": "markt@apache.org", "creation_time": "2013-07-12T09:05:51Z", "is_private": false}, {"count": 11, "tags": [], "creator": "pavel.rekun@gmail.com", "attachment_id": null, "text": "I've discovered a strange behavior - even though I set a timeout of 20 sec on the connector (in server.xml), sometimes it gets here:\n\natt.awaitWriteLatch(writeTimeout,TimeUnit.MILLISECONDS);\n\nWith the timeout I've set for the suspend resource (resource.suspend(30 minutes))\nand sometime with the 20 sec timeouts.\n\nYou can see my discussion here with the Jean (the creator of Atmosphere) and he says that atmosphere doesn't have access to that API)\n\nhttps://github.com/Atmosphere/atmosphere/issues/1148\n\nIs this a tomcat bug?", "id": 168600, "time": "2013-07-15T07:48:23Z", "bug_id": 55144, "creation_time": "2013-07-15T07:48:23Z", "is_private": false}, {"count": 12, "tags": [], "text": "Something is setting the timeout to 30 minutes and that something isn't Tomcat. \n\nIf Tomcat was using the wrong / old timeout then that would be a Tomcat bug but I don't see any evidence of that here.", "attachment_id": null, "id": 168604, "creator": "markt@apache.org", "time": "2013-07-15T12:07:30Z", "bug_id": 55144, "creation_time": "2013-07-15T12:07:30Z", "is_private": false}, {"count": 13, "tags": [], "bug_id": 55144, "text": "Well, this actually WAS a valid bug - it got re-filed and fixed as https://issues.apache.org/bugzilla/show_bug.cgi?id=55267\nI'm just wondering why they're not linked together.", "id": 180425, "time": "2015-01-20T22:22:26Z", "creator": "vkleinschmidt@blackboard.com", "creation_time": "2015-01-20T22:22:26Z", "is_private": false, "attachment_id": null}]