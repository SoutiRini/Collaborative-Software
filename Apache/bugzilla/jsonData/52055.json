[{"count": 0, "tags": [], "creator": "rfeng@apache.org", "attachment_id": null, "is_private": false, "id": 150741, "time": "2011-10-18T21:53:21Z", "bug_id": 52055, "creation_time": "2011-10-18T21:53:21Z", "text": "I'm using the standard servlet 3.0 async APIs with tomcat 7.0.22. The server side code is something like the following:\n\n        final AsyncContext asyncContext = request.startAsync();\n\n        ... // in a processing thread\n        asyncContext.getRequest().getInputStream();\n        ... read the input stream\n        asyncContext.complete();\n\nThe InputStream returns 0 bytes for the HTTP post with chunking. \n\nAfter debugging, I found that the ChunkedInputFilter is reused by org.apache.coyote.http11.AbstractInputBuffer. But it has never been recycled (nextRequest()?) before the reuse for another request. As a result, the endChunk flag is always true after the first request. And it always return immediately without reading more from the buffer."}, {"count": 1, "tags": [], "bug_id": 52055, "text": "This is an application error, not a Tomcat bug. I have confirmed that this works as expected. The users mailing list is the place to seek further help.", "id": 150764, "time": "2011-10-19T16:21:02Z", "creator": "markt@apache.org", "creation_time": "2011-10-19T16:21:02Z", "is_private": false, "attachment_id": null}, {"count": 2, "tags": [], "bug_id": 52055, "attachment_id": 27821, "is_private": false, "id": 150770, "time": "2011-10-19T19:05:06Z", "creator": "rfeng@apache.org", "creation_time": "2011-10-19T19:05:06Z", "text": "Created attachment 27821\nTest case\n\nThis the war file that contains a simple servlet filter that uses servelet 3.0 async api to echo the posted content. \n\npackage test;\n\nimport java.io.IOException;\nimport java.io.Reader;\n\nimport javax.servlet.AsyncContext;\nimport javax.servlet.Filter;\nimport javax.servlet.FilterChain;\nimport javax.servlet.FilterConfig;\nimport javax.servlet.ServletException;\nimport javax.servlet.ServletRequest;\nimport javax.servlet.ServletResponse;\nimport javax.servlet.http.HttpServletRequest;\n\npublic class TestAsyncFilter implements Filter {\n\n    @Override\n    public void destroy() {\n\n    }\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException,\n        ServletException {\n        HttpServletRequest httpServletRequest = (HttpServletRequest)request;\n        String path = httpServletRequest.getServletPath();\n        if (\"POST\".equalsIgnoreCase(httpServletRequest.getMethod()) && path.startsWith(\"/test\")) {\n            final AsyncContext asyncContext = request.startAsync();\n            asyncContext.start(new Runnable() {\n\n                @Override\n                public void run() {\n                    try {\n                        readContent(asyncContext);\n                    } catch (IOException e) {\n                        e.printStackTrace();\n                    }\n                }\n            });\n\n        } else {\n            chain.doFilter(request, response);\n        }\n    }\n\n    private void readContent(AsyncContext asyncContext) throws IOException {\n        HttpServletRequest httpServletRequest = (HttpServletRequest)asyncContext.getRequest();\n        Reader reader = httpServletRequest.getReader();\n        StringBuilder sb = new StringBuilder();\n        char[] buf = new char[4096];\n        while (true) {\n            int len = reader.read(buf);\n            if (len < 0) {\n                break;\n            } else {\n                sb.append(buf, 0, len);\n            }\n        }\n        System.out.println(\"Request: \" + sb.toString());\n        asyncContext.getResponse().getWriter().println(sb.toString());\n        asyncContext.complete();\n    }\n\n    @Override\n    public void init(FilterConfig config) throws ServletException {\n\n    }\n\n}\n\n\nAnd the test client is:\n\npackage test;\n\nimport java.io.IOException;\n\nimport org.apache.http.HttpResponse;\nimport org.apache.http.client.HttpClient;\nimport org.apache.http.client.methods.HttpPost;\nimport org.apache.http.entity.StringEntity;\nimport org.apache.http.impl.client.DefaultHttpClient;\nimport org.apache.http.util.EntityUtils;\n\npublic class TestHttpClient {\n    public static void main(String[] args) throws IOException {\n        HttpClient client = new DefaultHttpClient();\n        for (int i = 0; i < 5; i++) {\n            HttpPost post = new HttpPost(\"http://localhost:8080/tomcat7-async/test\");\n            StringEntity entity = new StringEntity(\"Test String: \" + i);\n            entity.setChunked(true);\n            post.setEntity(entity);\n            HttpResponse response = client.execute(post);\n            System.out.println(\"Response[\" + i + \"]: \" + EntityUtils.toString(response.getEntity()));\n        }\n    }\n}"}, {"count": 3, "tags": [], "bug_id": 52055, "text": "I would like to reopen the bug with the test case to prove I was wrong :-).\n\nThanks,\nRaymond", "id": 150771, "time": "2011-10-19T19:06:17Z", "creator": "rfeng@apache.org", "creation_time": "2011-10-19T19:06:17Z", "is_private": false, "attachment_id": null}, {"count": 4, "tags": [], "bug_id": 52055, "attachment_id": null, "text": "To reproduce the problem, deploy the WAR and run the test.TestHttpClient to send chunked post.\n\nBTW, the same application works well under Jetty 8.x. \n\nI also tried to convert the filter to a async servlet and the same behavior happened. Only the first request was processed correctly and other ones received empty content.", "id": 150773, "time": "2011-10-19T20:29:45Z", "creator": "rfeng@apache.org", "creation_time": "2011-10-19T20:29:45Z", "is_private": false}, {"count": 5, "text": "Yep, I can reproduce this now with the additional information. Previously, it looked like the standard getParameter() confusion.", "bug_id": 52055, "is_private": false, "id": 150774, "time": "2011-10-19T20:33:09Z", "creator": "markt@apache.org", "creation_time": "2011-10-19T20:33:09Z", "tags": [], "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 52055, "text": "Thanks for the test case. That made tracking this down a lot easier than it could have been.\n\nThe problem has been fixed in trunk (8.0.x) and 7.0.x and will be included in 7.0.23 onwards.", "id": 150776, "time": "2011-10-19T20:56:29Z", "creator": "markt@apache.org", "creation_time": "2011-10-19T20:56:29Z", "is_private": false, "attachment_id": null}, {"count": 7, "tags": [], "creator": "sudhan.moghe@gmail.com", "attachment_id": null, "text": "I am still facing this issue. My test cases are failing randomly when I run my application with <async-supported>true</async-supported>.\nEverything work fine when client is not using chunked encoding. \nAlso, everything work fine when run with <async-supported>false</async-supported>\n\nI have debugged the problem and this bug report helped me in finding root cause. I found that ChunkedInputFilter is getting reused even when keepalive=false. And my test cases start failing after that point.\nSo, we need to recycle it in that case. \n\nIssue was fixed by modifying the AbstractHttp11Processor file.\nReplaced lines #1538 to #1544\n          \n            if (!keepAlive) {\n                return SocketState.CLOSED;\n            } else {\n                getInputBuffer().nextRequest();\n                getOutputBuffer().nextRequest();\n                return SocketState.OPEN;\n            }\nWith\n            getInputBuffer().nextRequest();\n            if (!keepAlive) {\n                return SocketState.CLOSED;\n            } else {\n                getOutputBuffer().nextRequest();\n                return SocketState.OPEN;\n            }\n\nI was testing against 7.0.26. Will test with trunk and submit patch if required.\nWe can not recycle output buffer at this stage as it has not completed flushing data.\nLet me know if there is a better place to handle this. Will test it and then post a patch.\n\nI don't have a simple test case to reproduce the issue.\nLet me know what else I can do to help you fix this issue. \n\nThanks,\nSudhan", "id": 158224, "time": "2012-04-20T14:01:42Z", "bug_id": 52055, "creation_time": "2012-04-20T14:01:42Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 52055, "attachment_id": null, "text": "Thanks for the research. That pointed me in the right direction. This was fixed in r1334787 for trunk and r1334790 for 7.0.x and will be included in 7.0.28 onwards.\n\nIf you are able to confirm that the fix works, that would be great.", "id": 158864, "time": "2012-05-06T21:34:39Z", "creator": "markt@apache.org", "creation_time": "2012-05-06T21:34:39Z", "is_private": false}, {"count": 9, "tags": [], "bug_id": 52055, "attachment_id": null, "is_private": false, "id": 158902, "time": "2012-05-08T07:52:34Z", "creator": "sudhan.moghe@gmail.com", "creation_time": "2012-05-08T07:52:34Z", "text": "Yes, It is fixed.\nI have tested in 7.0.x. It is working fine.\nI was getting some NPE's in InternalNioInputBuffer with my fix. That's why did not post patch.\nNot getting any exception with latest code from 7.0.x.\n\nThanks,\nSudhan"}, {"count": 10, "tags": [], "bug_id": 52055, "attachment_id": null, "is_private": false, "id": 159254, "time": "2012-05-23T14:16:55Z", "creator": "sudhan.moghe@gmail.com", "creation_time": "2012-05-23T14:16:55Z", "text": "Got this issue again. :(\nEarlier I was running with protocol=\"HTTP/1.1\"\nThis time I got error with protocol=\"org.apache.coyote.http11.Http11NioProtocol\"\n\nShall I reopen?\n\nI was running single client. That is one request at a time.\nGot exception in client and client closed socket.\nGot following exception is server.\n\njava.io.EOFException: Unexpected EOF read on the socket\n\tat org.apache.coyote.http11.InternalNioInputBuffer.readSocket(InternalNioInputBuffer.java:455)\n\tat org.apache.coyote.http11.InternalNioInputBuffer.fill(InternalNioInputBuffer.java:808)\n\tat org.apache.coyote.http11.InternalNioInputBuffer$SocketInputBuffer.doRead(InternalNioInputBuffer.java:833)\n\tat org.apache.coyote.http11.filters.ChunkedInputFilter.readBytes(ChunkedInputFilter.java:271)\n\tat org.apache.coyote.http11.filters.ChunkedInputFilter.parseCRLF(ChunkedInputFilter.java:355)\n\tat org.apache.coyote.http11.filters.ChunkedInputFilter.doRead(ChunkedInputFilter.java:147)\n\tat org.apache.coyote.http11.AbstractInputBuffer.doRead(AbstractInputBuffer.java:346)\n\tat org.apache.coyote.Request.doRead(Request.java:422)\n\tat org.apache.catalina.connector.InputBuffer.realReadBytes(InputBuffer.java:290)\n\tat org.apache.tomcat.util.buf.ByteChunk.substract(ByteChunk.java:431)\n\tat org.apache.catalina.connector.InputBuffer.read(InputBuffer.java:315)\n\tat org.apache.catalina.connector.CoyoteInputStream.read(CoyoteInputStream.java:200)\n\nThis is expected. But looks like filters were not recycled before next request.\nNext request failed with\n\njava.io.IOException: Invalid CRLF\n\tat org.apache.coyote.http11.filters.ChunkedInputFilter.parseCRLF(ChunkedInputFilter.java:366)\n\tat org.apache.coyote.http11.filters.ChunkedInputFilter.doRead(ChunkedInputFilter.java:147)\n\tat org.apache.coyote.http11.AbstractInputBuffer.doRead(AbstractInputBuffer.java:346)\n\tat org.apache.coyote.Request.doRead(Request.java:422)\n\tat org.apache.catalina.connector.InputBuffer.realReadBytes(InputBuffer.java:290)\n\tat org.apache.tomcat.util.buf.ByteChunk.substract(ByteChunk.java:431)\n\tat org.apache.catalina.connector.InputBuffer.read(InputBuffer.java:315)\n\tat org.apache.catalina.connector.CoyoteInputStream.read(CoyoteInputStream.java:200)"}, {"count": 11, "tags": [], "bug_id": 52055, "text": "Re-open to investigate", "id": 159268, "time": "2012-05-23T21:21:24Z", "creator": "markt@apache.org", "creation_time": "2012-05-23T21:21:24Z", "is_private": false, "attachment_id": null}, {"count": 12, "text": "Full stack traces please.", "bug_id": 52055, "is_private": false, "id": 159270, "time": "2012-05-23T21:24:13Z", "creator": "markt@apache.org", "creation_time": "2012-05-23T21:24:13Z", "tags": [], "attachment_id": null}, {"count": 13, "tags": [], "bug_id": 52055, "text": "Rest of the stack trace is my application code.\nI am starting async processing with\n\nif (request.isAsyncSupported()) {\n\tfinal AsyncContext asyncCtx = request.startAsync(request, response);\n\tfinal Runnable command = new AsyncRequestProcessor(asyncCtx, this, this.actionObjects);\n\tExecutor executor = (Executor)request.getServletContext().getAttribute(Constants.ASYNC_REQUEST_EXECUTOR);\n\texecutor.execute(command);\n}\n\nNext line on stack trace is \nServletUtils.getBytesOfStream(ServletUtils.java:425)\n\nCode of same is\npublic static ByteArrayInputStreamExt getBytesOfStream(final Map<String, Object> helperMap, final InputStream stream, int len, boolean chunked) throws IOException {\n\ttry {\n\t\tByteArrayInputStreamExt bais = (ByteArrayInputStreamExt) helperMap.get(Constants.DATA);\n\t\tbyte[] tempBuf = null;\n\t\tif (bais == null) {\n\t\t\ttempBuf = new byte[len];\n\t\t\tbais = new ByteArrayInputStreamExt(tempBuf);\n\t\t} else {\n\t\t\ttempBuf = bais.getBuffer();\n\t\t}\n\t\tint readSoFar = 0;\n\t\tint bytesRead = 0;\n\t\twhile (len > readSoFar) {\n\t\t\tbytesRead = stream.read(tempBuf, readSoFar, (len - readSoFar));\n\t\t\tif (bytesRead == -1) {\n\t\t\t\tif (chunked) {\n\t\t\t\t\tif (readSoFar == 0) {\n\t\t\t\t\t\tbais.reset(0);\n\t\t\t\t\t\treturn bais;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tbais.reset(readSoFar);\n\t\t\t\t\t\treturn bais;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tthrow new IOException(\"Failed to read from inputstream\");\n\t\t\t}\n\t\t\treadSoFar += bytesRead;\n\t\t}\n\t\tbais.reset(len);\n\t\treturn bais;\n\t} catch (OutOfMemoryError th) {\n\t\tlogger.error(\"Creating a byte array of length: \" + len + \" has resulted in OOM error: \" + th.getMessage());\n\t}\n\treturn null;\n}\n\nAnd this method is called with\n\nServletUtils.getBytesOfStream(request.getInputStream(), length);\n\nThis method is called in a loop reading 10MB at a time. For chunked, loop stops with EOF. The ByteArrayInputStreamExt handling is for reusing the byte[].\n\nThis happens only when client aborts.\n\nLet me know if you need anymore info. Which class should I check if I have to debug this?\n\nThanks,\nSudhan", "id": 159282, "time": "2012-05-24T08:43:54Z", "creator": "sudhan.moghe@gmail.com", "creation_time": "2012-05-24T08:43:54Z", "is_private": false, "attachment_id": null}, {"count": 14, "tags": [], "bug_id": 52055, "attachment_id": null, "text": "I think the issue is because of ChunkedInputFilter.needCRLFParse not being reset.\nI think we need to do needCRLFParse = false; in ChunkedInputFilter.recycle().", "id": 159315, "time": "2012-05-25T12:24:56Z", "creator": "sudhan.moghe@gmail.com", "creation_time": "2012-05-25T12:24:56Z", "is_private": false}, {"count": 15, "tags": [], "bug_id": 52055, "attachment_id": null, "is_private": false, "id": 159332, "time": "2012-05-25T20:23:37Z", "creator": "markt@apache.org", "creation_time": "2012-05-25T20:23:37Z", "text": "That looks like the cause to me to. This has been fixed in trunk and 7.0.x and will be included in 7.0.28 onwards."}, {"count": 16, "tags": [], "bug_id": 52055, "text": "(In reply to comment #14)\n\nThank you. Fixed this in 6.0 as well, will be in 6.0.36.", "id": 159636, "time": "2012-06-01T23:07:44Z", "creator": "knst.kolinko@gmail.com", "creation_time": "2012-06-01T23:07:44Z", "is_private": false, "attachment_id": null}, {"count": 17, "tags": [], "creator": "knst.kolinko@gmail.com", "attachment_id": null, "text": "(In reply to comment #14)\n\nThe needCRLFParse fix was backported to 5.5 in r1359748 and will be in 5.5.36.", "id": 162596, "time": "2012-10-06T14:50:06Z", "bug_id": 52055, "creation_time": "2012-10-06T14:50:06Z", "is_private": false}]