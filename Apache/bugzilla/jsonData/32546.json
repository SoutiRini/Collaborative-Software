[{"text": "We deal with Very Large Objects in our archive, and the hard-coded 2k buffer\nsize in org/apache/slide/webdav/method/GetMethod.java was killing read\nperformance. Getting a single chunk of data from our archive is a bit expensive,\nso I implemented a better buffering scheme (with a still hard-coded limit).\nUnified diff is below.\n\nOn my local machine, I went from ~115KB/sec reads to ~9MB/sec reads for large\nobjects.\n\n--- GetMethod.java.~1.55.~\t2004-11-09 10:00:28.000000000 -0500\n+++ GetMethod.java\t2004-12-03 22:13:09.000000000 -0500\n@@ -70,8 +70,11 @@\n     // -------------------------------------------------------------- Constants\n \n \n-    protected final int BUFFER_SIZE = 2048;\n+    /** The default buffer size */\n+    protected final int DEFAULT_BUFFER_SIZE = 2048;\n \n+    /** The maximum buffer size */\n+    protected final int MAX_BUFFER_SIZE = 200 * 1024;\n \n \n     /**\n@@ -393,7 +396,7 @@\n             (resourceInputStream, input);\n \n         // Copy the input stream to the output stream\n-        exception = copyRange(istream, ostream);\n+        exception = copyRange(istream, ostream, resourceInfo.length);\n \n         // Clean up the input and output streams\n         try {\n@@ -544,11 +547,12 @@\n      * @return Exception which occured during processing\n      */\n     private IOException copyRange(InputStream istream,\n-                                  ServletOutputStream ostream) {\n+                                  ServletOutputStream ostream,\n+                                  long resourceLength) {\n \n         // Copy the input stream to the output stream\n         IOException exception = null;\n-        byte buffer[] = new byte[input];\n+        byte buffer[] = new byte[getBufferSize(resourceLength)];\n         int len = buffer.length;\n         while (true) {\n             try {\n@@ -591,7 +595,7 @@\n         IOException exception = null;\n         long bytesToRead = end - start + 1;\n \n-        byte buffer[] = new byte[input];\n+        byte buffer[] = new byte[getBufferSize(bytesToRead)];\n         int len = buffer.length;\n         while ( (bytesToRead > 0) && (len >= buffer.length)) {\n             try {\n@@ -615,6 +619,16 @@\n \n     }\n \n+    /**\n+     * Get a reasonable buffer size.\n+     */\n+    private int getBufferSize(long resourceLength)\n+    {\n+        if (resourceLength <= 0)\n+            return DEFAULT_BUFFER_SIZE;\n+\n+        return (int)Math.min(resourceLength, MAX_BUFFER_SIZE);\n+    }\n \n     /**\n      * Parse the range header.", "tags": [], "bug_id": 32546, "is_private": false, "count": 0, "id": 67991, "time": "2004-12-06T20:36:26Z", "creator": "JRRousseau@gmail.com", "creation_time": "2004-12-06T20:36:26Z", "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 32546, "text": "Created attachment 13657\nPatch to improve buffer size allocation", "id": 67992, "attachment_id": 13657, "creator": "JRRousseau@gmail.com", "creation_time": "2004-12-06T20:40:45Z", "time": "2004-12-06T20:40:45Z", "is_private": false}, {"count": 2, "tags": [], "creator": "masonjm@apache.org", "attachment_id": null, "id": 68025, "time": "2004-12-07T06:12:07Z", "bug_id": 32546, "creation_time": "2004-12-07T06:12:07Z", "is_private": false, "text": "John,\n\nThis looks good, and the performance improvements you've mentioned are\nimpressive. I'm concerned that the large maximum buffer size could lead to out\nof memory errors. Perhaps making the maximum size a configurable parameter so\nthat administrators could choose the best speed/memory ratio?\n\nAnother solution that might work for your case (since it sounds like the problem\nis magnified your custom store) is the wrap the InputStream (or Reader) that you\npass to the NodeRevisionContent so that it maintains an internal buffer that's\nlarger than the webdav buffer.\n\n-James"}, {"count": 3, "tags": [], "bug_id": 32546, "attachment_id": null, "is_private": false, "id": 68086, "time": "2004-12-08T03:41:30Z", "creator": "JRRousseau@gmail.com", "creation_time": "2004-12-08T03:41:30Z", "text": "Hi James,\n\nThe performance improvement is definitely impacted by how expensive a single \nread operation is on our distributed archive. I suppose that many clients \npulling down many large files at the same time could use a lot of memory, but \nI wouldn't think it would be an issue for modern machines.\n\nA configurable MAX would definitely be preferable.\n\nThere is also another fixed-size buffer in GetMethod.java which I fixed today, \nbut I don't have the sources handy right now. I'll update the patch tomorrow \nwith the fix for that too if you like.\n\nThanks\n-John\n"}, {"count": 4, "tags": [], "text": "John,\n\nThat would be good :).\n\nAs for getting a parameter, from the SlideToken class you can use\ngetNamespaceConfig().getParameter(\"parameter-name\") to access a parameter\ndefined in the namespace (in Domain.xml). This would be a good place to put the\nbuffer size.\n\n-James", "is_private": false, "id": 68091, "creator": "masonjm@apache.org", "time": "2004-12-08T09:01:59Z", "bug_id": 32546, "creation_time": "2004-12-08T09:01:59Z", "attachment_id": null}, {"attachment_id": 13689, "tags": [], "creator": "JRRousseau@gmail.com", "is_private": false, "count": 5, "id": 68144, "time": "2004-12-08T23:52:57Z", "bug_id": 32546, "creation_time": "2004-12-08T23:52:57Z", "text": "Created attachment 13689\nPatch rev 2 with configurable max buffer size\n\nNow uses a \"max-get-buffer-size\" config param to set the max buffer size.\nDefaults to the old 2k size."}, {"count": 6, "tags": [], "text": "Patch committed.\n\nThanks for making the changes, John.", "is_private": false, "id": 68148, "creator": "masonjm@apache.org", "time": "2004-12-09T03:31:44Z", "bug_id": 32546, "creation_time": "2004-12-09T03:31:44Z", "attachment_id": null}]