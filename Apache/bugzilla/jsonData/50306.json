[{"count": 0, "tags": [], "bug_id": 50306, "attachment_id": null, "text": "Feature request : \nregularly scan worker threads and if one has been processing the same request for longer than a configurable delay, log a warning with the stack trace of that thread.\nThis would allow to detect very long running threads, usually the ones that are stuck in a network call or in a deadlock.", "id": 141861, "time": "2010-11-19T18:09:10Z", "creator": "slaurent@apache.org", "creation_time": "2010-11-19T18:09:10Z", "is_private": false}, {"count": 1, "tags": [], "bug_id": 50306, "attachment_id": null, "text": "(In reply to comment #0)\n> Feature request : \n> regularly scan worker threads and if one has been processing the same request\n> for longer than a configurable delay, log a warning with the stack trace of\n> that thread.\n> This would allow to detect very long running threads, usually the ones that are\n> stuck in a network call or in a deadlock.\n\nThe diagnostics package Rainer & I discussed included something similar to this. It's possible to use JMX to identify if a thread is associated with a request and report the properties of the request.\n\nIt's also possible to enumerate deadlocked threads using JMX.\n\nI've got some sample code which does various diagnostic tasks, but it's very experimental...", "id": 141875, "time": "2010-11-20T15:53:38Z", "creator": "bugzilla@pidster.com", "creation_time": "2010-11-20T15:53:38Z", "is_private": false}, {"count": 2, "tags": [], "creator": "slaurent@apache.org", "text": "What is this diagnostics package ? something in preparation ? Should I wait for it before working on a patch for the stuck thread detection feature ?", "id": 141887, "time": "2010-11-21T15:26:18Z", "bug_id": 50306, "creation_time": "2010-11-21T15:26:18Z", "is_private": false, "attachment_id": null}, {"count": 3, "tags": [], "bug_id": 50306, "attachment_id": null, "text": "All of this info (except the stack trace) is already available via JMX and displayed via the Manager app. There are also plenty of tools to generate stack traces. I'm struggling to see what else could be usefully done here without adding bloat.", "id": 141888, "time": "2010-11-21T15:45:21Z", "creator": "markt@apache.org", "creation_time": "2010-11-21T15:45:21Z", "is_private": false}, {"count": 4, "attachment_id": null, "creator": "rainer.jung@kippdata.de", "text": "I see basically two aspects that code inside Tomcat could add:\n\n1) Extraction of relevant information\n\nSince we know, which thread works on which request and we know a lot about the request meta data (e.g. when it started) and about the threads (e.g. CPU usage), we can very efficiently inspect the stacks for exactly those threads that work on the same request for longer than some configured limit. No need to dump stacks for all threads (a lot of them idle in the pools or connected to the web server but not working on a request).\n\n2) Reduction of polling interval\n\nSince we focus on the relevant info we can do more frequent checks than one would do with a plain \"dump all stacks\". E.g. every 10 seconds instead of every one minute. If there is no long running request, no stack to inspect.\n\nYes, all of that can be done by a JMX client. But the client needs to be powerful enough to add logic like retrieving the in-flight requests, extracting those that already take to long, then retrieving the stacks for those requests etc.\n\nWhat I don't have a nice idea about is how to actually use that information. Of course you could simply log the stacks and the request meta data for the long running requests, but for an average admin this might be to technical (of no real use). Though it would be helpful for his application provider when he starts analyzing reasons for performance problems.", "id": 141919, "time": "2010-11-22T09:13:44Z", "bug_id": 50306, "creation_time": "2010-11-22T09:13:44Z", "tags": [], "is_private": false}, {"count": 5, "tags": [], "bug_id": 50306, "text": "1) The manager webapp already has a list of requests being processed on the \"Server Status\" page, as Mark mentioned.\n\nOne can make them as clickable, with more info displayed on demand.\n\n2) If some \"scan worker thread\" self-diagnostic activity is needed inside Tomcat, one can wrap it into o.a.c.LifecycleListener and add to server.xml. I doubt that there is one-fits-all solution here, but I won't stop anyone contributing.", "id": 141922, "time": "2010-11-22T09:52:30Z", "creator": "knst.kolinko@gmail.com", "creation_time": "2010-11-22T09:52:30Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "text": "My initial idea was : if a request takes too long, log a WARNING with the stack trace of the thread processing the request.\nI think that it is important to have this in log file (push information) than to just allow access by JMX (pull information) because it allows to discover problems pro-actively, for instance a network call that is stuck waiting for data from an unhealthy server and no socket timeout has been set for the call.\n\nAs for the actual implementation, it could indeed be a LifeCycleListener, but it will probably need some changes elsewhere to provide instrumentation.\n\nI'll try to propose a patch in a couple of days.", "attachment_id": null, "id": 141933, "creator": "slaurent@apache.org", "time": "2010-11-22T14:22:05Z", "bug_id": 50306, "creation_time": "2010-11-22T14:22:05Z", "is_private": false}, {"count": 7, "tags": [], "creator": "dirtchamber@hotmail.com", "attachment_id": 26451, "id": 142982, "time": "2010-12-29T05:02:04Z", "bug_id": 50306, "creation_time": "2010-12-29T05:02:04Z", "is_private": false, "text": "Created attachment 26451\nPatch for Tomcat's ThreadPoolExecutor\n\nWebSphere has hung/stuck thread detection logic that has been very useful to us, both in development and production. But since we are moving away from WebSphere, it would be nice to have this kind of feature in Tomcat as well. Some might consider this \"stuck thread detection\" unnessesary bloat, but we have found it useful on numerous occasions. \n\nApplications are never perfect, and often threads will run for too long (resulting in thread leakage and pool depletion):\n-External connections might hang (like URLConnection) [you can configure read/connect timeout, but not all applications has configured this]\n-Loops (can be detected by excessive CPU usage)\n-Loops which does not have excessive CPU usage (for example because it uses external slow IO in each loop iteration, or has Thread.sleep!! (bad coding, but it can happen))\n-Deadlocks.\n-Threads waiting on other hung threads.\n\nI have created an experimental patch for Tomcat 7.0.5 that we have been using in development for a short time now.\n\nThe hung thread monitoring does the following:\nMonitors active (managed) threads, and issues a warning for each thread that has been active for more than the configured threshold (seconds).\nThe message (log.warn) contains the thread name, stack trace, total time active, and total number of suspected hung threads. \n(In our environment, we use Cron to scan system.out, and send email if a hung thread is detected.)\n\nA timer thread runs at a configurable interval and checks and reports hung threads, and threads previously reported to be hung, but have now finished.\n\nIt detects hung threads in the HTTP/AJP thread pools. It does not monitor all JVM threads, but since many user threads are spawned from managed \"webrequest\" threads, the HTTP/AJP threads often waits for user threads to finish, so this patch should give an indication that something is wrong in those cases as well.\n\nIf anyone is interested, I've attached the patch. \nThis patch is just an example of how such monitoring can be implemented. This is not production ready code. Performance impacts have not been considered!\n\nThe patch is for the following class:\n\norg.apache.tomcat.util.threads.ThreadPoolExecutor\n\nIt uses 2 system properties for configuration:\n\norg.apache.catalina.threadmonitor.interval (in seconds. Default 120)\norg.apache.catalina.threadmonitor.threshold (in seconds. Default 600)\n\nThe monitoring is enabled by default, but can be disabled by setting the \"interval\" to a negative value or 0.\n\nAs stated, it only monitors managed threads created by Tomcat's HTTP/AJP thread pools.\n\nMonitoring all JVM threads is also an option, but may require more code and might lead to many false positives..? The most important thing is to detect problems with stuck HTTP/AJP threads.\n\nTODOs:\n\n-Should the thread monitor properties be moved to server.xml instead of being System properties?\n-Add JMX notification when hung thread detected/hung thread finished\n\n\nI have no problem understanding that such a feature might not be seen as useful for everybody. We will continue to patch our own version of Tomcat anyway :)"}, {"count": 8, "tags": [], "creator": "bugzilla@pidster.com", "attachment_id": null, "id": 142989, "time": "2010-12-29T16:47:42Z", "bug_id": 50306, "creation_time": "2010-12-29T16:47:42Z", "is_private": false, "text": "If I understand the patch correctly, I disagree with the idea of an arbitrary (even if configurable) time threshold per thread.\n\nA long running thread is not necessarily a broken one; there are existing methods in JMX for identifying deadlocked threads.\n\nThe same could be achieved with a Valve in the request processing pipeline, without modifying the Executor."}, {"count": 9, "tags": [], "creator": "dirtchamber@hotmail.com", "text": "Created attachment 26769\nNew Valve: StuckThreadDetectionValve\n\nLooking at the source, a Valve seems like a much better place for such code as you wrote. However, I still think we need some kind of a threshold (configurable or not). Deadlocked threads are not the only problem we can face. We are often experiencing problems with erronous data in our DB, which causes loops to occur. Http client request without any configured read or connect timeout can also cause a thread to appear \"stuck\". The threads this Valve should monitor are HTTP/AJP processor threads only (not user threads), and these threads should _normally_ not be processing long running requests. Such requests should preferably be allocated to a separate thread if possible. Users who do need to execute long running request in the HTTP/AJP threads can choose not to use this Valve. This seems to be the spirit of Tomcat 7.x: create tools to help users improve their apps (Ref memory leakage detection logic). In many servers, the main HTTP request threads are considered a limited resource, and thus if such threads are running for too long, the pool will eventually be depleted. This is the case for WebSphere, which has a default max pool size of 50, and a configurable threshold for it's \"hung thread detection\" (Default 600 seconds). A valve for detecting \"long running threads\" can be quite helpful not only in development, but also in production environments in many cases. This valve should probably be disabled by default.\n \nI have now moved the code i previously posted to a Valve. I have attached a patch for Tomcat 7.0.11 if anyone wants to add stuck thread detection logic to their Tomcat instance.", "id": 144996, "time": "2011-03-14T10:30:23Z", "bug_id": 50306, "creation_time": "2011-03-14T10:30:23Z", "is_private": false, "attachment_id": 26769}, {"count": 10, "tags": [], "text": "Tom, thanks for your contribution. I committed the valve to trunk http://svn.apache.org/viewvc?view=revision&revision=1090003 with the following changes :\n\n- fixed a race condition\n- removed the minimum check for the threshold\n- allowed any Container\n- added other information in the logged warning (request, threshold)\n- exposed as JMX, including the IDS of the stuck thread\n- documentation\n\nit will be available in tomcat 7.0.13", "attachment_id": null, "id": 145626, "creator": "slaurent@apache.org", "time": "2011-04-07T17:33:10Z", "bug_id": 50306, "creation_time": "2011-04-07T17:33:10Z", "is_private": false}, {"count": 11, "tags": [], "creator": "knst.kolinko@gmail.com", "attachment_id": 28961, "id": 160097, "time": "2012-06-19T12:45:58Z", "bug_id": 50306, "creation_time": "2012-06-19T12:45:58Z", "is_private": false, "text": "Created attachment 28961\n2012-06-19_tc6_50306_StuckThreadDetectionValve.patch\n\nPort of StuckThreadDetectionValve to Tomcat 6\n\nI've also implemented the following additional features\n(will apply to Tomcat 7 today, will be in 7.0.29):\n- getStuckThreadNames() property, inspectable through JMX\n- added thread ids to the log messages"}, {"count": 12, "tags": [], "bug_id": 50306, "attachment_id": 28966, "id": 160123, "time": "2012-06-20T09:06:47Z", "creator": "knst.kolinko@gmail.com", "creation_time": "2012-06-20T09:06:47Z", "is_private": false, "text": "Created attachment 28966\n2012-06-20_tc6_50306_StuckThreadDetectionValve.patch\n\nUpdated documentation and MBean description,\naligning proposed patch with Tomcat 7."}, {"count": 13, "tags": [], "bug_id": 50306, "attachment_id": null, "id": 160399, "time": "2012-07-02T14:31:27Z", "creator": "knst.kolinko@gmail.com", "creation_time": "2012-07-02T14:31:27Z", "is_private": false, "text": "StuckThreadDetectionValve added to Tomcat 6 in r1356246 and will be in 6.0.36."}]