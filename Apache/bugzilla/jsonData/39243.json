[{"count": 0, "attachment_id": null, "bug_id": 39243, "text": "I can't post files to an ssl site that are larger than 128k (120k works, 140k\ndoesn't work. The error message I get in ssl_error_log is\n\nrequest body exceeds maximum size for SSL buffer\ncould not buffer message body to allow SSL renegotiation to proceed\n\nand the client gets 413. This doesn't occur every time; apparently only when ssl\nrenegotiation is needed.", "id": 87678, "time": "2006-04-07T17:31:57Z", "creator": "mbertheau@gmail.com", "creation_time": "2006-04-07T17:31:57Z", "tags": [], "is_private": false}, {"count": 1, "tags": [], "creator": "rpluem@apache.org", "text": "\n\n*** This bug has been marked as a duplicate of 12355 ***", "id": 87692, "time": "2006-04-07T23:12:59Z", "bug_id": 39243, "creation_time": "2006-04-07T23:12:59Z", "is_private": false, "attachment_id": null}, {"count": 2, "tags": [], "creator": "mbertheau@gmail.com", "attachment_id": null, "text": "The fix to bug 12355 specificially lead to this bug. This is not a duplicate.", "id": 87698, "time": "2006-04-08T10:28:58Z", "bug_id": 39243, "creation_time": "2006-04-08T10:28:58Z", "is_private": false}, {"count": 3, "tags": [], "bug_id": 39243, "attachment_id": null, "id": 87699, "time": "2006-04-08T11:00:55Z", "creator": "rpluem@apache.org", "creation_time": "2006-04-08T11:00:55Z", "is_private": false, "text": "Ok, technically you are right and your report is not exactly a duplicate of\n12355, but it is currently not planned to change this behaviour in the case that\nyou have POST requests + SSL + Directory or Location based client certificates\nwhich require a SSL renegotiation. It would require to introduce disk buffering\nof the POST request body. If you are using client certificates for the whole\nvirtual host everything works fine. So I mark it a WONTFIX."}, {"count": 4, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "There may be good functional reasons for POSTs larger than 128k and to\nrequire client certificates only for access to certain URLs. And\nasking for an optional client certificate at SSL connect bothers users\nof other URLs with unnecessary prompts for client certificates that\nthey may not have and don't need (depending on the browser that they\nuse).\n\nFor us the hard-coded limit is still a problem.\n\n> It would require to introduce disk buffering of the POST request\n> body.\n\nThis is not clear to me. Where is the hard-coded limit of 128k coming\nfrom?\n\nCould the code not look at the value of the directive LimitRequestBody\nand if it is set allow SSL request bodies of that size?", "id": 90726, "time": "2006-06-28T15:15:25Z", "creator": "peter.wagemans@getronics.com", "creation_time": "2006-06-28T15:15:25Z", "is_private": false}, {"count": 5, "tags": [], "bug_id": 39243, "attachment_id": null, "id": 90974, "time": "2006-07-07T09:25:13Z", "creator": "gunchev@gmail.com", "creation_time": "2006-07-07T09:25:13Z", "is_private": false, "text": "I really need at least 200K limit. If I understand correctly, I can 'patch' \nthe code and increase this buffer from 128K to say 256K, recompile apache and \nit will work, right?"}, {"count": 6, "attachment_id": null, "bug_id": 39243, "text": "(In reply to comment #5)\n> I really need at least 200K limit. If I understand correctly, I can 'patch' \n> the code and increase this buffer from 128K to say 256K, recompile apache and \n> it will work, right?\n\nCorrect. You can do this.\n", "id": 91002, "time": "2006-07-07T15:50:49Z", "creator": "rpluem@apache.org", "creation_time": "2006-07-07T15:50:49Z", "tags": [], "is_private": false}, {"count": 7, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "Well, the default could be bumped to 256K, that wouldn't be unreasonable.\n\nBut you should really design your site to ensure that the first request to a\nclient-cert-protected area is not a POST request with a large body; make it a\nGET or something.  Any request body has to be buffered into RAM to handle this\ncase, so represents an opportunity to DoS the server.\n\nTo bump the limit you can build like:\n \n   ./configure CPPFLAGS=-DSSL_MAX_IO_BUFFER=256000\n\nAnybody for whom 128K is too small but 256K would be sufficient, please add a\ncomment here, to gauge interest in making that change.", "id": 91130, "time": "2006-07-11T14:42:21Z", "creator": "jorton@redhat.com", "creation_time": "2006-07-11T14:42:21Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 39243, "attachment_id": null, "id": 91282, "time": "2006-07-13T17:51:16Z", "creator": "powell.hazzard@hp.com", "creation_time": "2006-07-13T17:51:16Z", "is_private": false, "text": "While I do believe the previous unlimited approach could be a DoS. Nice catch. \nI have to believe the \"one size limit fits all\" approach will not work for all \nthe existing applications in the world.   However, shouldn't we add a \nSSLMaxIOBuffer directive instead of hardcoding the value at build time?  This \nway any pre-built server or existing applications have a way to raise/lower \nthis value as needed for any given virtual host or directory?\n"}, {"count": 9, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "> But you should really design your site to ensure that the first\n> request to a client-cert-protected area is not a POST request with a\n> large body; make it a GET or something.\n\nNot really an option with SOAP.\n\n> I have to believe the \"one size limit fits all\" approach will not\n> work for all the existing applications in the world.\n\nAgreed.\n\n> However, shouldn't we add a SSLMaxIOBuffer directive instead of\n> hardcoding the value at build time?\n\nThat is a good way to remove the hard-coded limit. But is there a\nreason why one could not use the existing directive LimitRequestBody,\nas suggested above? It can be set for the client cert protected area\nand then defines the size of requests that should be handled, thus the\namount that should be allowed to be buffered.", "id": 91304, "time": "2006-07-14T07:33:21Z", "creator": "peter.wagemans@getronics.com", "creation_time": "2006-07-14T07:33:21Z", "is_private": false}, {"count": 10, "tags": [], "creator": "jorton@redhat.com", "text": "I'm fairly reluctant to add a config directive for this.  I would be happy with\na \"one size fits most\" hard-coded limit if we could arrive at such a value; what\nis your input on changing the limit to 256K?  Would that be sufficient or not?\n\nOverloading LimitRequestBody for such a purpose is not acceptable, no - the\ndefault is unlimited.", "id": 91308, "time": "2006-07-14T09:22:44Z", "bug_id": 39243, "creation_time": "2006-07-14T09:22:44Z", "is_private": false, "attachment_id": null}, {"count": 11, "tags": [], "creator": "powell.hazzard@hp.com", "text": "> I'm fairly reluctant to add a config directive for this.  \n\n   I can understand your point of view. \n\n> I would be happy with a \"one size fits most\" hard-coded limit if we could \narrive at such a value; what is your input on changing the limit to 256K? \n\nIMHO\n\n   Since I work in support/engineering I can honestly say we have customers \nthat are using soap messages anywhere from 1k to 40mb in size (or higher).  \nSo, if you are asking for my input regarding any hard coded value I would have \nto vote for the 40mb-50mb range.  While I agree those values seem absurd for \nmost small web site, but large SOAP web sites will need this type of limit out \nof the box.  Without a large hard-coded value customers are going to request \nvendors like RedHat to give them a supported version of the Apache web Server \nwith a higher hard coded value because their existing applications that have \nbeen deployed all over the world just stopped working when they installed \nhttp://www.linuxsecurity.com/content/view/120313.  (I\u2019ve already had three \ncustomers)\n\n", "id": 91311, "time": "2006-07-14T14:00:00Z", "bug_id": 39243, "creation_time": "2006-07-14T14:00:00Z", "is_private": false, "attachment_id": null}, {"text": "\n> what is your input on changing the limit to 256K?  Would that be\n> sufficient or not?\n\nNo. We're looking at megabyte SOAP POSTs.\n\n> Overloading LimitRequestBody for such a purpose is not acceptable,\n> no - the default is unlimited.\n\nWith that overload idea, the default value of zero (unlimited) would\nbe translated to the hard-coded value to protect against DOS attempts.\nDefining a positive size for LimitRequestBody would allow that size to\nbe buffered for POSTs in mod_ssl (because it seems sensible to keep\nfunctioning up to the specified limit). I had something along these\nlines in mind:\n\n--- httpd-2.0.46/modules/ssl/ssl_engine_io.c.old ...\n+++ httpd-2.0.46/modules/ssl/ssl_engine_io.c.new ...\n@@ -1395,8 +1395,17 @@\n     struct modssl_buffer_ctx *ctx;\n     apr_bucket_brigade *tempb;\n     apr_off_t total = 0; /* total length buffered */\n+    apr_off_t max_ssl_buffered = 0; /* Maximum allowed memory buffering of ssl\ndata. */\n     int eos = 0; /* non-zero once EOS is seen */\n     \n+    max_ssl_buffered = ap_get_limit_req_body( r );\n+\n+    if (max_ssl_buffered == 0) { \n+      /* If undefined/unlimited, use default limit to defend against\n+       * DOS attempts. */\n+      max_ssl_buffered = SSL_MAX_IO_BUFFER;\n+    }\n+\n     /* Create the context which will be passed to the input filter. */\n     ctx = apr_palloc(r->pool, sizeof *ctx);\n     ctx->bb = apr_brigade_create(r->pool, c->bucket_alloc);\n@@ -1460,7 +1469,7 @@\n                       total, eos);\n \n         /* Fail if this exceeds the maximum buffer size. */\n-        if (total > SSL_MAX_IO_BUFFER) {\n+        if (total > max_ssl_buffered) {\n             ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r,\n                           \"request body exceeds maximum size for SSL buffer\");\n             return HTTP_REQUEST_ENTITY_TOO_LARGE;\n\n", "tags": [], "bug_id": 39243, "attachment_id": null, "count": 12, "id": 91313, "time": "2006-07-14T14:18:06Z", "creator": "peter.wagemans@getronics.com", "creation_time": "2006-07-14T14:18:06Z", "is_private": false}, {"count": 13, "tags": [], "text": "Here's an afterthought to the above patch to allow LimitRequestBody to\ncontrol the size of the SSL buffer. When doing this, it may be a good\nidea to refer to the controlling directive in the error message and\nchange\n\n\"request body exceeds maximum size for SSL buffer\"\n\ninto, for instance,\n\n\"request body exceeds maximum size for SSL buffer; try LimitRequestBody > 0\"", "attachment_id": null, "id": 91348, "creator": "peter.wagemans@getronics.com", "time": "2006-07-17T09:25:44Z", "bug_id": 39243, "creation_time": "2006-07-17T09:25:44Z", "is_private": false}, {"count": 14, "tags": [], "creator": "wrowe@apache.org", "attachment_id": null, "text": "We can allow to to configure this to be larger at a serious cost to how\nmany requests you can process.\n\nThe obvious answer for an 'upload' style operation is to ensure they never\nhit your upload page without going through a simpler front page which first\nenforces the renegotation.  This can be your upload form page.\n\nOnce the session is SSLClientVerify'ed it won't renegotate -again- so this\nproblem won't occur.\n\nNo matter what -we- do, if you design your huge-post page such that it won't\ncause renegotiation on large posts, your server will always have less stress.\nAnd that's a good thing IMHO.  2GB set asides are absurd, but pushing up a\n2GB iso image isn't inconcievable.\n", "id": 91512, "time": "2006-07-23T20:04:56Z", "bug_id": 39243, "creation_time": "2006-07-23T20:04:56Z", "is_private": false}, {"count": 15, "tags": [], "bug_id": 39243, "attachment_id": null, "id": 91519, "time": "2006-07-24T08:22:42Z", "creator": "gunchev@gmail.com", "creation_time": "2006-07-24T08:22:42Z", "is_private": false, "text": "While 256K suits our needs for now (I did recompile and it worked), tomorrow \nwe'll have to post larger scanned documents (say 512K), some time later even \nlarger. My apache is just a reverse proxy with SSL client authentication, so \nan option would be better or I'll have to recompile every change/update..."}, {"count": 16, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "Will Rowe wrote:\n\n> The obvious answer for an 'upload' style operation is to ensure they\n> never hit your upload page without going through a simpler front\n> page which first enforces the renegotation.  This can be your upload\n> form page.\n\n> Once the session is SSLClientVerify'ed it won't renegotate -again-\n> so this problem won't occur.\n\nThis can work for interactive applications, but there are common\nsituations without upload page: an application that wants to submit\ndata to the web server in a SOAP POST request.\n\n\nNote: the above proposal for using an upload page request to\nrenegotiate for the client certificate appears to work only with\n\"SSLVerifyClient none\" but not with \"SSLVerifyClient optional\" at top\nlevel. In the last case a renegotiation is performed on the subsequent\nform POST even when a client certificate is already present. Thus you\nagain run into the 128K limit. This is probably explained by the\nfollowing code in ssl_engine_kernel.c, which only treats \"none\" as a\nspecial case:\n\n        /* optimization */\n\n        if ((dc->nOptions & SSL_OPT_OPTRENEGOTIATE) &&\n            (verify_old == SSL_VERIFY_NONE) &&\n            ((peercert = SSL_get_peer_certificate(ssl)) != NULL))\n        {\n            renegotiate_quick = TRUE;\n            X509_free(peercert);\n        }", "id": 91534, "time": "2006-07-24T11:37:21Z", "creator": "peter.wagemans@getronics.com", "creation_time": "2006-07-24T11:37:21Z", "is_private": false}, {"count": 17, "tags": [], "creator": "smitha.jasti@gmail.com", "text": "Hi,\n\nI am too am facing problem due to the fixed buffer size. I saw the suggestion \nabout adding a directive to modify the buffer size as needed. Has there been \nany work done on this regard? Any other suggestion about how this problem \ncould be fixed?", "id": 95996, "time": "2006-11-19T22:51:05Z", "bug_id": 39243, "creation_time": "2006-11-19T22:51:05Z", "is_private": false, "attachment_id": null}, {"count": 18, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "We are currently in the process of getting this 'fixed' via Red Hat commercial\nsupport (which we have). The fix of Peter Wagemans will probably be extended a\nlittle and hopefully checked in.", "id": 98307, "time": "2007-01-20T07:38:34Z", "creator": "bugzilla@ronald.vankuijk.net", "creation_time": "2007-01-20T07:38:34Z", "is_private": false}, {"text": "I'm hitting this limit too, using apache 2.2.3, built from a standard\nGentoo-based ebuild.\n\nI'm using Apache here in this context as a front-end to Zope/Plone, and plone\noffers the user the option of uploading content.  This content has no inherent\nplone-based size limits.  So in my case, if I use SSL to secure my sites (which\nI do), and I use Apache as a front-end, as described in several places in plone\ndocumentation, two of which are here:\nhttp://plone.org/documentation/tutorial/plone-apache\nhttp://plone.org/documentation/how-to/apache-ssl\n\n...and I upload large files, then I get nailed by this limit.  Has any further\nwork been done with this in 2.2?\n\nWhat info is still needed to resolve this bug?", "tags": [], "bug_id": 39243, "attachment_id": null, "count": 19, "id": 100007, "time": "2007-03-03T09:01:16Z", "creator": "asfbugzilla@gnosys.biz", "creation_time": "2007-03-03T09:01:16Z", "is_private": false}, {"count": 20, "tags": [], "bug_id": 39243, "attachment_id": null, "id": 100012, "time": "2007-03-03T16:50:51Z", "creator": "asfbugzilla@gnosys.biz", "creation_time": "2007-03-03T16:50:51Z", "is_private": false, "text": "Sorry.  I should have added above that there are no client certificates involved\nin these uploads.  I'm not savvy enough about the internals of either apache or\nplone to know, but I suppose that means it's possible that what I'm seeing is\nnot actually this bug, but the behavior of my systems match the symptoms in\nevery way except for the involvement of client certificates, so to me that means\nthat if they are not the same, then they are at least, very probably strongly\nassociated with each other.\n\nWhen I upload files 128kb and smaller, it works as expected.  When I attempt to\nupload files 129kb and larger, I get this:\n\nError message in browser:\nTitle: 413 Request Entity Too Large\nPage: Request Entity Too Large\nThe requested resource\n/Members/admin/portal_factory/Image/image.2007-03-03.9545920618/atct_edit\ndoes not allow request data with POST requests, or the amount of data provided\nin the request exceeds the capacity limit.\n\nError message in logs:\n[Sat Mar 03 19:26:35 2007] [error] [client xxx.yyy.zzz.ttt] request body exceeds\nmaximum size for SSL buffer, referer:\nhttps://www.example.com/Members/admin/portal_factory/Image/image.2007-03-03.9545920618/edit\n[Sat Mar 03 19:26:35 2007] [error] [client xxx.yyy.zzz.ttt] could not buffer\nmessage body to allow SSL renegotiation to proceed, referer:\nhttps://www.example.com/Members/admin/portal_factory/Image/image.2007-03-03.9545920618/edit\n\nI've spoken with someone on the plone list who's using RHEL and apache/ssl/plone\nin the same manner that I am, and he reports not suffering from this problem. \nI'm not sure if he has any upper limit at all, or if the upper limit is simply\nlarger than 128kb.  I'm still talking with him.\n\nI guess redhat has applied some sort of patch.  Does anyone know about that?  Is\nit the same one mentioned in this bug report?  I'd like to have the limit (if it\nmust exist) be up in the 40-50 MB range myself.  If there's another patch,\nperhaps someone could refer me to it?"}, {"count": 21, "tags": [], "bug_id": 39243, "attachment_id": null, "id": 100014, "time": "2007-03-04T06:38:23Z", "creator": "asfbugzilla@gnosys.biz", "creation_time": "2007-03-04T06:38:23Z", "is_private": false, "text": "[somewhat sheepishly]: After all the discussion, and rereading documentation and\nconfig files and the bug report several times over, I noticed that my apache\nserver config file used the SSLVerifyClient Directive at level \"optional\" and\nthat the documentation states, \"In practice only levels 'none' and 'require' are\nreally interesting, because level 'optional' doesn't work with all browsers\".  I\nwas also using the SSLVerifyDepth Directive at a depth number of 1.\n\nBy commenting out these two directives, I solved the problem.\n\nWhen I remarked earlier that client certificates were not involved at all, I\nmistakenly considered only what was going on with the client, failing to\nconsider client certificate directives on the server. Apologies if I should have\nthought of that sooner, and if I generated a lot of commotion over nothing.\n"}, {"count": 22, "tags": [], "creator": "rocketraman@fastmail.fm", "text": "I am running httpd 2.2.3 on CentOS 5.\n\nThis problem also affects SugarCRM attachment uploads. The login page for\nSugarCRM uses a GET request, so the renegotiation should be fine, but users\nreport that sometimes the attachment upload still fails with this error. Perhaps\nthe client-certificate SSL session times out or something, which forces httpd to\nrenegotiate again? If so, this is yet another use case that supports adding a\nconfigurable per-location buffer directive.\n\nI can find no work-around for this other than setting \"SSLVerifyClient require\"\nat the virtual host level. However, we have good reasons to *not* set\n\"SSLVerifyClient require\" at the virtual host level, since many of our SSL\nservices do not require client certs. As stated in the docs, \"SSLVerifyClient\noptional\" doesn't work for all clients (e.g. WebDAV on win2k for one).", "id": 105434, "time": "2007-07-15T09:47:47Z", "bug_id": 39243, "creation_time": "2007-07-15T09:47:47Z", "is_private": false, "attachment_id": null}, {"count": 23, "tags": [], "creator": "rocketraman@fastmail.fm", "text": "(In reply to comment #22)\n> Perhaps the client-certificate SSL session times out or something, which\n> forces httpd to renegotiate again? If so, this is yet another use case that\n> supports adding a configurable per-location buffer directive.\n\nI confirmed that the SSLSessionCacheTimeout affects renegotiation. Therefore, at\nleast for interactive applications where the upload form uses a GET request, I\nbelieve this issue can be worked around by setting SSLSessionCacheTimeout to a\nvalue at least as large as the application session timeout. The default of 300\non CentOS 5 was easily exceeded by a user who is uploading an attachment, while\nalso filling in associated description and other form fields before clicking Submit.\n\n> As stated in the docs, \"SSLVerifyClient optional\" doesn't work for all\n> clients (e.g. WebDAV on win2k for one).\n\nCorrection: I'm not sure about WebDAV on win2k working with optional or not --\nthe test I did earlier was incorrect. However, the point stands since I do not\nwant clients to be prompted for certificates anyway.", "id": 105435, "time": "2007-07-15T10:25:31Z", "bug_id": 39243, "creation_time": "2007-07-15T10:25:31Z", "is_private": false, "attachment_id": null}, {"count": 24, "tags": [], "bug_id": 39243, "attachment_id": 21473, "id": 113449, "time": "2008-02-05T13:08:46Z", "creator": "krzysiek.pawlik@people.pl", "creation_time": "2008-02-05T13:08:46Z", "is_private": false, "text": "Created attachment 21473\nhttpd-2.2.8-ssl-io-buffer.patch\n\nThis patch adds SSLMaximumBufferSize tunable - it's global for whole module.\nDefaults to 0, which means to use the default 128k limit."}, {"count": 25, "tags": [], "creator": "nico@aw-con.de", "text": "(In reply to comment #24)\n> Created an attachment (id=21473) [details]\n> httpd-2.2.8-ssl-io-buffer.patch\n> \n> This patch adds SSLMaximumBufferSize tunable - it's global for whole module.\n> Defaults to 0, which means to use the default 128k limit.\n\nthanks, works well for me. I used the patch on debian apache2-2.2.3 source.\nAfter one week still no problems with the patch. Thanks for your work", "id": 114457, "time": "2008-03-10T05:20:20Z", "bug_id": 39243, "creation_time": "2008-03-10T05:20:20Z", "is_private": false, "attachment_id": null}, {"count": 26, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "\nI'm wondering about the reasons for a patch with a configurable global\nlimit (comment #24) instead of the patch described in comments #12 and\n#13 which uses the existing LimitRequestBody directive to give control\nover the SSL maximum buffer size at the level of server config,\nvirtual host, directory or .htaccess.\n\nIs there perhaps a reason for keeping the SSL maximum buffer size\nsmaller than a configured maximal request size?\n\n", "id": 114479, "time": "2008-03-11T03:24:14Z", "creator": "peter.wagemans@getronics.com", "creation_time": "2008-03-11T03:24:14Z", "is_private": false}, {"count": 27, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "\nI hope I am not overstepping my bounds, but I'm trying to increase the severity of this issue and bring some attention back to it.\n\nA virtualhost that mixes browser-based content and REST/SOAP services is not uncommon.\n\nThis hardcoded buffer makes the REST/SOAP activities likely to fail.\n\nI am not sure why Joe marked this as needs info - what info is needed?", "id": 121742, "time": "2008-10-21T10:58:36Z", "creator": "mcrawfor@u.washington.edu", "creation_time": "2008-10-21T10:58:36Z", "is_private": false}, {"count": 28, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "I've added a SSLRenegBufferSize directive in r726109 to make this buffer size configurable.  Thanks for all the feedback.", "id": 123267, "time": "2008-12-12T12:21:32Z", "creator": "jorton@redhat.com", "creation_time": "2008-12-12T12:21:32Z", "is_private": false}, {"count": 29, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "Thanks.... finally an official fix. Still I'm curious about why an additional directive... see also comment #26", "id": 123273, "time": "2008-12-13T13:03:24Z", "creator": "bugzilla@ronald.vankuijk.net", "creation_time": "2008-12-13T13:03:24Z", "is_private": false}, {"count": 30, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "(In reply to comment #29)\n> Thanks.... finally an official fix. Still I'm curious about why an additional\n> directive... see also comment #26\n\nOverloading the LimitRequestBody semantics would not be appropriate because it could make existing configurations do something completely surprising.", "id": 123340, "time": "2008-12-16T03:19:41Z", "creator": "jorton@redhat.com", "creation_time": "2008-12-16T03:19:41Z", "is_private": false}, {"count": 31, "attachment_id": null, "bug_id": 39243, "text": "Unfortunately I found this report after posting https://issues.apache.org/bugzilla/show_bug.cgi?id=46508\n\nJoe's patch in https://issues.apache.org/bugzilla/show_bug.cgi?id=39243#c28 >almost< works.  \n\nSee https://issues.apache.org/bugzilla/show_bug.cgi?id=46508 for the issue, a fix, and a backport to 2.2.  Hopefully he'll make it official.\n\nHope this helps anyone else who tries to use or backport as-is.\n\n(I didn't re-open this report since 46508 has all the detail and is open.)", "id": 123924, "time": "2009-01-10T07:26:36Z", "creator": "tlhackque@yahoo.com", "creation_time": "2009-01-10T07:26:36Z", "tags": [], "is_private": false}, {"count": 32, "tags": [], "text": "Proposed for backport as r733472.", "attachment_id": null, "id": 123933, "creator": "rpluem@apache.org", "time": "2009-01-11T05:44:00Z", "bug_id": 39243, "creation_time": "2009-01-11T05:44:00Z", "is_private": false}, {"count": 33, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "\nThanks very much for the official fix.\n\nOne remark on comment #30 from Joe Orton.\n\n> Overloading the LimitRequestBody semantics would not be appropriate\n> because it could make existing configurations do something\n> completely surprising.\n\nA separate directive is the best solution, but all the old patch in\ncomments #12,#13 does is allow requests up to the configured\nLimitRequestBody value (if set > 0), also for requests > 128k. For me\nthat's not a \"completely surprising\" behaviour change.\n", "id": 124285, "time": "2009-01-22T05:23:27Z", "creator": "peter.wagemans@getronics.com", "creation_time": "2009-01-22T05:23:27Z", "is_private": false}, {"count": 34, "tags": [], "bug_id": 39243, "attachment_id": null, "text": "My point was that e.g. if someone had LimitRequestBody set to 100mb for some PHP script they were using in the same context, and mod_ssl started buffering up to 100mb of request body into RAM per process -- from untrusted users -- that would allow a DoS attack against the server.  This would be surprising ;)", "id": 124291, "time": "2009-01-22T07:44:30Z", "creator": "jorton@redhat.com", "creation_time": "2009-01-22T07:44:30Z", "is_private": false}, {"count": 35, "tags": [], "text": "Yes, that's the side effect of the memory buffering. Explicitly\nallowing large POSTS and doing client certificate authentication is\nprobably a rare combination for untrusted users.\n\nAnyway, I'm glad it is fixed after using the old workaround patch for\n2.5 years. Thanks.\n", "attachment_id": null, "id": 124297, "creator": "peter.wagemans@getronics.com", "time": "2009-01-22T09:51:29Z", "bug_id": 39243, "creation_time": "2009-01-22T09:51:29Z", "is_private": false}, {"count": 36, "attachment_id": null, "bug_id": 39243, "text": "To overcome this limit in version 2.0.x, it looks like the options are either backporting the fix to 2.0, or recompiling the standard 2.0 source with \" ./configure CPPFLAGS=-DSSL_MAX_IO_BUFFER=256000\".   The latter option seems the simplest.  If I do this, which files will this modify?  Is it just the mod_ssl.so file, or are other files impacted?", "id": 125784, "time": "2009-03-25T07:05:27Z", "creator": "ms.carmen.alvarez@gmail.com", "creation_time": "2009-03-25T07:05:27Z", "tags": [], "is_private": false}, {"text": "Note that I found out that the workaround mentioned in comment #22, to place the SSLVerifyClient directive outside of the Directory section, inside the main Vhost container, *inactivated* the complete client cert verification part, i.e. any client could connect. Moving the statements back into the <Directory> restored security. I'll see if I can isolate it and file another bug.", "tags": [], "bug_id": 39243, "attachment_id": null, "count": 37, "id": 129058, "time": "2009-07-22T15:05:03Z", "creator": "apache.org@spam.ennes.nl", "creation_time": "2009-07-22T15:05:03Z", "is_private": false}, {"text": "Note to my last comment, I was stung by this issue:\nhttps://issues.apache.org/bugzilla/show_bug.cgi?id=12355\nSo take care...", "tags": [], "bug_id": 39243, "attachment_id": null, "count": 38, "id": 129063, "time": "2009-07-23T01:32:07Z", "creator": "apache.org@spam.ennes.nl", "creation_time": "2009-07-23T01:32:07Z", "is_private": false}, {"count": 39, "tags": [], "creator": "rpluem@apache.org", "text": "See SSLRenegBufferSize for adjusting the buffersize (http://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslrenegbuffersize)", "id": 132348, "time": "2009-11-26T00:38:04Z", "bug_id": 39243, "creation_time": "2009-11-26T00:38:04Z", "is_private": false, "attachment_id": null}, {"count": 40, "tags": [], "bug_id": 39243, "attachment_id": null, "id": 165840, "time": "2013-03-14T15:44:03Z", "creator": "Ryan.Slominski@gmail.com", "creation_time": "2013-03-14T15:44:03Z", "is_private": false, "text": "It seems curl solves this issue by sending the HTTP 1.1 header \"Expect\" with value \"100-continue\" when you use a client certificate to perform an HTTP PUT of a huge file.  Apparently this allows the SSL renegotiate to occur before the large payload (body) is even transferred.  I noticed this due to my Java client failing with the error discussed in this thread, but curl (and libcurl) had no issues.  Unfortunately the Sun Java built-in HttpsUrlConnection implementation doesn't support the \"Expect\" header.  Use the Java Apache Foundation HTTP Client library or OpenJDK implementation instead.  Hope this helps someone who digs up this thread."}]