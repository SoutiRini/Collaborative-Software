[{"count": 0, "tags": [], "creator": "Dick.Snippe@tech.omroep.nl", "attachment_id": null, "text": "We run a number of reverse caching proxies. Basically mod_cache + mod_disk_cache\n+ mod_proxy + mod_proxy_http, doing ProxyPass all in the worker mpm.\ncompiled using:\n./configure --prefix=/software/apache-cache-2.2.0 --with-mpm=worker\n--enable-cache --enable-mem-cache --enable-disk-cache --enable-proxy\n--enable-proxy-http --enable-proxy-ajp --enable-proxy-balancer --enable-rewrite\n--enable-mods-shared=all --enable-nonportable-atomics\n\nRecently we upgrade from 2.0.X to 2.2.0\nIn some of the webserver instances we see processes consuming 100% CPU time.\nWhen interrogated with ptrace, such a thread appears to be in a futex:\nProcess 8329 attached - interrupt to quit\nfutex(0xa83c9bf8, FUTEX_WAIT, 8361, NULL <unfinished ...>\n\nThe webserver does not hang. I assume because it spawns another process that\nhandles all new requests.\napache status shows only 2 active threads in the old process:\nR.......W.......................................................\nI guess that 1 thread (the one you canb see in ptrace) is halted on the futex\nsystem call, and the other thread is spinning in the usermode part of a futex\n(and hence is not visible from ptrace)\n\nUnfortunately I cannot reproduce the bug, all I can do is wait until it happens\nagain on a production server.\nHowever I've done some testing. Here are the results:\n* compiling with --enable-nonportable-atomics does nor make a differerence\n* using mem_cechc or disk_cache does not make a difference\n* compiling with the prefork mpm DOES appear to make a difference, I haven't\nseen prefork mpm's behaving like this (yet)\n\nAlso, out of the 5 instances we run, 1 instance does not appear to have this\nproblem (it has been running for ~3 weeks, whereas the others will show the\nproblem within a couple of hours). Alle instances are moderately busy with\nanything between 100.000 and 10.000.000 hits per day.", "id": 87146, "time": "2006-03-23T12:31:50Z", "bug_id": 39079, "creation_time": "2006-03-23T12:31:50Z", "is_private": false}, {"count": 1, "attachment_id": null, "creator": "rpluem@apache.org", "is_private": false, "id": 87173, "time": "2006-03-23T20:56:16Z", "bug_id": 39079, "creation_time": "2006-03-23T20:56:16Z", "tags": [], "text": "Can you try to attach with gdb next time and provide a full backtrace\n(thread apply all bt full, see http://httpd.apache.org/dev/debugging.html)?"}, {"count": 2, "tags": [], "bug_id": 39079, "attachment_id": null, "id": 87213, "time": "2006-03-24T14:38:27Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-24T14:38:27Z", "is_private": false, "text": "Here's one, I'll try to catch some others.\n\n(gdb) thread apply all bt full\n\nThread 4 (Thread -1229993040 (LWP 20483)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7c74a5c in poll () from /lib/libc.so.6\nNo symbol table info available.\n#2  0xb7d5461a in apr_wait_for_io_or_timeout (f=0x0, s=0x82aa208, for_read=0)\n    at support/unix/waitio.c:51\n        pfd = {fd = 14, events = 4, revents = 0}\n        rc = Variable \"rc\" is not available.\n\nThread 3 (Thread -1297134672 (LWP 20491)):\n#0  apr_pool_cleanup_kill (p=0x85c4690, data=0x85ddde0, \n    cleanup_fn=0xb7ecb444 <brigade_cleanup>) at memory/unix/apr_pools.c:1985\n        c = (cleanup_t *) 0x85dc650\n        lastp = (cleanup_t **) 0x85dc650\n#1  0xb7ecb488 in apr_brigade_destroy (b=0x85ddde0) at buckets/apr_brigade.c:52\nNo locals.\n#2  0x08066ccc in ap_getline (s=0xb2af2038 \"\", n=8192, r=0x85dc750, fold=0)\n    at protocol.c:464\n        tmp_s = 0xb2af2038 \"\"\n        rv = 70014\n        len = Variable \"len\" is not available.\n\nThread 2 (Thread -1464988752 (LWP 20511)):\n---Type <return> to continue, or q <return> to quit---\n#0  apr_pool_cleanup_kill (p=0x85c4690, data=0x822eb70, \n    cleanup_fn=0xb7b8b220 <connection_cleanup>) at memory/unix/apr_pools.c:1985\n        c = (cleanup_t *) 0x85dc650\n        lastp = (cleanup_t **) 0x85dc650\n#1  0xb7b8b929 in ap_proxy_release_connection (\n    proxy_function=0xb7b7fb0e \"HTTP\", conn=0x822eb70, s=0x81cad40)\n    at proxy_util.c:1774\nNo locals.\n#2  0xb7b7ce06 in ap_proxy_http_cleanup (scheme=0xb7b7fb0e \"HTTP\", \n    r=0x85e45e0, backend=0x822eb70) at mod_proxy_http.c:1586\nNo locals.\n#3  0xb7b7df6f in proxy_http_handler (r=0x85e45e0, worker=0x80bfa70, \n    conf=0x80bf7c0, url=0x85dbf88 \"/teletubbies/images/liedjes.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1728\n        status = 502\n        server_portstr =\n\"\\000\\210\\001&#65533;&#65533;5&#65533;&#65533;&#65533;&#65533;^\\b\\220E^\\b\\020\\000\\000\\000\\210\\2238888^\\b&#65533;^\"\n        scheme = 0x85c6640 \"http\"\n        proxy_function = 0xb7b7fb0e \"HTTP\"\n        backend = (proxy_conn_rec *) 0x822eb70\n        is_ssl = 0\n        p = (apr_pool_t *) 0x85c4690\n        uri = (apr_uri_t *) 0x85c6610\n---Type <return> to continue, or q <return> to quit---\n#4  0xb7b85a17 in proxy_run_scheme_handler (r=0x85e45e0, worker=0x80bfa70, \n    conf=0x80bf7c0, \n    url=0x85e59de \"http://tleac5as:8888/teletubbies/images/liedjes.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n        pHook = Variable \"pHook\" is not available.\n\nThread 1 (Thread -1212471616 (LWP 20480)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7ce6f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7d558d1 in apr_thread_join (retval=0xbfa1e6d8, thd=0x81e8cf0)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n"}, {"count": 3, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 87234, "time": "2006-03-24T21:41:28Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-24T21:41:28Z", "text": "Here's another\n(gdb) thread apply all bt full\n\nThread 2 (Thread -1532130384 (LWP 7929)):\n#0  apr_brigade_length (bb=0x95bc4470, read_all=0, length=0xa4ad8040)\n    at buckets/apr_brigade.c:171\n        total = 9210405550841194496\n        bkt = (apr_bucket *) 0x9620a920\n#1  0xb7b7ccd8 in pass_brigade (bucket_alloc=Variable \"bucket_alloc\" is not\navailable.\n) at mod_proxy_http.c:181\n        status = Variable \"status\" is not available.\n\nThread 1 (Thread -1212471616 (LWP 7890)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7ce6f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7d558d1 in apr_thread_join (retval=0xbfa1e6d8, thd=0x81e9170)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available."}, {"count": 4, "tags": [], "text": "Thanks for the backtraces. Currently I cannot see a reason from them for the\nproblem. Could you please provide the following to increase the chance of\nfinding the reason:\n\n- The full output of thread apply all bt full. I think the stack traces are \nlonger then just 3 or 4 steps.\n- Provide the number of the LWP that is consuming the CPU.\n- After doing thread apply all bt full detach gdb attach it again 10 secs later\nand do a thread apply all bt full again. Repeat this twice such that we have 3 \nthread apply all bt full outputs with 10 secs difference in between", "is_private": false, "id": 87239, "creator": "rpluem@apache.org", "time": "2006-03-24T22:53:30Z", "bug_id": 39079, "creation_time": "2006-03-24T22:53:30Z", "attachment_id": null}, {"count": 5, "tags": [], "creator": "Dick.Snippe@tech.omroep.nl", "is_private": false, "text": "No really. It's only  3 threads.\n\nHere's another example\n(23999 is the LWP that's consuming all CPU)\n$ sudo gdb /local/apache-cache/bin/httpd 23999\n### initital output \"Loading symbols...\" removed ###\n(gdb) thread apply all bt full\nThread 3 (Thread -1498559568 (LWP 24034)):\n#0  0x080705bc in ap_core_input_filter (f=0x84170c8, b=0x854bf68, \n    mode=AP_MODE_READBYTES, block=APR_NONBLOCK_READ, readbytes=8192)\n    at core_filters.c:141\n        e = (apr_bucket *) 0x84170fc\n        e = Variable \"e\" is not available.\n\nThread 2 (Thread -1540523088 (LWP 24039)):\n#0  0x080705e1 in ap_core_input_filter (f=0x84170c8, b=0x8565278, \n    mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0)\n    at core_filters.c:141\n        e = (apr_bucket *) 0x84170fc\n        e = Variable \"e\" is not available.\n\nThread 1 (Thread -1212471616 (LWP 23999)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7ce6f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7d558d1 in apr_thread_join (retval=0xbfa1e6d8, thd=0x8106d78)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n(gdb)  quit\nThe program is running.  Quit anyway (and detach it)? (y or n) y\nDetaching from program: /software/apache-cache-2.2.0/bin/httpd, process 23999\n\n### wait 10 seconds ###\n$  sudo gdb /local/apache-cache/bin/httpd 23999\n(gdb) thread apply all bt full\n\nThread 3 (Thread -1498559568 (LWP 24034)):\n#0  0x080705bc in ap_core_input_filter (f=0x84170c8, b=0x854bf68, \n    mode=AP_MODE_READBYTES, block=APR_NONBLOCK_READ, readbytes=8192)\n    at core_filters.c:141\n        e = (apr_bucket *) 0x84170fc\n        e = Variable \"e\" is not available.\n\nThread 2 (Thread -1540523088 (LWP 24039)):\n#0  0x080705d3 in ap_core_input_filter (f=0x84170c8, b=0x8565278, \n    mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0)\n    at core_filters.c:141\n        e = (apr_bucket *) 0x84170fc\n        e = Variable \"e\" is not available.\n\nThread 1 (Thread -1212471616 (LWP 23999)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7ce6f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7d558d1 in apr_thread_join (retval=0xbfa1e6d8, thd=0x8106d78)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n(gdb) \n\n### and once more ####\n(gdb) thread apply all bt full\n\nThread 3 (Thread -1498559568 (LWP 24034)):\n#0  0x080705bf in ap_core_input_filter (f=0x84170c8, b=0x854bf68, \n    mode=AP_MODE_READBYTES, block=APR_NONBLOCK_READ, readbytes=8192)\n    at core_filters.c:141\n        e = (apr_bucket *) 0x84170fc\n        e = Variable \"e\" is not available.\n\nThread 2 (Thread -1540523088 (LWP 24039)):\n#0  0x080705d9 in ap_core_input_filter (f=0x84170c8, b=0x8565278, \n    mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0)\n    at core_filters.c:141\n        e = (apr_bucket *) 0x84170fc\n        e = Variable \"e\" is not available.\n\nThread 1 (Thread -1212471616 (LWP 23999)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7ce6f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7d558d1 in apr_thread_join (retval=0xbfa1e6d8, thd=0x8106d78)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n(gdb) ", "id": 87313, "time": "2006-03-27T12:13:40Z", "bug_id": 39079, "creation_time": "2006-03-27T12:13:40Z", "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 87314, "time": "2006-03-27T13:51:40Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-27T13:51:40Z", "text": "Another one. Process 22506 this time.\n(gdb) thread apply all bt full\nThread 3 (Thread -1682400336 (LWP 22563)):\n#0  0xb7e0f174 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:334\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 2 (Thread -1732756560 (LWP 22569)):\n#0  0xb7e0f182 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:343\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 1 (Thread -1211672896 (LWP 22506)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7da9f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7e188d1 in apr_thread_join (retval=0xbfce2d98, thd=0x81e93b0)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n\n### detach, wait 10 seconds, attach ###\n(gdb) thread apply all bt full\n\nThread 3 (Thread -1682400336 (LWP 22563)):\n#0  apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:347\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 2 (Thread -1732756560 (LWP 22569)):\n#0  apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:334\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 1 (Thread -1211672896 (LWP 22506)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7da9f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7e188d1 in apr_thread_join (retval=0xbfce2d98, thd=0x81e93b0)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n\n\n\n\n(gdb) thread apply all bt full\n\nThread 3 (Thread -1682400336 (LWP 22563)):\n#0  0xb7e0f168 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:358\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 2 (Thread -1732756560 (LWP 22569)):\n#0  apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:332\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 1 (Thread -1211672896 (LWP 22506)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7da9f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7e188d1 in apr_thread_join (retval=0xbfce2d98, thd=0x81e93b0)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n\n### detach, wait 10 seconds, attach ###\n\n(gdb) thread apply all bt full\n\nThread 3 (Thread -1682400336 (LWP 22563)):\n#0  0xb7e0f179 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:339\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 2 (Thread -1732756560 (LWP 22569)):\n#0  apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:348\nNo locals.\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n        node = Variable \"node\" is not available.\n\nThread 1 (Thread -1211672896 (LWP 22506)):\n#0  0xffffe410 in __kernel_vsyscall ()\nNo symbol table info available.\n#1  0xb7da9f98 in pthread_join () from /lib/libpthread.so.0\nNo symbol table info available.\n#2  0xb7e188d1 in apr_thread_join (retval=0xbfce2d98, thd=0x81e93b0)\n    at threadproc/unix/thread.c:214\n        stat = Variable \"stat\" is not available.\n"}, {"count": 7, "text": "Ah! \"thread apply bt\" (i.e. without \"full\") shows deeper backtraces. Does this\nhelp you?\n\n(same process as in previous comment)\n(gdb) thread apply all bt\n\nThread 3 (Thread -1682400336 (LWP 22563)):\n#0  0xb7e0f172 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:334\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n#2  0xb7f8dd6e in socket_bucket_read (a=0x968b9c68, str=0x9bb84ee4, \n    len=0x9bb84ee0, block=APR_BLOCK_READ) at buckets/apr_buckets_socket.c:43\n#3  0x0807079c in ap_core_input_filter (f=0x870a1c0, b=0x870b4d0, \n    mode=AP_MODE_READBYTES, block=APR_BLOCK_READ, readbytes=8192)\n    at core_filters.c:245\n#4  0x0807d1f6 in ap_http_filter (f=0x88f8c80, b=0x870b4d0, \n    mode=AP_MODE_READBYTES, block=APR_BLOCK_READ, readbytes=8192)\n    at http_filters.c:337\n#5  0xb7c40176 in ap_proxy_http_process_response (p=0x8709628, r=0x8b767e0, \n    backend=0x9624518, origin=0x8709d88, conf=0x80bf7c0, \n    server_portstr=0x9bb89167 \"\") at mod_proxy_http.c:1459\n#6  0xb7c41790 in proxy_http_handler (r=0x8b767e0, worker=0x80bfa70, \n    conf=0x80bf7c0, url=0x8709d68 \"/destraat/gfx/md_1_islam.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1715\n#7  0xb7c48a17 in proxy_run_scheme_handler (r=0x8b767e0, worker=0x80bfa70, \n    conf=0x80bf7c0, \n    url=0x8b77bbe \"http://tleac5as:8888/destraat/gfx/md_1_islam.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#8  0xb7c4a9ec in proxy_handler (r=0x8b767e0) at mod_proxy.c:739\n#9  0x08071efa in ap_run_handler (r=0x8b767e0) at config.c:157\n#10 0x08072272 in ap_invoke_handler (r=0x8b767e0) at config.c:371\n#11 0x0807ccf1 in ap_process_request (r=0x8b767e0) at http_request.c:258\n#12 0x0807a8bd in ap_process_http_connection (c=0x8709818) at http_core.c:171\n#13 0x0807789e in ap_run_process_connection (c=0x8709818) at connection.c:43\n#14 0x08080bd7 in worker_thread (thd=0x81e93b0, dummy=0x822a3d8)\n    at worker.c:531\n#15 0xb7e18760 in dummy_worker (opaque=0x81e93b0)\n    at threadproc/unix/thread.c:138\n#16 0xb7da9b80 in start_thread () from /lib/libpthread.so.0\n#17 0xb7d419ce in clone () from /lib/libc.so.6\n\nThread 2 (Thread -1732756560 (LWP 22569)):\n#0  0xb7e0f182 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:343\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n#2  0xb7f8dd6e in socket_bucket_read (a=0x968ba030, str=0x98b7ee14, \n    len=0x98b7ee18, block=APR_BLOCK_READ) at buckets/apr_buckets_socket.c:43\n#3  0x0806806d in ap_content_length_filter (f=0x96420ba0, b=0x8403890)\n    at protocol.c:1258\n#4  0x0807ee7f in ap_byterange_filter (f=0x96420b88, bb=0x8403868)\n    at byterange_filter.c:169\n#5  0xb7c6b2d8 in cache_save_filter (f=0x96421060, in=0x8403868)\n    at mod_cache.c:810\n#6  0xb7c4010c in ap_proxy_http_process_response (p=0x8402e20, r=0x9641fe30, \n    backend=0x9624518, origin=0x8709d88, conf=0x80bf7c0, \n    server_portstr=0x98b83167 \"\") at mod_proxy_http.c:1518\n#7  0xb7c41790 in proxy_http_handler (r=0x9641fe30, worker=0x80bfa70, \n    conf=0x80bf7c0, url=0x8403558 \"/leesdas/swf/reactie3.swf\", proxyname=0x0, \n    proxyport=0) at mod_proxy_http.c:1715\n#8  0xb7c48a17 in proxy_run_scheme_handler (r=0x9641fe30, worker=0x80bfa70, \n    conf=0x80bf7c0, \n    url=0x9642122e \"http://tleac5as:8888/leesdas/swf/reactie3.swf\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#9  0xb7c4a9ec in proxy_handler (r=0x9641fe30) at mod_proxy.c:739\n#10 0x08071efa in ap_run_handler (r=0x9641fe30) at config.c:157\n#11 0x08072272 in ap_invoke_handler (r=0x9641fe30) at config.c:371\n#12 0x0807ccf1 in ap_process_request (r=0x9641fe30) at http_request.c:258\n#13 0x0807a8bd in ap_process_http_connection (c=0x8403010) at http_core.c:171\n#14 0x0807789e in ap_run_process_connection (c=0x8403010) at connection.c:43\n#15 0x08080bd7 in worker_thread (thd=0x81e9470, dummy=0x822a798)\n    at worker.c:531\n#16 0xb7e18760 in dummy_worker (opaque=0x81e9470)\n    at threadproc/unix/thread.c:138\n#17 0xb7da9b80 in start_thread () from /lib/libpthread.so.0\n#18 0xb7d419ce in clone () from /lib/libc.so.6\n\nThread 1 (Thread -1211672896 (LWP 22506)):\n#0  0xffffe410 in __kernel_vsyscall ()\n#1  0xb7da9f98 in pthread_join () from /lib/libpthread.so.0\n#2  0xb7e188d1 in apr_thread_join (retval=0xbfce2d98, thd=0x81e93b0)\n    at threadproc/unix/thread.c:214\n#3  0x0808102f in join_workers (listener=0x81e94d0, threads=0x821fc60)\n    at worker.c:1079\n#4  0x080813c8 in child_main (child_num_arg=Variable \"child_num_arg\" is not\navailable.) at worker.c:1262\n#5  0x080814e0 in make_child (s=0x80a4d88, slot=2) at worker.c:1316\n#6  0x08082125 in ap_mpm_run (_pconf=0x80a30a8, plog=0x80d1160, s=0x80a4d88)\n    at worker.c:1518\n#7  0x08061b50 in main (argc=3, argv=0xbfce3004) at main.c:712\n(gdb) \n\n\n### detach, sleep 10, attach ###\n\nThread 3 (Thread -1682400336 (LWP 22563)):\n#0  0xb7e0f193 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:358\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n#2  0xb7f8dd6e in socket_bucket_read (a=0x968b9c68, str=0x9bb84ee4, \n    len=0x9bb84ee0, block=APR_BLOCK_READ) at buckets/apr_buckets_socket.c:43\n#3  0x0807079c in ap_core_input_filter (f=0x870a1c0, b=0x870b4d0, \n    mode=AP_MODE_READBYTES, block=APR_BLOCK_READ, readbytes=8192)\n    at core_filters.c:245\n#4  0x0807d1f6 in ap_http_filter (f=0x88f8c80, b=0x870b4d0, \n    mode=AP_MODE_READBYTES, block=APR_BLOCK_READ, readbytes=8192)\n    at http_filters.c:337\n#5  0xb7c40176 in ap_proxy_http_process_response (p=0x8709628, r=0x8b767e0, \n    backend=0x9624518, origin=0x8709d88, conf=0x80bf7c0, \n    server_portstr=0x9bb89167 \"\") at mod_proxy_http.c:1459\n#6  0xb7c41790 in proxy_http_handler (r=0x8b767e0, worker=0x80bfa70, \n    conf=0x80bf7c0, url=0x8709d68 \"/destraat/gfx/md_1_islam.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1715\n#7  0xb7c48a17 in proxy_run_scheme_handler (r=0x8b767e0, worker=0x80bfa70, \n    conf=0x80bf7c0, \n    url=0x8b77bbe \"http://tleac5as:8888/destraat/gfx/md_1_islam.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#8  0xb7c4a9ec in proxy_handler (r=0x8b767e0) at mod_proxy.c:739\n#9  0x08071efa in ap_run_handler (r=0x8b767e0) at config.c:157\n#10 0x08072272 in ap_invoke_handler (r=0x8b767e0) at config.c:371\n#11 0x0807ccf1 in ap_process_request (r=0x8b767e0) at http_request.c:258\n#12 0x0807a8bd in ap_process_http_connection (c=0x8709818) at http_core.c:171\n#13 0x0807789e in ap_run_process_connection (c=0x8709818) at connection.c:43\n#14 0x08080bd7 in worker_thread (thd=0x81e93b0, dummy=0x822a3d8)\n    at worker.c:531\n#15 0xb7e18760 in dummy_worker (opaque=0x81e93b0)\n    at threadproc/unix/thread.c:138\n#16 0xb7da9b80 in start_thread () from /lib/libpthread.so.0\n#17 0xb7d419ce in clone () from /lib/libc.so.6\n\nThread 2 (Thread -1732756560 (LWP 22569)):\n#0  apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:358\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n#2  0xb7f8dd6e in socket_bucket_read (a=0x968ba030, str=0x98b7ee14, \n    len=0x98b7ee18, block=APR_BLOCK_READ) at buckets/apr_buckets_socket.c:43\n#3  0x0806806d in ap_content_length_filter (f=0x96420ba0, b=0x8403890)\n    at protocol.c:1258\n#4  0x0807ee7f in ap_byterange_filter (f=0x96420b88, bb=0x8403868)\n    at byterange_filter.c:169\n#5  0xb7c6b2d8 in cache_save_filter (f=0x96421060, in=0x8403868)\n    at mod_cache.c:810\n#6  0xb7c4010c in ap_proxy_http_process_response (p=0x8402e20, r=0x9641fe30, \n    backend=0x9624518, origin=0x8709d88, conf=0x80bf7c0, \n    server_portstr=0x98b83167 \"\") at mod_proxy_http.c:1518\n#7  0xb7c41790 in proxy_http_handler (r=0x9641fe30, worker=0x80bfa70, \n    conf=0x80bf7c0, url=0x8403558 \"/leesdas/swf/reactie3.swf\", proxyname=0x0, \n    proxyport=0) at mod_proxy_http.c:1715\n#8  0xb7c48a17 in proxy_run_scheme_handler (r=0x9641fe30, worker=0x80bfa70, \n    conf=0x80bf7c0, \n    url=0x9642122e \"http://tleac5as:8888/leesdas/swf/reactie3.swf\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#9  0xb7c4a9ec in proxy_handler (r=0x9641fe30) at mod_proxy.c:739\n#10 0x08071efa in ap_run_handler (r=0x9641fe30) at config.c:157\n#11 0x08072272 in ap_invoke_handler (r=0x9641fe30) at config.c:371\n#12 0x0807ccf1 in ap_process_request (r=0x9641fe30) at http_request.c:258\n#13 0x0807a8bd in ap_process_http_connection (c=0x8403010) at http_core.c:171\n#14 0x0807789e in ap_run_process_connection (c=0x8403010) at connection.c:43\n#15 0x08080bd7 in worker_thread (thd=0x81e9470, dummy=0x822a798)\n    at worker.c:531\n#16 0xb7e18760 in dummy_worker (opaque=0x81e9470)\n    at threadproc/unix/thread.c:138\n#17 0xb7da9b80 in start_thread () from /lib/libpthread.so.0\n#18 0xb7d419ce in clone () from /lib/libc.so.6\n\nThread 1 (Thread -1211672896 (LWP 22506)):\n#0  0xffffe410 in __kernel_vsyscall ()\n#1  0xb7da9f98 in pthread_join () from /lib/libpthread.so.0\n#2  0xb7e188d1 in apr_thread_join (retval=0xbfce2d98, thd=0x81e93b0)\n    at threadproc/unix/thread.c:214\n#3  0x0808102f in join_workers (listener=0x81e94d0, threads=0x821fc60)\n    at worker.c:1079\n#4  0x080813c8 in child_main (child_num_arg=Variable \"child_num_arg\" is not\navailable.\n) at worker.c:1262\n#5  0x080814e0 in make_child (s=0x80a4d88, slot=2) at worker.c:1316\n---Type <return> to continue, or q <return> to quit---\n#6  0x08082125 in ap_mpm_run (_pconf=0x80a30a8, plog=0x80d1160, s=0x80a4d88)\n    at worker.c:1518\n#7  0x08061b50 in main (argc=3, argv=0xbfce3004) at main.c:712\n\n\n### detach sleep 10 attach ####\n\n(gdb) thread apply all bt\n\nThread 3 (Thread -1682400336 (LWP 22563)):\n#0  0xb7e0f182 in apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:343\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n#2  0xb7f8dd6e in socket_bucket_read (a=0x968b9c68, str=0x9bb84ee4, \n    len=0x9bb84ee0, block=APR_BLOCK_READ) at buckets/apr_buckets_socket.c:43\n#3  0x0807079c in ap_core_input_filter (f=0x870a1c0, b=0x870b4d0, \n    mode=AP_MODE_READBYTES, block=APR_BLOCK_READ, readbytes=8192)\n    at core_filters.c:245\n#4  0x0807d1f6 in ap_http_filter (f=0x88f8c80, b=0x870b4d0, \n    mode=AP_MODE_READBYTES, block=APR_BLOCK_READ, readbytes=8192)\n    at http_filters.c:337\n#5  0xb7c40176 in ap_proxy_http_process_response (p=0x8709628, r=0x8b767e0, \n    backend=0x9624518, origin=0x8709d88, conf=0x80bf7c0, \n    server_portstr=0x9bb89167 \"\") at mod_proxy_http.c:1459\n#6  0xb7c41790 in proxy_http_handler (r=0x8b767e0, worker=0x80bfa70, \n    conf=0x80bf7c0, url=0x8709d68 \"/destraat/gfx/md_1_islam.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1715\n#7  0xb7c48a17 in proxy_run_scheme_handler (r=0x8b767e0, worker=0x80bfa70, \n    conf=0x80bf7c0, \n    url=0x8b77bbe \"http://tleac5as:8888/destraat/gfx/md_1_islam.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#8  0xb7c4a9ec in proxy_handler (r=0x8b767e0) at mod_proxy.c:739\n#9  0x08071efa in ap_run_handler (r=0x8b767e0) at config.c:157\n#10 0x08072272 in ap_invoke_handler (r=0x8b767e0) at config.c:371\n#11 0x0807ccf1 in ap_process_request (r=0x8b767e0) at http_request.c:258\n#12 0x0807a8bd in ap_process_http_connection (c=0x8709818) at http_core.c:171\n#13 0x0807789e in ap_run_process_connection (c=0x8709818) at connection.c:43\n#14 0x08080bd7 in worker_thread (thd=0x81e93b0, dummy=0x822a3d8)\n    at worker.c:531\n#15 0xb7e18760 in dummy_worker (opaque=0x81e93b0)\n    at threadproc/unix/thread.c:138\n#16 0xb7da9b80 in start_thread () from /lib/libpthread.so.0\n#17 0xb7d419ce in clone () from /lib/libc.so.6\n\nThread 2 (Thread -1732756560 (LWP 22569)):\n#0  apr_allocator_free (allocator=0x87095a0, node=0x99001b8)\n    at memory/unix/apr_pools.c:348\n#1  0xb7f8dbb6 in apr_bucket_free (mem=0x99001e0)\n    at buckets/apr_buckets_alloc.c:182\n#2  0xb7f8dd6e in socket_bucket_read (a=0x968ba030, str=0x98b7ee14, \n    len=0x98b7ee18, block=APR_BLOCK_READ) at buckets/apr_buckets_socket.c:43\n#3  0x0806806d in ap_content_length_filter (f=0x96420ba0, b=0x8403890)\n    at protocol.c:1258\n#4  0x0807ee7f in ap_byterange_filter (f=0x96420b88, bb=0x8403868)\n    at byterange_filter.c:169\n#5  0xb7c6b2d8 in cache_save_filter (f=0x96421060, in=0x8403868)\n    at mod_cache.c:810\n#6  0xb7c4010c in ap_proxy_http_process_response (p=0x8402e20, r=0x9641fe30, \n    backend=0x9624518, origin=0x8709d88, conf=0x80bf7c0, \n    server_portstr=0x98b83167 \"\") at mod_proxy_http.c:1518\n#7  0xb7c41790 in proxy_http_handler (r=0x9641fe30, worker=0x80bfa70, \n    conf=0x80bf7c0, url=0x8403558 \"/leesdas/swf/reactie3.swf\", proxyname=0x0, \n    proxyport=0) at mod_proxy_http.c:1715\n#8  0xb7c48a17 in proxy_run_scheme_handler (r=0x9641fe30, worker=0x80bfa70, \n    conf=0x80bf7c0, \n    url=0x9642122e \"http://tleac5as:8888/leesdas/swf/reactie3.swf\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#9  0xb7c4a9ec in proxy_handler (r=0x9641fe30) at mod_proxy.c:739\n#10 0x08071efa in ap_run_handler (r=0x9641fe30) at config.c:157\n#11 0x08072272 in ap_invoke_handler (r=0x9641fe30) at config.c:371\n#12 0x0807ccf1 in ap_process_request (r=0x9641fe30) at http_request.c:258\n#13 0x0807a8bd in ap_process_http_connection (c=0x8403010) at http_core.c:171\n#14 0x0807789e in ap_run_process_connection (c=0x8403010) at connection.c:43\n#15 0x08080bd7 in worker_thread (thd=0x81e9470, dummy=0x822a798)\n    at worker.c:531\n#16 0xb7e18760 in dummy_worker (opaque=0x81e9470)\n    at threadproc/unix/thread.c:138\n#17 0xb7da9b80 in start_thread () from /lib/libpthread.so.0\n#18 0xb7d419ce in clone () from /lib/libc.so.6\n\nThread 1 (Thread -1211672896 (LWP 22506)):\n#0  0xffffe410 in __kernel_vsyscall ()\n#1  0xb7da9f98 in pthread_join () from /lib/libpthread.so.0\n#2  0xb7e188d1 in apr_thread_join (retval=0xbfce2d98, thd=0x81e93b0)\n    at threadproc/unix/thread.c:214\n#3  0x0808102f in join_workers (listener=0x81e94d0, threads=0x821fc60)\n    at worker.c:1079\n#4  0x080813c8 in child_main (child_num_arg=Variable \"child_num_arg\" is not\navailable.\n) at worker.c:1262\n#5  0x080814e0 in make_child (s=0x80a4d88, slot=2) at worker.c:1316\n#6  0x08082125 in ap_mpm_run (_pconf=0x80a30a8, plog=0x80d1160, s=0x80a4d88)\n    at worker.c:1518\n#7  0x08061b50 in main (argc=3, argv=0xbfce3004) at main.c:712\n(gdb) ", "bug_id": 39079, "is_private": false, "id": 87315, "time": "2006-03-27T14:04:03Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-27T14:04:03Z", "tags": [], "attachment_id": null}, {"count": 8, "text": "I've got another (from another apache isntance)\nApparently, there are always 2 threads running, together they eat 100% cpu.\nI presume they shouldn't both bne in the same function at the same time?\nAnd now that they are, they create problems, because they -possibly- create\ncircular linked lists ore somesuch?\n\nHere the bvacktraces for the 2 running threads:\n(gdb) thread apply 2 bt\n\nThread 2 (Thread -1724445776 (LWP 22154)):\n#0  apr_pool_cleanup_kill (p=0x82dfde0, data=0x8b89448, \n    cleanup_fn=0xb7c3c220 <connection_cleanup>) at memory/unix/apr_pools.c:1977\n#1  0xb7c3c929 in ap_proxy_release_connection (\n    proxy_function=0xb7c30b0e \"HTTP\", conn=0x8b89448, s=0x818f848)\n    at proxy_util.c:1774\n#2  0xb7c2de06 in ap_proxy_http_cleanup (scheme=0xb7c30b0e \"HTTP\", \n    r=0x84963f8, backend=0x8b89448) at mod_proxy_http.c:1586\n#3  0xb7c2ef6f in proxy_http_handler (r=0x84963f8, worker=0x81ac9f0, \n    conf=0x81ac340, url=0x8477c90 \"/clubvan100/_gfx/dotline_table.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1728\n#4  0xb7c36a17 in proxy_run_scheme_handler (r=0x84963f8, worker=0x81ac9f0, \n    conf=0x81ac340, \n    url=0x8497fce \"http://shrd01as:8888/clubvan100/_gfx/dotline_table.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#5  0xb7c389ec in proxy_handler (r=0x84963f8) at mod_proxy.c:739\n#6  0x08071efa in ap_run_handler (r=0x84963f8) at config.c:157\n#7  0x08072272 in ap_invoke_handler (r=0x84963f8) at config.c:371\n#8  0x0807ccf1 in ap_process_request (r=0x84963f8) at http_request.c:258\n#9  0x0807a8bd in ap_process_http_connection (c=0x8477740) at http_core.c:171\n#10 0x0807789e in ap_run_process_connection (c=0x8477740) at connection.c:43\n#11 0x08080bd7 in worker_thread (thd=0x810a0e0, dummy=0x81edba0)\n    at worker.c:531\n#12 0xb7e06760 in dummy_worker (opaque=0x810a0e0)\n    at threadproc/unix/thread.c:138\n#13 0xb7d97b80 in start_thread () from /lib/libpthread.so.0\n#14 0xb7d2f9ce in clone () from /lib/libc.so.6\n\n(gdb) thread apply 3 bt\n\nThread 3 (Thread -1355166800 (LWP 22110)):\n#0  apr_pool_cleanup_kill (p=0x82dfde0, data=0x8b89448, \n    cleanup_fn=0xb7c3c220 <connection_cleanup>) at memory/unix/apr_pools.c:1985\n#1  0xb7c3c929 in ap_proxy_release_connection (\n    proxy_function=0xb7c30b0e \"HTTP\", conn=0x8b89448, s=0x818f848)\n    at proxy_util.c:1774\n#2  0xb7c2de06 in ap_proxy_http_cleanup (scheme=0xb7c30b0e \"HTTP\", \n    r=0x84f40d0, backend=0x8b89448) at mod_proxy_http.c:1586\n#3  0xb7c2ef6f in proxy_http_handler (r=0x84f40d0, worker=0x81ac9f0, \n    conf=0x81ac340, url=0x82e0520 \"/clubvan100/_gfx/dotline_table.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1728\n#4  0xb7c36a17 in proxy_run_scheme_handler (r=0x84f40d0, worker=0x81ac9f0, \n    conf=0x81ac340, \n    url=0x84f5ca6 \"http://shrd01as:8888/clubvan100/_gfx/dotline_table.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#5  0xb7c389ec in proxy_handler (r=0x84f40d0) at mod_proxy.c:739\n#6  0x08071efa in ap_run_handler (r=0x84f40d0) at config.c:157\n#7  0x08072272 in ap_invoke_handler (r=0x84f40d0) at config.c:371\n#8  0x0807ccf1 in ap_process_request (r=0x84f40d0) at http_request.c:258\n#9  0x0807a8bd in ap_process_http_connection (c=0x82dffd0) at http_core.c:171\n#10 0x0807789e in ap_run_process_connection (c=0x82dffd0) at connection.c:43\n#11 0x08080bd7 in worker_thread (thd=0x8109b60, dummy=0x81e4000)\n    at worker.c:531\n#12 0xb7e06760 in dummy_worker (opaque=0x8109b60)\n    at threadproc/unix/thread.c:138\n#13 0xb7d97b80 in start_thread () from /lib/libpthread.so.0\n#14 0xb7d2f9ce in clone () from /lib/libc.so.6\n\n", "bug_id": 39079, "attachment_id": null, "id": 87320, "time": "2006-03-27T15:59:19Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-27T15:59:19Z", "tags": [], "is_private": false}, {"count": 9, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 87321, "time": "2006-03-27T16:02:49Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-27T16:02:49Z", "text": "This would appear to indicate a circular linked list in apr_pool_cleanup_kill:\n(gdb) thread apply 2 print c                       \n\nThread 2 (Thread -1724445776 (LWP 22154)):\n$18 = (cleanup_t *) 0x82e0e58\n(gdb) thread apply 2 print c->next\n\nThread 2 (Thread -1724445776 (LWP 22154)):\n$19 = (struct cleanup_t *) 0x96918540\n(gdb) thread apply 2 print c->next->next\n\nThread 2 (Thread -1724445776 (LWP 22154)):\n$20 = (struct cleanup_t *) 0x96918520\n(gdb) thread apply 2 print c->next->next->next\n\nThread 2 (Thread -1724445776 (LWP 22154)):\n$21 = (struct cleanup_t *) 0x82e0e58\n\nFurthermore: theread 2 and thread 3 appear to be ding the same thing?\n(gdb) thread apply 2 print p                  \n\nThread 2 (Thread -1724445776 (LWP 22154)):\n$22 = (apr_pool_t *) 0x82dfde0\n(gdb) thread apply 2 print data\n\nThread 2 (Thread -1724445776 (LWP 22154)):\n$23 = (const void *) 0x8b89448\n(gdb) thread apply 3 print data\n\nThread 3 (Thread -1355166800 (LWP 22110)):\n$24 = (const void *) 0x8b89448\n(gdb) thread apply 3 print p   \n\nThread 3 (Thread -1355166800 (LWP 22110)):\n$25 = (apr_pool_t *) 0x82dfde0\n\n(I'm not too sure, because I don't use gdb very often. Especially not on\nthreaded applications)"}, {"count": 10, "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "id": 87338, "time": "2006-03-27T22:17:42Z", "bug_id": 39079, "creation_time": "2006-03-27T22:17:42Z", "is_private": false, "text": "First of all many thanks for the output. Whats currently confusing me is that\nthe thread that consumes the CPU is non of the ones that are in the same function.\nBasicly I agree that it seems to be a dangerous thing to have two threads\nworking on the same data structures at the same time. This is waiting for an\naccident to happen. I think this is caused by the fact that two threads use the\nsame worker (see worker at #4) This should not happen.\nA similar problem was already discovered in PR#38793. Could you please check if\nthe patch attached to PR#38793 fixes your problem?\n"}, {"count": 11, "text": "PR#38793 appears relevant to our situation.\n\nI compiled another apache. I've never compiled trunk versions before, so here's\nwhat I did; please let me know if it is incorrect:\n$ wget http://issues.apache.org/bugzilla/attachment.cgi?id=17874\n$ svn checkout http://svn.apache.org/repos/asf/httpd/httpd/branches/2.2.x httpd-2.2\n$ cd httpd-2.2\n$ svn co http://svn.apache.org/repos/asf/apr/apr/branches/1.2.x srclib/apr\n$ svn co http://svn.apache.org/repos/asf/apr/apr-util/branches/1.2.x srclib/apr-util\n$ patch -p0 <~/attachment.cgi\\?id=17874   (gave some fuzz)\n$ ./buildconf\n$ ./configure ......\n\nHowever, this did not appear to fix the problem.\nI've got another apache eating 99% CPU:\n\nThe LWP's eating all the CPU are 4974 and 4976:\nThread 3 (Thread -1396925520 (LWP 4974)):\n#0  0x08070635 in ap_core_input_filter (f=0x82c0b70, b=0x96a9c070, \n    mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0)\n    at core_filters.c:141\n#1  0x0806696e in ap_rgetline_core (s=0xacbc4fa8, n=8192, read=0xacbc4fa4, \n    r=0x82c1050, fold=0, bb=0x96a9c070) at protocol.c:222\n#2  0x08066ce1 in ap_getline (s=0xacbc7038 \"\", n=8192, r=0x82c1050, fold=0)\n    at protocol.c:463\n#3  0xb7c5def7 in ap_proxy_http_process_response (p=0x82bffc0, r=0x96a99910, \n    backend=0x822b3f8, origin=0x82c0738, conf=0x80bf888, \n    server_portstr=0xacbc9167 \"\") at mod_proxy_http.c:1217\n#4  0xb7c5f764 in proxy_http_handler (r=0x96a99910, worker=0x80bfb38, \n    conf=0x80bf888, url=0x82c0708 \"/hbb/gfx/archief/icons/34_gebedshuis.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1713\n#5  0xb7c66a53 in proxy_run_scheme_handler (r=0x96a99910, worker=0x80bfb38, \n    conf=0x80bf888, \n    url=0x96a9ad7e\n\"http://tleac5as:8888/hbb/gfx/archief/icons/34_gebedshuis.gif\", proxyhost=0x0,\nproxyport=0) at mod_proxy.c:1936\n#6  0xb7c68a28 in proxy_handler (r=0x96a99910) at mod_proxy.c:739\n#7  0x08071f56 in ap_run_handler (r=0x96a99910) at config.c:157\n#8  0x080722ce in ap_invoke_handler (r=0x96a99910) at config.c:371\n#9  0x0807cd4d in ap_process_request (r=0x96a99910) at http_request.c:258\n#10 0x0807a919 in ap_process_http_connection (c=0x82c01b0) at http_core.c:171\n#11 0x080778fa in ap_run_process_connection (c=0x82c01b0) at connection.c:43\n#12 0x08080d03 in worker_thread (thd=0x81e6338, dummy=0x821d0a8)\n    at worker.c:531\n#13 0xb7e348ec in dummy_worker (opaque=0x81e6338)\n    at threadproc/unix/thread.c:138\n#14 0xb7dc7b80 in start_thread () from /lib/libpthread.so.0\n#15 0xb7d5e9ce in clone () from /lib/libc.so.6\n\nThread 2 (Thread -1413710928 (LWP 4976)):\n#0  0x0807061b in ap_core_input_filter (f=0x82c0b70, b=0x96a9c080, \n    mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0)\n    at core_filters.c:141\n#1  0x0806696e in ap_rgetline_core (s=0xabbc2fa8, n=8192, read=0xabbc2fa4, \n    r=0x82c0eb0, fold=0, bb=0x96a9c080) at protocol.c:222\n#2  0x08066ce1 in ap_getline (s=0xabbc5038 \"\", n=8192, r=0x82c0eb0, fold=0)\n    at protocol.c:463\n#3  0xb7c5def7 in ap_proxy_http_process_response (p=0x96af2a48, r=0x83f02e8, \n    backend=0x822b3f8, origin=0x82c0738, conf=0x80bf888, \n    server_portstr=0xabbc7167 \"\") at mod_proxy_http.c:1217\n#4  0xb7c5f764 in proxy_http_handler (r=0x83f02e8, worker=0x80bfb38, \n    conf=0x80bf888, url=0x96af3188 \"/hbb/gfx/archief/icons/32_media.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1713\n#5  0xb7c66a53 in proxy_run_scheme_handler (r=0x83f02e8, worker=0x80bfb38, \n    conf=0x80bf888, \n    url=0x83f170e \"http://tleac5as:8888/hbb/gfx/archief/icons/32_media.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#6  0xb7c68a28 in proxy_handler (r=0x83f02e8) at mod_proxy.c:739\n#7  0x08071f56 in ap_run_handler (r=0x83f02e8) at config.c:157\n#8  0x080722ce in ap_invoke_handler (r=0x83f02e8) at config.c:371\n#9  0x0807cd4d in ap_process_request (r=0x83f02e8) at http_request.c:258\n#10 0x0807a919 in ap_process_http_connection (c=0x96af2c38) at http_core.c:171\n#11 0x080778fa in ap_run_process_connection (c=0x96af2c38) at connection.c:43\n#12 0x08080d03 in worker_thread (thd=0x81e6378, dummy=0x821df50)\n    at worker.c:531\n#13 0xb7e348ec in dummy_worker (opaque=0x81e6378)\n    at threadproc/unix/thread.c:138\n#14 0xb7dc7b80 in start_thread () from /lib/libpthread.so.0\n#15 0xb7d5e9ce in clone () from /lib/libc.so.6\n\nThread 1 (Thread -1211549216 (LWP 4951)):\n#0  0xffffe410 in ?? ()\n#1  0xbfa08e38 in ?? ()\n#2  0x0000136e in ?? ()\n#3  0x00000000 in ?? ()\n", "bug_id": 39079, "is_private": false, "id": 87394, "time": "2006-03-29T14:57:46Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-29T14:57:46Z", "tags": [], "attachment_id": null}, {"count": 12, "tags": [], "bug_id": 39079, "attachment_id": null, "id": 87400, "time": "2006-03-29T16:13:41Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-29T16:13:41Z", "is_private": false, "text": "Here another one, from another webserver instance:\n(gdb) thread 3\n[Switching to thread 3 (Thread -1295930448 (LWP 22476))]#0 \napr_pool_cleanup_kill (p=0x8239d50, data=0x8203648, cleanup_fn=0xb7cad25c\n<connection_cleanup>)\n    at memory/unix/apr_pools.c:1984\n1984            if (c->data == data && c->plain_cleanup_fn == cleanup_fn) {\n(gdb) thread 4\n[Switching to thread 4 (Thread -1253966928 (LWP 22471))]#0  0xb7e6cf33 in\napr_pool_cleanup_kill (p=0x8239d50, data=0x96a37cb0, \n    cleanup_fn=0xb7ef55c0 <brigade_cleanup>) at memory/unix/apr_pools.c:1984\n1984            if (c->data == data && c->plain_cleanup_fn == cleanup_fn) {\n\n(gdb) thread apply 3 bt  \n\nThread 3 (Thread -1295930448 (LWP 22476)):\n#0  apr_pool_cleanup_kill (p=0x8239d50, data=0x8203648, \n    cleanup_fn=0xb7cad25c <connection_cleanup>) at memory/unix/apr_pools.c:1984\n#1  0xb7cad9ad in ap_proxy_release_connection (\n    proxy_function=0xb7ca1ae6 \"HTTP\", conn=0x8203648, s=0x819b0a8)\n    at proxy_util.c:1788\n#2  0xb7c9ee06 in ap_proxy_http_cleanup (scheme=0xb7ca1ae6 \"HTTP\", \n    r=0x968044d0, backend=0x8203648) at mod_proxy_http.c:1584\n#3  0xb7c9ff43 in proxy_http_handler (r=0x968044d0, worker=0x8201b28, \n    conf=0x82015f8, url=0x96a36600 \"/frontend/images/tree-leaf.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1726\n#4  0xb7ca7a53 in proxy_run_scheme_handler (r=0x968044d0, worker=0x8201b28, \n    conf=0x82015f8, \n    url=0x968058f6 \"http://bnn02as:8888/frontend/images/tree-leaf.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#5  0xb7ca9a28 in proxy_handler (r=0x968044d0) at mod_proxy.c:739\n#6  0x08071f56 in ap_run_handler (r=0x968044d0) at config.c:157\n#7  0x080722ce in ap_invoke_handler (r=0x968044d0) at config.c:371\n#8  0x0807cd4d in ap_process_request (r=0x968044d0) at http_request.c:258\n#9  0x0807a919 in ap_process_http_connection (c=0x8239f40) at http_core.c:171\n#10 0x080778fa in ap_run_process_connection (c=0x8239f40) at connection.c:43\n#11 0x08080d03 in worker_thread (thd=0x81be0e0, dummy=0x81f5830)\n    at worker.c:531\n#12 0xb7e758ec in dummy_worker (opaque=0x81be0e0)\n    at threadproc/unix/thread.c:138\n#13 0xb7e08b80 in start_thread () from /lib/libpthread.so.0\n#14 0xb7d9f9ce in clone () from /lib/libc.so.6\n\n(gdb) thread apply 4 bt\n\nThread 4 (Thread -1253966928 (LWP 22471)):\n#0  0xb7e6cf33 in apr_pool_cleanup_kill (p=0x8239d50, data=0x96a37cb0, \n    cleanup_fn=0xb7ef55c0 <brigade_cleanup>) at memory/unix/apr_pools.c:1984\n#1  0xb7ef5604 in apr_brigade_destroy (b=0x96a37cb0)\n    at buckets/apr_brigade.c:52\n#2  0x08066cec in ap_getline (\n    s=0xb541d038 \"HTTP/1.1 404 /frontend/images/tree-folder-open.gif\", n=8192, \n    r=0x96a36d48, fold=0) at protocol.c:464\n#3  0xb7c9eef7 in ap_proxy_http_process_response (p=0x96902878, r=0x96a59448, \n    backend=0x8203648, origin=0x96a36620, conf=0x82015f8, \n    server_portstr=0xb541f167 \"\") at mod_proxy_http.c:1217\n#4  0xb7ca0764 in proxy_http_handler (r=0x96a59448, worker=0x8201b28, \n    conf=0x82015f8, url=0x96a29e68 \"/frontend/images/tree-folder-open.gif\", \n    proxyname=0x0, proxyport=0) at mod_proxy_http.c:1713\n#5  0xb7ca7a53 in proxy_run_scheme_handler (r=0x96a59448, worker=0x8201b28, \n    conf=0x82015f8, \n    url=0x96a5a8c6 \"http://bnn02as:8888/frontend/images/tree-folder-open.gif\", \n    proxyhost=0x0, proxyport=0) at mod_proxy.c:1936\n#6  0xb7ca9a28 in proxy_handler (r=0x96a59448) at mod_proxy.c:739\n#7  0x08071f56 in ap_run_handler (r=0x96a59448) at config.c:157\n#8  0x080722ce in ap_invoke_handler (r=0x96a59448) at config.c:371\n#9  0x0807cd4d in ap_process_request (r=0x96a59448) at http_request.c:258\n#10 0x0807a919 in ap_process_http_connection (c=0x96902a68) at http_core.c:171\n#11 0x080778fa in ap_run_process_connection (c=0x96902a68) at connection.c:43\n#12 0x08080d03 in worker_thread (thd=0x81be040, dummy=0x81f5510)\n    at worker.c:531\n#13 0xb7e758ec in dummy_worker (opaque=0x81be040)\n    at threadproc/unix/thread.c:138\n#14 0xb7e08b80 in start_thread () from /lib/libpthread.so.0\n#15 0xb7d9f9ce in clone () from /lib/libc.so.6\n"}, {"count": 13, "tags": [], "bug_id": 39079, "attachment_id": null, "id": 87404, "time": "2006-03-29T21:34:26Z", "creator": "rpluem@apache.org", "creation_time": "2006-03-29T21:34:26Z", "is_private": false, "text": "(In reply to comment #11)\n\n> I compiled another apache. I've never compiled trunk versions before, so here's\n> what I did; please let me know if it is incorrect:\n> $ wget http://issues.apache.org/bugzilla/attachment.cgi?id=17874\n> $ svn checkout http://svn.apache.org/repos/asf/httpd/httpd/branches/2.2.x\nhttpd-2.2\n> $ cd httpd-2.2\n> $ svn co http://svn.apache.org/repos/asf/apr/apr/branches/1.2.x srclib/apr\n> $ svn co http://svn.apache.org/repos/asf/apr/apr-util/branches/1.2.x\nsrclib/apr-util\n> $ patch -p0 <~/attachment.cgi\\?id=17874   (gave some fuzz)\n> $ ./buildconf\n> $ ./configure ......\n\nThis is fine, but you do not need to pull apr, apr-util and httpd from\nsubversion in this case. You can try to apply the patch to the untared\nhttpd-2.2.0.tar.gz.\n"}, {"count": 14, "tags": [], "text": "(In reply to comment #12)\n\nI think a weird data corruption seems to have happened:\n\nAs far as I understand the code p in #0 should have the same value as p in #3.\n\n> \n> Thread 4 (Thread -1253966928 (LWP 22471)):\n> #0  0xb7e6cf33 in apr_pool_cleanup_kill (p=0x8239d50, data=0x96a37cb0, \n>     cleanup_fn=0xb7ef55c0 <brigade_cleanup>) at memory/unix/apr_pools.c:1984\n> #1  0xb7ef5604 in apr_brigade_destroy (b=0x96a37cb0)\n>     at buckets/apr_brigade.c:52\n> #2  0x08066cec in ap_getline (\n>     s=0xb541d038 \"HTTP/1.1 404 /frontend/images/tree-folder-open.gif\", n=8192, \n>     r=0x96a36d48, fold=0) at protocol.c:464\n> #3  0xb7c9eef7 in ap_proxy_http_process_response (p=0x96902878, r=0x96a59448, \n>     backend=0x8203648, origin=0x96a36620, conf=0x82015f8, \n>     server_portstr=0xb541f167 \"\") at mod_proxy_http.c:1217\n\nSomeone else an eye on this? Is it just me or is it really that badly corrupted?", "attachment_id": null, "id": 87405, "creator": "rpluem@apache.org", "time": "2006-03-29T21:38:35Z", "bug_id": 39079, "creation_time": "2006-03-29T21:38:35Z", "is_private": false}, {"count": 15, "tags": [], "creator": "Dick.Snippe@tech.omroep.nl", "attachment_id": null, "text": " \n> This is fine, but you do not need to pull apr, apr-util and httpd from\n> subversion in this case. You can try to apply the patch to the untared\n> httpd-2.2.0.tar.gz.\n\nThe patch does not apply cleanly against 2.2.0: \npatching file modules/proxy/proxy_util.c\nHunk #1 succeeded at 1510 (offset -2 lines).\nHunk #3 succeeded at 1562 (offset -2 lines).\nHunk #4 succeeded at 1773 (offset -24 lines).\npatching file modules/proxy/mod_proxy_http.c\nReversed (or previously applied) patch detected!  Assume -R? [n] \n", "id": 87406, "time": "2006-03-29T23:03:25Z", "bug_id": 39079, "creation_time": "2006-03-29T23:03:25Z", "is_private": false}, {"count": 16, "tags": [], "bug_id": 39079, "attachment_id": null, "id": 87407, "time": "2006-03-29T23:12:30Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-03-29T23:12:30Z", "is_private": false, "text": "(In reply to comment #14) \n> Someone else an eye on this? Is it just me or is it really that badly corrupted?\n\nPerhaps I should also mention that we see Segmentation faults in the errorlog\nregularly: (+/- 100 per day)\n$ zgrep 'Segmentation fault' errors.20060329.gz |wc -l\n95\n\nSo there appears to be some data corruption"}, {"count": 17, "tags": [], "bug_id": 39079, "attachment_id": null, "id": 87410, "time": "2006-03-30T09:14:05Z", "creator": "jorton@redhat.com", "creation_time": "2006-03-30T09:14:05Z", "is_private": false, "text": "Some things which might be useful to try:\n\n1) try a fresh build passing --enable-pool-debug to configure; using the 2.2.x\nbranch is certainly a sensible idea here.\n2) capture some core dumps and backtraces from the segfaults; these might be\nmore enlightening than the hangs\n3) run some builds with glibc malloc checking enabled too; set\n\n  export MALLOC_CHECK_=2\n\nbefore starting the server."}, {"count": 18, "tags": [], "creator": "Dick.Snippe@tech.omroep.nl", "attachment_id": null, "id": 88788, "time": "2006-05-03T21:10:21Z", "bug_id": 39079, "creation_time": "2006-05-03T21:10:21Z", "is_private": false, "text": "Today I tried apache-2.2.0\nI haven't seen any hangs yet, but I collected ~20 coredumps in 10 hours.\n\nAll coredumps show more or less the same when doing \"thread apply all bt\",\nviz:\n* always ~64 threads (because ThreadsPerChild==64)\n\n* many threads report a null pointer\nThread 3 (process 2636):\n#0  0x00000216 in ?? ()\nCannot access memory at address 0x0\n\n* others report all the same address (0x4067 in this case)\nThread 5 (process 2634):\n#0  0x00000216 in ?? ()\nCannot access memory at address 0x4067\n\n* thread 1 always reports\nThread 1 (process 2571):\n#0  0xffffe410 in __kernel_vsyscall ()\n#1  0xb7e5b21b in __read_nocancel () from /lib/libpthread.so.0\n#2  0x08083365 in ap_mpm_pod_check (pod=0x815e140) at pod.c:54\n#3  0x0808161f in child_main (child_num_arg=0) at worker.c:1233\n#4  0x08081774 in make_child (s=0x80a4f48, slot=0) at worker.c:1316\n#5  0x080823b9 in ap_mpm_run (_pconf=0x80a30a8, plog=0x80d1160, s=0x80a4f48)\n    at worker.c:1518\n#6  0x08061c2c in main (argc=3, argv=0xbfe96574) at main.c:717\n\nI'll try --enable-pool-debug and MALLOC_CHECK_=2\nand see what happens..."}, {"count": 19, "tags": [], "bug_id": 39079, "attachment_id": null, "id": 88801, "time": "2006-05-04T08:52:54Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-05-04T08:52:54Z", "is_private": false, "text": "(In reply to comment #18)\n> I'll try --enable-pool-debug and MALLOC_CHECK_=2\n> and see what happens...\n\nI'v got one instance running with --enable-pool-debug, Unfortunately when it\ndumped core, it also garbled the error log. There's a hole in the error log, and\na garbled line:\n[Thu May 04 10:20:06 2006] [info] mem_cache: Cached url:\nhttp://beeldbank.schooltv.nl:80/peuterskleuters/mmbase/attachments/4591?\napr_table_addn: key not in ancestor pool of t\n[Thu May 04 10:27:25 2006] [info] removed PID file /e/fp/teleac/log/httpd2.pid\n(pid=12045)\n[Thu May 04 10:27:25 2006] [notice] caught SIGTERM, shutting down\n[Thu May 04 10:27:50 2006] [notice] Apache/2.2.2 (Unix) configured -- resuming\nnormal operations\n\nThe stack trace from the coredump is more or less the same: many threads without\na stacktrace:\nThread 7 (process 17594):\n#0  0x00000202 in ?? ()\nCannot access memory at address 0x702f6e69\n\nThread 6 (process 17595):\n#0  0x00000246 in ?? ()\nCannot access memory at address 0x0\n\nonly thread1 has a stacktrace:\nThread 1 (process 17533):\n#0  0xffffe410 in __kernel_vsyscall ()\n#1  0xb7e9721b in __read_nocancel () from /lib/libpthread.so.0\n#2  0x08083521 in ap_mpm_pod_check (pod=0x816a340) at pod.c:54\n#3  0x080817c7 in child_main (child_num_arg=0) at worker.c:1233\n#4  0x0808191c in make_child (s=0x8168b20, slot=0) at worker.c:1316\n#5  0x080819bb in startup_children (number_to_start=1) at worker.c:1350\n#6  0x08082133 in ap_mpm_run (_pconf=0x809d778, plog=0x80a68c0, s=0x8168b20)\n    at worker.c:1612\n#7  0x08061e04 in main (argc=3, argv=0xbfbd1f74) at main.c:717\n\nI'll try to catch another coredump.\n\nAnother instance is running with MALLOC_CHECK_=2, this instance reports\n[Thu May 04 00:34:21 2006] [notice] child pid 12514 exit signal Segmentation fau\nlt (11), possible coredump in /e/fp/shrd01/tmp\n\n(mind you: a Segmentation fault and not Aborted, so MALLOC_CHECK_=2 didn't do\nanything in this case, i.e. the corruption might not be malloc related)\n\nThe stacktrace is the same\nThread 4 (process 13082):\n#0  0x00000206 in ?? ()\nCannot access memory at address 0x0\n\nThread 3 (process 13083):\n#0  0x00000206 in ?? ()\nCannot access memory at address 0x4067\n\nThread 2 (process 13084):\n#0  0x00000293 in ?? ()\nCannot access memory at address 0x4067\n\nThread 1 (process 13018):\n#0  0xffffe410 in __kernel_vsyscall ()\n#1  0xb7ead21b in __read_nocancel () from /lib/libpthread.so.0\n#2  0x08083365 in ap_mpm_pod_check (pod=0x80c0bb8) at pod.c:54\n#3  0x0808161f in child_main (child_num_arg=0) at worker.c:1233\n#4  0x08081774 in make_child (s=0x80a4f48, slot=0) at worker.c:1316\n#5  0x080823b9 in ap_mpm_run (_pconf=0x80a30a8, plog=0x80d1160, s=0x80a4f48)\n    at worker.c:1518\n#6  0x08061c2c in main (argc=3, argv=0xbffe82d4) at main.c:717\n"}, {"count": 20, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 88899, "time": "2006-05-08T09:35:57Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2006-05-08T09:35:57Z", "text": "(In reply to comment #19)\n> I'v got one instance running with --enable-pool-debug, Unfortunately when it\n> dumped core, it also garbled the error log. There's a hole in the error log, and\n> a garbled line:\n> apr_table_addn: key not in ancestor pool of t\n\nIn the mean time I've collected hundreds of coredumps. All coredumps coincide\nwith the same line:\napr_table_addn: key not in ancestor pool of t\n[Sun May 07 09:33:53 2006] [notice] child pid 3816 exit signal Aborted (6),\npossible coredump in /d/0/tmp/teleacfp\n\n(...reads source...) Aha not a garbled line at all, just something written to\nstderr:\n#if APR_POOL_DEBUG\n    {\n        if (!apr_pool_is_ancestor(apr_pool_find(key), t->a.pool)) {\n            fprintf(stderr, \"apr_table_addn: key not in ancestor pool of t\\n\");\n            abort();\n        }\n        if (!apr_pool_is_ancestor(apr_pool_find(val), t->a.pool)) {\n            fprintf(stderr, \"apr_table_addn: key not in ancestor pool of t\\n\");\n            abort();\n        }\n    }\n#endif\n\n"}, {"count": 21, "tags": [], "bug_id": 39079, "attachment_id": null, "id": 115753, "time": "2008-04-21T12:06:54Z", "creator": "richard_hubbe11@yahoo.com", "creation_time": "2008-04-21T12:06:54Z", "is_private": false, "text": "Has this been abandonded?  I'm not a seasoned bug-reporter or reader.\n\nSeeing this issue with 2.2.8 and worker MPM.\n\nAm running at ~200 xactions per second\n\nIf I strace -fF -p pid\nit shows that it looks to be stuck in futex.\n\nAfter strace it clears up the problem of consuming 100% of the cpu.\n\nI will probably revert to using pre-fork, although mpm worker was looking nice.\nThis problem prevents using worker."}, {"count": 22, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 115783, "time": "2008-04-22T17:41:49Z", "creator": "richard_hubbe11@yahoo.com", "creation_time": "2008-04-22T17:41:49Z", "text": "I don't know if this is relevant to this issue or not.  I have also seen cases where there is the long-lived parent process running as root and another child of that that is very long-lived but should get recycled as the other child procs do.\nI was unable to get any info from that process."}, {"count": 23, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 115798, "time": "2008-04-23T04:46:20Z", "creator": "Dick.Snippe@tech.omroep.nl", "creation_time": "2008-04-23T04:46:20Z", "text": "(In reply to comment #21)\n> Has this been abandonded?  I'm not a seasoned bug-reporter or reader.\n\nWe switched to prefork MPM; in the mean time not much has changed.\n \n> Seeing this issue with 2.2.8 and worker MPM.\n\nAh, so the issue still exists in 2.2.8 :-(\nI'll see if we can upgrade to 2.2.8 to check if we still experience the same problem.\n\n> I will probably revert to using pre-fork, although mpm worker was looking nice.\n> This problem prevents using worker.\n\nsame here\n\n"}, {"count": 24, "tags": [], "text": "\nI'm having the exact same problem, 100% CPU on futex_wait, few times a week. Not a very busy server, 100k requests a day.\n\nhttpd-2.2.11, no cache/proxy, just php5+apc, --with-included-apr --with-mpm=worker\n\nWhile not a dealbreaker (made a script to kill such process), it's somewhat annoying. Sadly I'm not experienced debugger either.", "attachment_id": null, "id": 126208, "creator": "hege@hege.li", "time": "2009-04-13T23:30:22Z", "bug_id": 39079, "creation_time": "2009-04-13T23:30:22Z", "is_private": false}, {"count": 25, "tags": [], "text": "Same problem detected in version 2.2.11.   Happens everyday on an 8-cpu centos .  Sometimes eatup 4 cups by 100%, while sometimes 8.   \nI was wondering whether it's because there is a loop in the linked list p->cleanups and the length of the loop is bigger than 4, so that it's not detected by the corrupt list detection mechanism? Or is that not possible?\n\n\nSource below is part of apr_pool_cleanup_kill(all my  threads with 100% CPU are detected running this function ):\n\nwhile (c) {\n#if APR_POOL_DEBUG\n        /* Some cheap loop detection to catch a corrupt list: */\n        if (c == c->next\n            || (c->next && c == c->next->next)\n            || (c->next && c->next->next && c == c->next->next->next)) {\n            abort();\n        }\n#endif\n\n        if (c->data == data && c->plain_cleanup_fn == cleanup_fn) {\n            *lastp = c->next;\n            /* move to freelist */\n            c->next = p->free_cleanups;\n            p->free_cleanups = c;\n            break;\n        }\n\n        lastp = &c->next;\n        c = c->next;\n    }", "attachment_id": null, "id": 129577, "creator": "questionii@yahoo.cn", "time": "2009-08-09T10:57:45Z", "bug_id": 39079, "creation_time": "2009-08-09T10:57:45Z", "is_private": false}, {"count": 26, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 129582, "time": "2009-08-09T17:02:39Z", "creator": "questionii@yahoo.cn", "creation_time": "2009-08-09T17:02:39Z", "text": "I found bug 36324, and the \n        if (c == c->next\n            || (c->next && c == c->next->next)\n            || (c->next && c->next->next && c == c->next->next->next)) {\n            abort();\n        }\npart of the code seems to be added for that bug, but I think that maybe not enough...\nWhat if we add a counter to the loop, and if the length of the loop reaches 1 millon, then just abort()?"}, {"count": 27, "tags": [], "creator": "questionii@yahoo.cn", "is_private": false, "text": "http://mail-archives.apache.org/mod_mbox/httpd-bugs/200508.mbox/%3C20050823164004.8C82C124@ajax.apache.org%3E\n\nthis seems to be also related to this bug, and seems to explain why this is happening.\nIn our case, we have 5000 sockets at the same time for about 4-5 hours a day, so 2 threads calling this function at the same time is quite reasonable...", "id": 129583, "time": "2009-08-09T17:18:33Z", "bug_id": 39079, "creation_time": "2009-08-09T17:18:33Z", "attachment_id": null}, {"count": 28, "tags": [], "text": "sorry, I sent the wrong email, the link should be this\n\nhttp://mail-archives.apache.org/mod_mbox/httpd-dev/200507.mbox/%3C20050722010051.1634.qmail@web30512.mail.mud.yahoo.com%3E\n\n\n(In reply to comment #27)\n> http://mail-archives.apache.org/mod_mbox/httpd-bugs/200508.mbox/%3C20050823164004.8C82C124@ajax.apache.org%3E\n> \n> this seems to be also related to this bug, and seems to explain why this is\n> happening.\n> In our case, we have 5000 sockets at the same time for about 4-5 hours a day,\n> so 2 threads calling this function at the same time is quite reasonable...", "attachment_id": null, "id": 129584, "creator": "questionii@yahoo.cn", "time": "2009-08-09T17:22:49Z", "bug_id": 39079, "creation_time": "2009-08-09T17:22:49Z", "is_private": false}, {"count": 29, "tags": [], "creator": "patrick133t@yahoo.com", "attachment_id": null, "text": "I'm also seeing a problem that I believe is the same as this one. I think this may be related to bug 44402, but I am not sure and have not been able to apply the patch yet because of fear of losing support from our enterprise distribution vendor. The later comments by Henrik K and Huo Mingyu about the problem persisting in 2.2.11 make me think it may be separate from 44402, but since they did not post additional information, I am not sure that they are really seeing this problem or perhaps a different one.\n\nI am running the Red Hat httpd-2.2.3-22.el5 version on two servers that jointly serve around 8 million hits per day. I see around 15 segfaults per day, and so far I've seen two processes enter and endless loop in the two weeks I have been using the worker MPM.\n\nI am using the following modules:\n\nLoadModule alias_module modules/mod_alias.so\nLoadModule authz_host_module modules/mod_authz_host.so\nLoadModule deflate_module modules/mod_deflate.so\nLoadModule expires_module modules/mod_expires.so\nLoadModule headers_module modules/mod_headers.so\nLoadModule jk_module modules/mod_jk.so\nLoadModule log_config_module modules/mod_log_config.so\nLoadModule mime_module modules/mod_mime.so\nLoadModule proxy_http_module modules/mod_proxy_http.so\nLoadModule proxy_module modules/mod_proxy.so\nLoadModule rewrite_module modules/mod_rewrite.so\nLoadModule security2_module modules/mod_security2.so\nLoadModule setenvif_module modules/mod_setenvif.so\nLoadModule ssl_module modules/mod_ssl.so\nLoadModule status_module modules/mod_status.so\nLoadModule unique_id_module modules/mod_unique_id.so\n\nI have not used the log_forensic_module to get better reporting on the segfaults, but the two hung processes (three looping threads total) have all been serving requests in a space that is reverse proxied through mod_proxy_http with rewrite rules.\n\nThe unusual modules of note that I am loading are mod_security2 and mod_jk. Both of those were compiled in-house, and are versions 2.5.9 and 1.2.28, respectively.\n\nI have not captured any core files yet, but have taken stack traces from the two processes in the endless loop. I used \"gstack <pid>\" to get the traces. I took three with a pause of a few seconds between each. The complete stack traces follow (there really were only 3 and 4 threads total), along with a unified diff showing what changed between the three traces I took:\n\nWeek 2 trace:\n\nThread 3 (Thread 0x428fc940 (LWP 6559)):\n#0  0x00002ae823ee85f2 in select () from /lib64/libc.so.6\n#1  0x00002ae8237f46d5 in apr_sleep () from /usr/lib64/libapr-1.so.0\n#2  0x00002ae8278dcd6f in jk_watchdog_func ()\n#3  0x00002ae823a02367 in start_thread () from /lib64/libpthread.so.0\n#4  0x00002ae823eef0ad in clone () from /lib64/libc.so.6\nThread 2 (Thread 0x4fb11940 (LWP 6581)):\n#0  0x00002ae821f1da9f in ap_core_input_filter () from /proc/6558/exe\n#1  0x00002ae821f2d90a in ap_http_filter () from /proc/6558/exe\n#2  0x00002ae827f4300a in ?? ()\n#3  0x00002ae827f44f16 in ?? ()\n#4  0x00002ae827d31cb1 in proxy_run_scheme_handler ()\n#5  0x00002ae827d357f3 in ?? ()\n#6  0x00002ae821f1e38a in ap_run_handler () from /proc/6558/exe\n#7  0x00002ae821f21802 in ap_invoke_handler () from /proc/6558/exe\n#8  0x00002ae821f2bfc8 in ap_process_request () from /proc/6558/exe\n#9  0x00002ae821f29200 in ?? () from /proc/6558/exe\n#10 0x00002ae821f255f2 in ap_run_process_connection () from /proc/6558/exe\n#11 0x00002ae821f30a37 in ?? () from /proc/6558/exe\n#12 0x00002ae823a02367 in start_thread () from /lib64/libpthread.so.0\n#13 0x00002ae823eef0ad in clone () from /lib64/libc.so.6\nThread 1 (Thread 0x2ae826528560 (LWP 6558)):\n#0  0x00002ae823a03655 in pthread_join () from /lib64/libpthread.so.0\n#1  0x00002ae8237f3d85 in apr_thread_join () from /usr/lib64/libapr-1.so.0\n#2  0x00002ae821f301b0 in ?? () from /proc/6558/exe\n#3  0x00002ae821f30d79 in ?? () from /proc/6558/exe\n#4  0x00002ae821f30ef5 in ?? () from /proc/6558/exe\n#5  0x00002ae821f3173b in ap_mpm_run () from /proc/6558/exe\n#6  0x00002ae821f0b7e0 in main () from /proc/6558/exe\n\nWeek 2 diff:\n\n--- 6558.1\t2009-10-06 14:37:51.000000000 -0600\n+++ 6558.2\t2009-10-06 14:37:51.000000000 -0600\n@@ -5,7 +5,7 @@\n #3  0x00002ae823a02367 in start_thread () from /lib64/libpthread.so.0\n #4  0x00002ae823eef0ad in clone () from /lib64/libc.so.6\n Thread 2 (Thread 0x4fb11940 (LWP 6581)):\n-#0  0x00002ae821f1da9f in ap_core_input_filter () from /proc/6558/exe\n+#0  0x00002ae821f1daab in ap_core_input_filter () from /proc/6558/exe\n #1  0x00002ae821f2d90a in ap_http_filter () from /proc/6558/exe\n #2  0x00002ae827f4300a in ?? ()\n #3  0x00002ae827f44f16 in ?? ()\n\nThe third trace was identical to the second.\n\nWeek 1 trace:\n\nThread 4 (Thread 0x4142a940 (LWP 7213)):\n#0  0x00002b593d3ea5f2 in select () from /lib64/libc.so.6\n#1  0x00002b593ccf66d5 in apr_sleep () from /usr/lib64/libapr-1.so.0\n#2  0x00002b5940dded6f in jk_watchdog_func ()\n#3  0x00002b593cf04367 in start_thread () from /lib64/libpthread.so.0\n#4  0x00002b593d3f10ad in clone () from /lib64/libc.so.6\nThread 3 (Thread 0x4282c940 (LWP 7215)):\n#0  0x00002b593cceca80 in apr_allocator_free () from /usr/lib64/libapr-1.so.0\n#1  0x00002b593bf29c57 in ?? () from /usr/lib64/libaprutil-1.so.0\n#2  0x00002b593bf2a7c0 in apr_brigade_cleanup ()\n#3  0x00002b593b41f307 in ap_core_output_filter () from /proc/7212/exe\n#4  0x00002b593b42df0d in ap_process_request () from /proc/7212/exe\n#5  0x00002b593b42b200 in ?? () from /proc/7212/exe\n#6  0x00002b593b4275f2 in ap_run_process_connection () from /proc/7212/exe\n#7  0x00002b593b432a37 in ?? () from /proc/7212/exe\n#8  0x00002b593cf04367 in start_thread () from /lib64/libpthread.so.0\n#9  0x00002b593d3f10ad in clone () from /lib64/libc.so.6\nThread 2 (Thread 0x43c2e940 (LWP 7217)):\n#0  0x00002b593bf2a92e in apr_brigade_length ()\n#1  0x00002b593b42f77e in ap_http_filter () from /proc/7212/exe\n#2  0x00002b594144500a in ?? ()\n#3  0x00002b5941446f16 in ?? ()\n#4  0x00002b5941233cb1 in proxy_run_scheme_handler ()\n#5  0x00002b59412377f3 in ?? ()\n#6  0x00002b593b42038a in ap_run_handler () from /proc/7212/exe\n#7  0x00002b593b423802 in ap_invoke_handler () from /proc/7212/exe\n#8  0x00002b593b42dfc8 in ap_process_request () from /proc/7212/exe\n#9  0x00002b593b42b200 in ?? () from /proc/7212/exe\n#10 0x00002b593b4275f2 in ap_run_process_connection () from /proc/7212/exe\n#11 0x00002b593b432a37 in ?? () from /proc/7212/exe\n#12 0x00002b593cf04367 in start_thread () from /lib64/libpthread.so.0\n#13 0x00002b593d3f10ad in clone () from /lib64/libc.so.6\nThread 1 (Thread 0x2b593fa2a560 (LWP 7212)):\n#0  0x00002b593cf05655 in pthread_join () from /lib64/libpthread.so.0\n#1  0x00002b593ccf5d85 in apr_thread_join () from /usr/lib64/libapr-1.so.0\n#2  0x00002b593b4321b0 in ?? () from /proc/7212/exe\n#3  0x00002b593b432d79 in ?? () from /proc/7212/exe\n#4  0x00002b593b432ef5 in ?? () from /proc/7212/exe\n#5  0x00002b593b43373b in ap_mpm_run () from /proc/7212/exe\n#6  0x00002b593b40d7e0 in main () from /proc/7212/exe\n\nWeek 2 diff 1:\n\n--- 7212.1\t2009-09-24 14:22:42.000000000 -0600\n+++ 7212.2\t2009-09-24 14:25:13.000000000 -0600\n@@ -5,7 +5,7 @@\n #3  0x00002b593cf04367 in start_thread () from /lib64/libpthread.so.0\n #4  0x00002b593d3f10ad in clone () from /lib64/libc.so.6\n Thread 3 (Thread 0x4282c940 (LWP 7215)):\n-#0  0x00002b593cceca80 in apr_allocator_free () from /usr/lib64/libapr-1.so.0\n+#0  0x00002b593cceca84 in apr_allocator_free () from /usr/lib64/libapr-1.so.0\n #1  0x00002b593bf29c57 in ?? () from /usr/lib64/libaprutil-1.so.0\n #2  0x00002b593bf2a7c0 in apr_brigade_cleanup ()\n #3  0x00002b593b41f307 in ap_core_output_filter () from /proc/7212/exe\n@@ -16,7 +16,7 @@\n #8  0x00002b593cf04367 in start_thread () from /lib64/libpthread.so.0\n #9  0x00002b593d3f10ad in clone () from /lib64/libc.so.6\n Thread 2 (Thread 0x43c2e940 (LWP 7217)):\n-#0  0x00002b593bf2a92e in apr_brigade_length ()\n+#0  0x00002b593bf2a935 in apr_brigade_length ()\n #1  0x00002b593b42f77e in ap_http_filter () from /proc/7212/exe\n #2  0x00002b594144500a in ?? ()\n #3  0x00002b5941446f16 in ?? ()\n\nWeek 2 diff 2:\n\n--- 7212.1\t2009-09-24 14:22:42.000000000 -0600\n+++ 7212.3\t2009-09-24 14:25:22.000000000 -0600\n@@ -5,7 +5,7 @@\n #3  0x00002b593cf04367 in start_thread () from /lib64/libpthread.so.0\n #4  0x00002b593d3f10ad in clone () from /lib64/libc.so.6\n Thread 3 (Thread 0x4282c940 (LWP 7215)):\n-#0  0x00002b593cceca80 in apr_allocator_free () from /usr/lib64/libapr-1.so.0\n+#0  0x00002b593cceca8c in apr_allocator_free () from /usr/lib64/libapr-1.so.0\n #1  0x00002b593bf29c57 in ?? () from /usr/lib64/libaprutil-1.so.0\n #2  0x00002b593bf2a7c0 in apr_brigade_cleanup ()\n #3  0x00002b593b41f307 in ap_core_output_filter () from /proc/7212/exe\n@@ -16,7 +16,7 @@\n #8  0x00002b593cf04367 in start_thread () from /lib64/libpthread.so.0\n #9  0x00002b593d3f10ad in clone () from /lib64/libc.so.6\n Thread 2 (Thread 0x43c2e940 (LWP 7217)):\n-#0  0x00002b593bf2a92e in apr_brigade_length ()\n+#0  0x00002b593bf2a924 in apr_brigade_length ()\n #1  0x00002b593b42f77e in ap_http_filter () from /proc/7212/exe\n #2  0x00002b594144500a in ?? ()\n #3  0x00002b5941446f16 in ?? ()\n\nFrom a quick look at the code, it looks like the bucket brigade functions and/or macros are looping endlessly. I don't understand that data structure so I don't know how this might happen. I can try to inspect the data structure the next time we get a process in an endless loop to see what it looks like.\n\nI was hoping that someone might already know what state the data structure would need to be in to cause an endless loop, and solving the problem would be a matter of determining how it's getting into that state.\n\nI can gather more information as needed, but making changes to these servers can be slow and requires fighting bureaucracy. It unlikely that I would be able to test anything other than small patches to the Red Hat httpd.", "id": 130923, "time": "2009-10-06T16:38:42Z", "bug_id": 39079, "creation_time": "2009-10-06T16:38:42Z", "is_private": false}, {"count": 30, "text": "(In reply to comment #29)\n\nIn case it happens again: you can use ps with the \"L\" flag to see the list of threads (lightweigt processes) of a process and also CPU time by thread. That way you can identify which thread (or threads) are consuming CPU thus allowing to map it more reliably to the stacks generated with gstack.", "bug_id": 39079, "is_private": false, "id": 130924, "time": "2009-10-06T17:16:07Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2009-10-06T17:16:07Z", "tags": [], "attachment_id": null}, {"count": 31, "tags": [], "text": "(In reply to comment #30)\n\nActually, I already did that and forgot to mention the threads that were busy. In the week 2 trace, it was thread 2. In the week 1 trace, it was threads 2 and 3. These were the threads somewhere in the call chain of ap_process_request.\n\nCan the people who have also seen this issue confirm that they only see this problem on requests served by mod_proxy_http? Is everyone using mod_proxy?", "attachment_id": null, "id": 130956, "creator": "patrick133t@yahoo.com", "time": "2009-10-07T14:18:39Z", "bug_id": 39079, "creation_time": "2009-10-07T14:18:39Z", "is_private": false}, {"count": 32, "text": "Our situation worsened since applying a RedHat patch recently (upgraded from 2.2.3-43 to 2.2.3-45). We've actually been running with the segfaults and CPU spinning ever since my last report because it has not caused any trouble for actual requests until we applied the most recent patch.\n\nThis new problem (apache stopped servicing requests entirely) prompted us to install gdb and debuginfo packages onto our production servers and I have been analyzing the processes that are using 100% CPU today.\n\nThis new data makes it look like we are experiencing bug 45792, so I tried running similar test cases to the ones in that bug report, and found that I can reproduce the crashes outside of production.\n\nArmed with that, I grabbed the patch for rev 713146 (http://svn.apache.org/viewvc?view=revision&revision=713146) and added it as yet another patch to the RPM and installed it onto a test system. With that patch, I no longer see segfaults.\n\nWe do see three different symptoms, so I'm not sure if we're seeing three different bugs. The fact that others are seeing this problem on 2.2.11 makes me think we might be seeing at least two bugs. I'll work on getting the patch into production and report if I continue to see any issues.", "bug_id": 39079, "is_private": false, "id": 144570, "time": "2011-02-25T19:00:43Z", "creator": "patrick133t@yahoo.com", "creation_time": "2011-02-25T19:00:43Z", "tags": [], "attachment_id": null}, {"count": 33, "tags": [], "bug_id": 39079, "attachment_id": null, "is_private": false, "id": 144571, "time": "2011-02-26T03:10:21Z", "creator": "wrowe@apache.org", "creation_time": "2011-02-26T03:10:21Z", "text": "> Our situation worsened since applying a RedHat patch recently\n\nStop.\n\nBugzilla is for reporting specific defects in ASF code.  It is not for\nunwinding flaws in your configuration or vendors builds.\n\nPlease consult either the users@ peer to peer discussion forum or a vendor\nfor the resolution to your ills."}, {"count": 34, "tags": [], "creator": "patrick133t@yahoo.com", "attachment_id": null, "id": 144583, "time": "2011-02-26T12:15:50Z", "bug_id": 39079, "creation_time": "2011-02-26T12:15:50Z", "is_private": false, "text": "(In reply to comment #33)\n> Bugzilla is for reporting specific defects in ASF code.  It is not for\n> unwinding flaws in your configuration or vendors builds.\n\nSorry. Allow me to summarize: I think this issue (as reported by Dick Snippe) is a dupe of 45792. The reports against 2.2.11 sound like a separate issue. I've combed through many stack traces and never seen what HuoMingyu reports, though I've seen many like Dick's. A stack trace or two from Henrik K would be helpful to be sure.\n\nHave you tried the worker MPM on 2.2.11 or later, Dick?"}]