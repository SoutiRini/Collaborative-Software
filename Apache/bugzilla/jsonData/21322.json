[{"count": 0, "tags": [], "creator": "glenn@apache.org", "attachment_id": null, "id": 39950, "creation_time": "2003-07-03T18:21:47Z", "time": "2003-07-03T18:21:47Z", "bug_id": 21322, "text": "Once or twice a week we see the main apache process lock up and stop accepting\nrequests.  We have seen this on two different Sun E450's with dual CPU's running\nSolaris 7.  Apache 2 is compiled with CC from Sun Workshop 5 and linked against\npthread.\n\nWe forced a core dump using gcore and this is what the backtrace reported:\n\ncurrent thread: t@0\n=>[1] ___lwp_cond_wait(0xfee63f08, 0xfee63f18, 0xfee25c90, 0x0, 0x0,\n0xfee25c88), at 0xfef19a94\n  [2] __lwp_cond_timedwait(0xfee63f08, 0xfee63f18, 0x0, 0x3f046de1, 0x0, 0x0),\nat 0xfef11560\n  [3] _age(0xfee5c9ac, 0xfee5d72c, 0xfee5dfe8, 0xfee5e000, 0x3, 0xfee5c9ac), at\n0xfee3a8dc\n  [4] _lwp_start(0x6000, 0xffbef75c, 0xfee5d778, 0xfee5d760, 0xfee5c9ac,\n0xffbef650), at 0xfee3be5c\n\nI suspect this is happening when the main apache process worker MPM is forking\na child.  I did a quick man of fork and found the following which may help\nexplain what is going on:\n\n  Fork-safety\n     If fork1() is called in a Solaris thread program  or  fork()\n     is  called  in  a   POSIX thread program, and the child does\n     more than just  call  exec(),  there  is  a  possibility  of\n     deadlocking  in the child. To ensure that the application is\n     safe  with  respect  to  this  deadlock,   it   should   use\n     pthread_atfork(3T).  Should there be any outstanding mutexes\n     throughout  the  process,  the   application   should   call\n     pthread_atfork(3T),  to  wait for and acquire those mutexes,\n     prior to calling fork(). (See   attributes(5)  \"MT-Level  of\n     Libraries\")\n\nI checked the source for the worker MPM and it is not using pthread_atfork().", "is_private": false}, {"count": 1, "tags": [], "bug_id": 21322, "attachment_id": null, "id": 40090, "time": "2003-07-07T19:01:22Z", "creator": "aaron@apache.org", "creation_time": "2003-07-07T19:01:22Z", "is_private": false, "text": "The main parent apache process is linked against the pthread library, but it does not create any \nthreads. In addition, the main apache process does not accept incoming requests, it only creates \nchildren to handle requests. Are you sure this backtrace is from the parent process and not from a \nhung child?"}, {"count": 2, "tags": [], "creator": "glenn@apache.org", "attachment_id": null, "text": "I am pretty sure that is from the main apache process which runs as root,\nbut the backtrace is for the main process.  I just did a gcore of a an\napache 2 main process on a server running normally and the backtrace for\nthe main process and its three threads look identical to the gcore generated\nfor the root process when apache 2 was not accepting requests.\n\nAfter comparing the two I realized that my diagnosis of the problem I originally\nposted is flawed.  Thanks for pointing out my bad assumptions. :-)\n\nHere is what I am seeing.\n\nA child process fails with a coredump.  From that time on apache 2 is still\nbound to port 80 but does not respond to HTTP requests.  Our system monitoring\nsoftware does not detect that Apache 2 has failed. Usually a stop/start\nof apache 2 will solve the problem.  This doesn't happen everytime a child\nprocess dies.  There are no error messages other than that the child process\ndied.  LogLevel is set to WARN.\n\nCan a hung child cause this behaviour?\n\nThe next time this happens I will make sure gcore is used\nto generate a core file for all the apache processes.\n\nThanks, glenn", "id": 40135, "time": "2003-07-08T02:29:35Z", "bug_id": 21322, "creation_time": "2003-07-08T02:29:35Z", "is_private": false}, {"count": 3, "tags": [], "bug_id": 21322, "text": "I finally had a server fail in a way that it was still running but not\naccepting requests.\n\nI checked to see what httpd process were running using ps.  There were\nthree httpd processes.  One running as root, one child started at the\nsame time as the root process for the cgid daemon, and one child for\nhandling requests.\n\nWhen reviewing the logs I found that an httpd child process had core dumped\njust prior to when apache 2 stopped accepting HTTP requests.\n  \nHere is the backtrace for the thread in the child process which caused it\nto core dump.  Note that this is apache 2.0.46 with two patches applied\nto mod_cgid.c.  The cgid daemon restart patch I submitted a month ago\nand the patch to prevent a double close on a socket.\n  \nt@6 (l@4) terminated by signal ILL (illegal opcode)\nCurrent function is send_parsed_content\n 3152                       apr_bucket_delete(tmp_bkt);\n(/opt/SUNWspro/bin/dbx) where\ncurrent thread: t@6\n  [1] 0x20f7d8(0xff317ad8, 0xfdc05860, 0x32ca50, 0x2fb558, 0x20fd10,\n0xfdc05784), at 0x20f7d7\n=>[2] send_parsed_content(bb = 0xfdc05860, r = 0x32ca50, f = 0x2fb558), line\n3152 in \"mod_include.c\"\n  [3] includes_filter(f = 0x2fb558, b = 0x2fc810), line 3430 in \"mod_include.c\"\n  [4] ap_pass_brigade(next = 0x2fb558, bb = 0x2fb258), line 550 in \"util_filter.c\"  \n  [5] cgid_handler(r = 0x32ca50), line 1484 in \"mod_cgid.c\"\n  [6] ap_run_handler(0x32ca50, 0x0, 0x32ca50, 0x2f89a8, 0x0, 0x0), at 0x7fa98\n  [7] ap_invoke_handler(r = 0x32ca50), line 401 in \"config.c\"\n  [8] ap_process_request(r = 0x32ca50), line 288 in \"http_request.c\"\n  [9] ap_process_http_connection(c = 0x2f89a8), line 293 in \"http_core.c\"\n  [10] ap_run_process_connection(0x2f89a8, 0x2f88e8, 0x2f88e8, 0x81, 0x2f89a0,\n0x20f7f0), at 0x95b60\n  [11] ap_process_connection(c = 0x2f89a8, csd = 0x2f88e8), line 211 in\n\"connection.c\"\n  [12] process_socket(p = 0x2f88b0, sock = 0x2f88e8, my_child_num = 2,\nmy_thread_num = 1, bucket_alloc = 0x20f7f0), line 632 in \"worker.c\"\n  [13] worker_thread(thd = 0x1aa400, dummy = 0x1ea550), line 947 in \"worker.c\"\n  [14] dummy_worker(opaque = 0x1aa400), line 127 in \"thread.c\"\n\n\nHere is a summary of the threads for the child process which core dumped:\n\n      t@1  a l@1  ?()   LWP suspended   in _read()\n      t@2  b l@2  ?()   LWP suspended   in __signotifywait()\n      t@3         ?()   sleep on (unknown)      in _reap_wait()\n      t@5  a l@10 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\no>    t@6  a l@4  dummy_worker()        signal SIGILL   in ?()\n      t@7  a l@12 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n      t@8  a l@5  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n      t@9         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@10  a l@6  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@11         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@12         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@13         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@14         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@15         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@16         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@17         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@18         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@19         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@20         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@21         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@22         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@23         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@24  a l@8  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@25         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@26  a l@11 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@27         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@28  a l@9  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@29  a l@7  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@30  a l@3  dummy_worker()        LWP suspended   in _poll()\n\nHere is the backtrace for its listener_thread:\n\nCurrent function is listener_thread\n  762                   ret = apr_poll(pollset, num_listensocks, &n, -1);\nt@30 (l@3) stopped in _poll at 0xfef15f54\n0xfef15f54: _poll+0x0008:       bgeu    _poll+0x30\n(/opt/SUNWspro/bin/dbx) where\ncurrent thread: t@30\n  [1] _poll(0x4, 0x2, 0xffffffff, 0x0, 0x0, 0x0), at 0xfef15f54\n  [2] _ti_poll(0x20b818, 0x2, 0xfb807cd8, 0xffffffff, 0xffffffff, 0x0), at\n0xfee4b438\n=>[3] listener_thread(thd = 0x1aa700, dummy = 0x1edbd0), line 762 in \"worker.c\"\n  [4] dummy_worker(opaque = 0x1aa700), line 127 in \"thread.c\"\n\nHere is a list of threads for the one remaining child httpd process for\nhandling HTTP requests:\n\n      t@1  a l@1  ?()   running                 in _read()\n      t@2  b l@2  ?()   running                 in __signotifywait()\n      t@3         ?()   sleep on (unknown)      in _reap_wait()\n      t@5         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n      t@6  a l@12 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n      t@7  a l@4  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n      t@8         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n      t@9  a l@11 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@10  a l@7  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@11  a l@14 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@12         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@13         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@14         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@15         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@16         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@17  a l@6  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@18         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@19  a l@5  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@20  a l@8  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@21         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@22         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@23  a l@15 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@24  a l@9  dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@25         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@26  a l@13 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@27  a l@10 dummy_worker()        sleep on 0x1aa180       in __lwp_sema_wait()\n     t@28         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@29         dummy_worker()        sleep on 0x1aa180       in\n_cond_wait_cancel()\n     t@30  a l@3  dummy_worker()        running                 in\n___lwp_mutex_lock()\n\nThread 30 looks like it is the worker listener_thread, here is its backtrace:\n\nCurrent function is proc_mutex_proc_pthread_acquire\n  412       if ((rv = pthread_mutex_lock(mutex->pthread_interproc))) {\nt@30 (l@3) stopped in ___lwp_mutex_lock at 0xfef17c70\n0xfef17c70: ___lwp_mutex_lock+0x0008:   ta      0x8\n(/opt/SUNWspro/bin/dbx) where\ncurrent thread: t@30\n  [1] ___lwp_mutex_lock(0xfed60000, 0x4d58, 0x20254, 0x0, 0x0, 0x0), at 0xfef17c70\n  [2] _mutex_lwp_lock(0xfed60000, 0x1, 0x10000, 0xfee5ca0c, 0x10000,\n0xfee69474), at 0xfee3c7f0\n  [3] _pthread_mutex_lock(0xfed60000, 0xfee5ca0c, 0x329280, 0x0, 0x0, 0x0), at\n0xfee3c914\n=>[4] proc_mutex_proc_pthread_acquire(mutex = 0x1a88e0), line 412 in \"proc_mutex.c\"\n  [5] apr_proc_mutex_lock(mutex = 0x1a88e0), line 857 in \"proc_mutex.c\"\n  [6] listener_thread(thd = 0x1aa700, dummy = 0x1edbd0), line 736 in \"worker.c\"\n  [7] dummy_worker(opaque = 0x1aa700), line 127 in \"thread.c\"\n\nThe 25 threads used for handling requests are in one of two states:\n\nCurrent function is apr_thread_cond_wait\n  118       rv = pthread_cond_wait(cond->cond, &mutex->mutex);\nt@29 (l@X) stopped in _cond_wait_cancel at 0xfee39b64\n0xfee39b64: _cond_wait_cancel+0x00bc:   call    _swtch\n(/opt/SUNWspro/bin/dbx) where\ncurrent thread: t@29\n  [1] _cond_wait_cancel(0x1aa180, 0x1aa140, 0x4356, 0xfee5ca0c, 0x0, 0x3c863d),\nat 0xfee39b64\n  [2] pthread_cond_wait(0x1aa180, 0x1aa140, 0xfb9099ec, 0x58, 0x3c85e0,\n0x23e248), at 0xfee39a88\n=>[3] apr_thread_cond_wait(cond = 0x1aa178, mutex = 0x1aa138), line 118 in\n\"thread_cond.c\"\n  [4] ap_queue_pop(queue = 0x1aa120, sd = 0xfb909ce4, p = 0xfb909cd8), line 300\nin \"fdqueue.c\"\n  [5] worker_thread(thd = 0x1aa6e0, dummy = 0x1e7420), line 915 in \"worker.c\"\n  [6] dummy_worker(opaque = 0x1aa6e0), line 127 in \"thread.c\"\n\nCurrent function is apr_thread_cond_wait\n  118       rv = pthread_cond_wait(cond->cond, &mutex->mutex);\nt@27 (l@10) stopped in __lwp_sema_wait at 0xfef17d34\n0xfef17d34: __lwp_sema_wait+0x0008:     ta      0x8\n(/opt/SUNWspro/bin/dbx) where\ncurrent thread: t@27\n  [1] __lwp_sema_wait(0xfbb0de78, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfef17d34\n  [2] _park(0xfbb0ddc8, 0xfbb0de78, 0x0, 0xb, 0xfee5d7a0, 0xfc509dc8), at 0xfee3b1c8\n  [3] _swtch(0x5, 0xfee5ca0c, 0xfbb0de58, 0xfbb0de54, 0xfbb0de50, 0xfbb0de4c),\nat 0xfee3aebc\n  [4] _cond_wait_cancel(0x1aa180, 0x1aa140, 0x4356, 0xfee5ca0c, 0x0, 0x3293cd),\nat 0xfee39b64\n  [5] pthread_cond_wait(0x1aa180, 0x1aa140, 0xfbb0d9ec, 0x56, 0x329370,\n0x23a158), at 0xfee39a88\n=>[6] apr_thread_cond_wait(cond = 0x1aa178, mutex = 0x1aa138), line 118 in\n\"thread_cond.c\"\n  [7] ap_queue_pop(queue = 0x1aa120, sd = 0xfbb0dce4, p = 0xfbb0dcd8), line 300\nin \"fdqueue.c\"\n  [8] worker_thread(thd = 0x1aa6a0, dummy = 0x1e43d8), line 915 in \"worker.c\"\n  [9] dummy_worker(opaque = 0x1aa6a0), line 127 in \"thread.c\"\n\nWhich looks like what we would expect if this child were hung, they are all waiting\nto for a request to handle in the queue.\n\nConclusions:\n\nThe listener_thread for the child process which core dumped had a lock on the\naccept mutex when it failed. The process mutex's are implemented using a semaphore.\nThe remaining child process is hung trying to obtain a lock on the accept mutex.\nAccording to the man pages for Solaris, when a process recieves a signal which\ntriggers a core all the actions of exit() are executed to cleanup the process.\nThis includes closing any open semaphores. So it appears that semaphores are\nnot getting closed when a child process fails with a core dump on these two\nservers where we are seeing this problem.\n\nDoes this sound like a valid conclusion of the problem?\n\nThe question now is whether this is a bug in Solaris 7 for which there is a patch?\n\nThanks,\n\nGlenn", "id": 40546, "time": "2003-07-12T13:41:58Z", "creator": "glenn@apache.org", "creation_time": "2003-07-12T13:41:58Z", "is_private": false, "attachment_id": null}, {"count": 4, "tags": [], "text": "Okay, so this is really maybe two separate bugs that are interacting with each \nother.  One is a bug in mod_include that is causing a segfault.  The other is a \nfailure to release a semaphore upon segv.  Let's segregate the two issues.  Can \nyou open a second PR to track the mod_include problem?  We'll leave this one \nopen to look into the semaphore thing since it fits this job's summary already.\n\nIn the new mod_include PR, please paste the backtrace with send_parsed_content\n() near the top, and please see if you can figure out for us what page was \nbeing served at the time of the segfault.  I realize this may be difficult to \ndo since the request would not yet have been logged at the time of the fault, \nbut maybe through a little bit of induction you can figure out what page it \nwas.  See if you can reproduce the problem with a particular request reliably.  \n\nAndre and I have been working on a triplet of mod_include bugs over the last \nthree days or so -- this might be one of those or it might be a new one.  \n(Probably a new one I think.)\n\napr_bucket_delete() is a macro that calls APR_BUCKET_REMOVE() and \napr_bucket_destroy().  Frame #1 being a function pointer would seem to indicate \nthat it was apr_bucket_destroy() that failed (APR_BUCKET_REMOVE just \nmanipulates a few pointers -- it doesn't call any functions).  \napr_bucket_destroy() is a macro that calls bucket->type->destroy() and bucket-\n>free().  It's hard to say which of those is the one that actually bombed.  But \nin either case the most likely scenario is that we attempted to delete a bucket \nthat had already been deleted.  I'd have to see the file that mod_include was \nattempting to parse that caused the fault to know more.\n\n--Cliff", "is_private": false, "bug_id": 21322, "id": 40550, "time": "2003-07-12T19:46:12Z", "creator": "jwoolley@apache.org", "creation_time": "2003-07-12T19:46:12Z", "attachment_id": null}, {"count": 5, "tags": [], "creator": "glenn@apache.org", "text": "I already opened a bug 21222 for mod_include where send_parsed_content\nwas in the backtrace.  I have dozens of core files where mod_include\nwas in the backtrace.  I can usually find out what file was being parsed\nby examining the request_rec memory.\n\nI'll go through all these core files and get the backtrace and identify\nthe file being processed any time mod_include is in the backtrace.  \nThen submit this information for bug 21222.\n\nI can live with mod_include causing a child process to die. What is\nmaking us seriously consider backing out of Apache 2 are the problems\nwith the accept mutex semaphore which cause Apache 2 to stop accepting \nrequests.\n\nThanks for all the help.\n", "id": 40551, "time": "2003-07-12T20:39:13Z", "bug_id": 21322, "creation_time": "2003-07-12T20:39:13Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "text": "Ahh, okay, thanks for the reference.  As for backing down to 1.3 from 2.0, I'd \nrecommend trying the prefork MPM before I'd recommend going all the way back \ndown to 1.3.  Of course the optimal solution would be to just fix these two \nbugs so you can stick with worker.  :)", "is_private": false, "bug_id": 21322, "id": 40557, "time": "2003-07-12T21:48:58Z", "creator": "jwoolley@apache.org", "creation_time": "2003-07-12T21:48:58Z", "attachment_id": null}, {"count": 7, "tags": [], "creator": "trawick@apache.org", "attachment_id": null, "text": ">What is making us seriously consider backing out \n>of Apache 2 are the problems with the accept mutex \n>semaphore which cause Apache 2 to stop accepting \n>requests.\n\na couple of things to try first:\n\n1) switch mutex types\n\n2) switch MPM to prefork\n\nMaybe I didn't read the doc carefully enough and I missed something, but here\ngoes anyway:\n\nThe only way to know if there is a deadlock due to the accept mutex is to look\nat the listener thread in *every* Apache child process and verify that each one\nis in a call to apr_proc_mutex_lock().  If all but one are stuck in\napr_proc_mutex_lock, that is normal behavior.  If all listener threads are stuck\nin apr_proc_mutex_lock, then you have the suspected deadlock.\n\nIf the listener thread is in pthread_cond_wait(), all workers in that process\nare busy, but we don't hold the mutex in that condition and there should be\nanother child with idle children (unless you've hit MaxClients and the parent is\nunable to create another child due to your configuration).\n\nThere is a script at http://www.apache.org/~trawick/lsap that makes checking out\nall listener threads pretty easy; it displays something like this:\n\nparent: 17943\nchild: 17946    listener thread not waiting on mutex\nchild: 17944    cgid daemon\nchild: 17948    listener thread waiting on mutex\nchild: 17945    listener thread waiting on mutex\n\nI note that you're using proc pthread mutexes currently, which are supposed to\nbe robust on Solaris in that APR implements the protocol for recovering the\nmutex after a process dies holding the mutex.  But in case that isn't working\nperfectly for you, it is worth trying a different mutex type.  I've seen a\nscenario on Solaris 8 where if you send SIGSEGV to all worker child processes\nthe pthread accept mutex isn't recovered properly and you get a deadlock, but\nwhen I test it with a child crashing once every several seconds the special\nSolaris recovery logic works consistently.\n", "id": 40559, "time": "2003-07-12T21:54:34Z", "bug_id": 21322, "creation_time": "2003-07-12T21:54:34Z", "is_private": false}, {"count": 8, "tags": [], "creator": "glenn@apache.org", "attachment_id": null, "text": "Switching to the prefork worker is an option, but then we lose all the\nbenefits of the worker MPM.  In our testing we saw 30-50% reduction in\nCPU usage and memory usage reduced to 15-20% from Apache 1.3 for the\nsame request volume.  :-)  prefork might be an option, at least we could\nstill use filters.  I am willing to stick with the worker MPM until we either\nfind a solution to the semaphore problem or the sysads where I work get tired\nof getting paged once a week and lynch me. ;-)\n\nIn all cases where apache 2 has stopped accepting requests there were only\nthree httpd processes, the root process, the cgid daemon process, and\none child worker process, that single child worker process was hung trying \nto lock the accept mutex.\n\nWith regard to switching mutex types, how would I specify to configure\nthat I wanted to use native Solaris mutex's rather than posix?\n", "id": 40562, "time": "2003-07-12T22:29:46Z", "bug_id": 21322, "creation_time": "2003-07-12T22:29:46Z", "is_private": false}, {"count": 9, "tags": [], "bug_id": 21322, "text": ">With regard to switching mutex types, how would I specify to configure\n>that I wanted to use native Solaris mutex's rather than posix?\n\nWe don't support native Solaris thread stuff.  But we do support other mutex\ntypes on Solaris, including fcntl and sysvsem.  Stick this in your httpd conf file:\n\nAcceptMutex fcntl\n\nor\n\nAcceptMutex sysvsem\n\n", "id": 40567, "time": "2003-07-13T02:20:31Z", "creator": "trawick@apache.org", "creation_time": "2003-07-13T02:20:31Z", "is_private": false, "attachment_id": null}, {"count": 10, "tags": [], "creator": "glenn@apache.org", "text": "Ahh, thanks.  Somehow I missed that config directive.\n\nI have reconfigured our apache 2 servers to use sysvsem.\nIf that fails I can fall back to fcntl and take the performance\nhit rather than server unavailability.\n\nNow I will just wait for a child process to die which had a lock\non the accept mutex and see what happens.\n\nThanks!", "id": 40571, "time": "2003-07-13T15:17:38Z", "bug_id": 21322, "creation_time": "2003-07-13T15:17:38Z", "is_private": false, "attachment_id": null}, {"count": 11, "attachment_id": null, "bug_id": 21322, "text": "I can now confirm that on our Solaris 7 systems changing the AcceptMutex\nto sysvsem does fix the problem with the hung child processes.  We have\nhad 2 child processes die since I changed the config, one of these had\na lock on the semaphore.  The semaphore must have been closed properly\nsince apache 2 continued on its merry way handling requests.\n\nThanks for all your help diagnosing this problem.\n\nI don't know whether this is a common problem on Solaris or\nspecific versions of Solaris.  Perhaps one thing to consider\nchanging to fix this bug would be to swtich which semaphore \nimplementation is used as the default when building Apache 2\non Solaris in general or specific versions of Solaris.\n", "id": 40688, "time": "2003-07-15T01:29:03Z", "creator": "glenn@apache.org", "creation_time": "2003-07-15T01:29:03Z", "tags": [], "is_private": false}, {"count": 12, "tags": [], "bug_id": 21322, "is_private": false, "id": 41095, "attachment_id": null, "creator": "aaron@apache.org", "creation_time": "2003-07-21T17:57:34Z", "time": "2003-07-21T17:57:34Z", "text": "Hi Glenn. First of all, thank you very much for the extremely detailed bug reports.\n\nI'm glad you've tracked down the problem, but I wonder if the recovery mechanism\nwe're using on solaris hasn't been fixed or improved in newer versions of the\nthread libraries. Are you sure you're using the latest code? I wonder if it's\npossible to test out this same scenario on the same hardware (at least a dual CPU\nsystem) on Solaris 9 (or even Solaris 8). Pthread (which is implemented directly\nupon the native Solaris libthread library) was by far the fastest mutex mechanism\nin my own tests, on both single and MP boxes, so it would be nice if we could\nrule out that library completely or find a way to use it."}, {"count": 13, "tags": [], "text": "We found a Solaris 7 patch for a libthread bug which looks suspicously like\nhow some of the child process have failed.  We patched one of the production\nservers Sunday and will be scheduling patching the remaining systems.\n\nI can try testing again on the patched system with Solaris 7\n(E450 dual CPU).\n\nWe don't have any servers with Solaris 9, but I do have a scratch\ndual CPU E250 with Solaris 8 I can test the semaphores on.", "attachment_id": null, "bug_id": 21322, "id": 41167, "time": "2003-07-22T15:00:01Z", "creator": "glenn@apache.org", "creation_time": "2003-07-22T15:00:01Z", "is_private": false}, {"count": 14, "tags": [], "creator": "glenn@apache.org", "attachment_id": null, "id": 41171, "creation_time": "2003-07-22T16:35:28Z", "time": "2003-07-22T16:35:28Z", "bug_id": 21322, "text": "You asked if we were running the latest code?  By that do you mean\napache 2? Solaris 7 OS patches?", "is_private": false}, {"count": 15, "tags": [], "creator": "glenn@apache.org", "attachment_id": null, "text": "I was able to test this on a dual CPU e250 running Solaris 8.\nMy testing showed that posix semaphores seemed to work fine.\n\nI did a quick test on this box to compare posix vs sysv semaphores.\nIn my tests sysvsem had slightly better ab results.", "id": 41669, "time": "2003-07-28T16:13:43Z", "bug_id": 21322, "creation_time": "2003-07-28T16:13:43Z", "is_private": false}, {"count": 16, "tags": [], "bug_id": 21322, "text": "Wanted to add that I had what appears to be the same issue with 2.0.47 using the\nworker MOM and the pthread AcceptMutex on SGI Irix.  One of the workers took a\nSIGBUS which left the other two workers sitting around.  Apache was accepting\nrequests, but was not responding.  I'm guessing the SIGBUS happened because the\nwebserver is running a NFS server and has a client nearly constantly writing\nfiles which Apache2 serves up and EnableMMAP was turned on.  I have seen similar\nSIGBUSs using Apache 1 but of course functionality was not affected.  I will\nalso give the sysvsem AcceptMutex a shot and go back to the worker MPM.\n\nGlenn, have you had any more issues with the server hanging when a worker dies\nsince switching AcceptMutexes?", "id": 45810, "time": "2003-10-17T00:58:59Z", "creator": "drees76@gmail.com", "creation_time": "2003-10-17T00:58:59Z", "is_private": false, "attachment_id": null}, {"count": 17, "tags": [], "bug_id": 21322, "is_private": false, "id": 46634, "attachment_id": null, "creator": "glenn@apache.org", "creation_time": "2003-11-02T17:12:43Z", "time": "2003-11-02T17:12:43Z", "text": "Since switching AcceptMutex to sysvsem I have not had any problems\nwith the server hanging when a worker dies."}, {"count": 18, "tags": [], "creator": "trawick@apache.org", "attachment_id": null, "id": 59918, "creation_time": "2004-06-26T12:29:17Z", "time": "2004-06-26T12:29:17Z", "bug_id": 21322, "text": "In current APR, and in the upcoming Apache httpd 2.0.50, APR configure won't\nchoose pthread mutex as the default cross-process mutex because of this issue.", "is_private": false}]