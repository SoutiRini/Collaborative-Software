[{"count": 0, "tags": [], "text": "I am seeing the following every 2-3 seconds when I have more than 1 pool member enabled with a healthcheck\n\n[Tue Aug 30 19:57:09.637546 2016] [core:notice] [pid 30412:tid 140320377878336] AH00051: child pid 24665 exit signal Segmentation fault (11), possible coredump in /webApps/apache\n\nExample is my hcheck config for a single BalancerMember\n\nProxyHCExpr ok2x3x {%{REQUEST_STATUS} =~ /^[23]/}\n\nBalancerMember  https://xxxxxxxxxxxxxx:443 loadfactor=1 timeout=5 connectiontimeout=500ms hcmethod=GET hcexpr=ok2x3x hcinterval=5 hcpasses=2 hcfails=3 hcuri=/xxxxx/healthcheck\n\nWhen adding additional members I would simply do\n\nBalancerMember  https://xxxxxxxxxxxxxx:443 loadfactor=1 timeout=5 connectiontimeout=500ms hcmethod=GET hcexpr=ok2x3x hcinterval=5 hcpasses=2 hcfails=3 hcuri=/xxxxx/healthcheck\nBalancerMember  https://xxxxxxxxxxxxxx:443 loadfactor=1 timeout=5 connectiontimeout=500ms hcmethod=GET hcexpr=ok2x3x hcinterval=5 hcpasses=2 hcfails=3 hcuri=/xxxxx/healthcheck\n\netc and then start to see the segmentation faults which eventually lead to the parent process stop responding after 12-14 hours.", "attachment_id": null, "bug_id": 60071, "id": 193423, "time": "2016-08-31T00:10:18Z", "creator": "eric@nutramorph.com", "creation_time": "2016-08-31T00:10:18Z", "is_private": false}, {"count": 1, "tags": [], "text": "encountered same bug here\n\ncoredump info below:\n\nCore was generated by `/data/apache/bin/httpd -k start'.\nProgram terminated with signal 11, Segmentation fault.\n#0  0x00007fd8c4384169 in hc_get_hcworker (ctx=0x117af70, worker=0x1238f28, p=0x7fd8bc030e58) at mod_proxy_hcheck.c:426\n426\t        PROXY_STRNCPY(hc->s->name,     wptr);", "attachment_id": null, "bug_id": 60071, "id": 194566, "time": "2016-10-20T02:12:29Z", "creator": "6731165@163.com", "creation_time": "2016-10-20T02:12:29Z", "is_private": false}, {"count": 2, "tags": [], "creator": "6731165@163.com", "attachment_id": null, "is_private": false, "id": 194567, "time": "2016-10-20T02:32:13Z", "bug_id": 60071, "creation_time": "2016-10-20T02:32:13Z", "text": "encountered same bug here\n\napache info:\nServer version: Apache/2.4.23 (Unix)\nServer built:   Oct 17 2016 17:26:32\nServer's Module Magic Number: 20120211:61\nServer loaded:  APR 1.5.2, APR-UTIL 1.5.4\nCompiled using: APR 1.5.2, APR-UTIL 1.5.4\nArchitecture:   64-bit\nServer MPM:     event\n  threaded:     yes (fixed thread count)\n    forked:     yes (variable process count)\nServer compiled with....\n -D APR_HAS_SENDFILE\n -D APR_HAS_MMAP\n -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled)\n -D APR_USE_SYSVSEM_SERIALIZE\n -D APR_USE_PTHREAD_SERIALIZE\n -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT\n -D APR_HAS_OTHER_CHILD\n -D AP_HAVE_RELIABLE_PIPED_LOGS\n -D DYNAMIC_MODULE_LIMIT=256\n -D HTTPD_ROOT=\"/data/apache\"\n -D SUEXEC_BIN=\"/data/apache/bin/suexec\"\n -D DEFAULT_PIDLOG=\"logs/httpd.pid\"\n -D DEFAULT_SCOREBOARD=\"logs/apache_runtime_status\"\n -D DEFAULT_ERRORLOG=\"logs/error_log\"\n -D AP_TYPES_CONFIG_FILE=\"conf/mime.types\"\n -D SERVER_CONFIG_FILE=\"conf/httpd.conf\"\n\napache config:\n#MODULES\nLoadFile /usr/lib64/libxml2.so\nLoadFile /usr/lib64/liblua-5.1.so\nLoadModule security2_module modsecurity/lib/mod_security2.so\nLoadModule authz_core_module modules/mod_authz_core.so\nLoadModule mime_module modules/mod_mime.so\nLoadModule log_config_module modules/mod_log_config.so\nLoadModule headers_module modules/mod_headers.so\nLoadModule unique_id_module modules/mod_unique_id.so\nLoadModule socache_shmcb_module modules/mod_socache_shmcb.so\n#REVERSE_PROXY MODULES\nLoadModule proxy_module modules/mod_proxy.so\nLoadModule proxy_http_module modules/mod_proxy_http.so\nLoadModule ssl_module modules/mod_ssl.so\nLoadModule unixd_module modules/mod_unixd.so\nLoadModule proxy_balancer_module modules/mod_proxy_balancer.so\nLoadModule slotmem_shm_module modules/mod_slotmem_shm.so\nLoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so\nLoadModule proxy_hcheck_module modules/mod_proxy_hcheck.so\nLoadModule watchdog_module modules/mod_watchdog.so\n\nhc config:\n<Proxy \"balancer://xxx.xxx.com\">\n    BalancerMember \"http://xxx:8080\" #hcmethod=TCP hcinterval=2 hcpasses=3 hcfails=3\n    BalancerMember \"http://xxx:8080\" #hcmethod=TCP hcinterval=2 hcpasses=3 hcfails=3\n</Proxy>\n\ncoredump info:\nCore was generated by `/data/apache/bin/httpd -k start'.\nProgram terminated with signal 11, Segmentation fault.\n#0  0x00007fd8c4384169 in hc_get_hcworker (ctx=0x117af70, worker=0x1238f28, p=0x7fd8bc030e58) at mod_proxy_hcheck.c:426\n426\t        PROXY_STRNCPY(hc->s->name,     wptr);"}, {"count": 3, "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "id": 194591, "time": "2016-10-20T18:52:03Z", "bug_id": 60071, "creation_time": "2016-10-20T18:52:03Z", "is_private": false, "text": "It would be helpful to have the following additional information:\n\n1. The full stack trace of the crashing thread not just the last frame.\n2. Having debug logging enabled. You should see an error message with the number 03248 in the log just before the crash.\n3. In gdb please execute the following commands for the crashed thread:\n\nprint wptr\nprint *wptr\nprint hc\nprint hc->s\nprint hc->s->name\nprint *worker->s->name"}, {"attachment_id": null, "tags": [], "creator": "6731165@163.com", "text": "apache debug log:\n[Fri Oct 21 15:34:18.030981 2016] [proxy_balancer:debug] [pid 8773:tid 140072177952512] mod_proxy_balancer.c(132): AH01158: Looking at balancer://xxx.xxx.com -> http://x.x.x.x:8080 initialized?\n[Fri Oct 21 15:34:18.031014 2016] [proxy_balancer:debug] [pid 8774:tid 140072177952512] mod_proxy_balancer.c(132): AH01158: Looking at balancer://xxx.xxx.com -> http://x.x.x.x:8080 initialized?\n[Fri Oct 21 15:34:18.031056 2016] [proxy_balancer:debug] [pid 8775:tid 140072177952512] mod_proxy_balancer.c(132): AH01158: Looking at balancer://xxx.xxx.com -> http://x.x.x.x:8080 initialized?\n[Fri Oct 21 15:34:18.031212 2016] [proxy_balancer:debug] [pid 8773:tid 140072177952512] mod_proxy_balancer.c(132): AH01158: Looking at balancer://xxx.xxx.com -> http://x.x.x.x:8080 initialized?\n[Fri Oct 21 15:34:18.031236 2016] [proxy_balancer:debug] [pid 8774:tid 140072177952512] mod_proxy_balancer.c(132): AH01158: Looking at balancer://xxx.xxx.com -> http://x.x.x.x:8080 initialized?\n[Fri Oct 21 15:34:18.031262 2016] [proxy_balancer:debug] [pid 8775:tid 140072177952512] mod_proxy_balancer.c(132): AH01158: Looking at balancer://xxx.xxx.com -> http://x.x.x.x:8080 initialized?\n[Fri Oct 21 15:34:49.075240 2016] [core:notice] [pid 8439:tid 140072177952512] AH00051: child pid 8775 exit signal Segmentation fault (11), possible coredump in /tmp/test\n\nstrace:\n8439  <... clone resumed> child_stack=0, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f651868e9d0) = 8775\n8775  set_robust_list(0x7f651868e9e0, 24 <unfinished ...>\n8775  <... set_robust_list resumed> )   = 0\n8775  rt_sigaction(SIGTERM, {0x46fb10, [], SA_RESTORER|SA_INTERRUPT, 0x7f65175707e0}, {0x46b620, [], SA_RESTORER, 0x7f65175707e0}, 8) = 0\n8775  geteuid( <unfinished ...>\n8775  <... geteuid resumed> )           = 0\n8775  setgid(2 <unfinished ...>\n8775  <... setgid resumed> )            = 0\n8775  open(\"/proc/sys/kernel/ngroups_max\", O_RDONLY <unfinished ...>\n8775  <... open resumed> )              = 11\n8775  read(11,  <unfinished ...>\n8775  <... read resumed> \"65536\\n\", 31) = 6\n8775  close(11 <unfinished ...>\n8775  <... close resumed> )             = 0\n8775  open(\"/etc/group\", O_RDONLY|O_CLOEXEC <unfinished ...>\n8775  <... open resumed> )              = 11\n8775  fstat(11,  <unfinished ...>\n8775  <... fstat resumed> {st_mode=S_IFREG|0644, st_size=529, ...}) = 0\n8775  mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0 <unfinished ...>\n8775  <... mmap resumed> )              = 0x7f6518697000\n8775  lseek(11, 0, SEEK_CUR <unfinished ...>\n8775  <... lseek resumed> )             = 0\n8775  read(11,  <unfinished ...>\n8775  <... read resumed> \"root:x:0:\\nbin:x:1:bin,daemon\\ndae\"..., 4096) = 529\n8775  read(11,  <unfinished ...>\n8775  <... read resumed> \"\", 4096)      = 0\n8775  close(11 <unfinished ...>\n8775  <... close resumed> )             = 0\n8775  munmap(0x7f6518697000, 4096 <unfinished ...>\n8775  <... munmap resumed> )            = 0\n8775  setgroups(4, [2, 1, 4, 7] <unfinished ...>\n8775  <... setgroups resumed> )         = 0\n8775  geteuid( <unfinished ...>\n8775  <... geteuid resumed> )           = 0\n8775  setuid(2 <unfinished ...>\n8775  <... setuid resumed> )            = 0\n8775  prctl(PR_SET_DUMPABLE, 1 <unfinished ...>\n8775  <... prctl resumed> )             = 0\n8775  futex(0x7f6516dc5a80, FUTEX_WAKE_PRIVATE, 2147483647 <unfinished ...>\n8775  <... futex resumed> )             = 0\n8775  semop(3702787, {{0, -1, SEM_UNDO}}, 1 <unfinished ...>\n8775  <... semop resumed> )             = 0\n8775  semop(3702787, {{0, 1, SEM_UNDO}}, 1 <unfinished ...>\n8775  <... semop resumed> )             = 0\n8775  semop(3702787, {{0, -1, SEM_UNDO}}, 1 <unfinished ...>\n8775  <... semop resumed> )             = 0\n8775  semop(3702787, {{0, 1, SEM_UNDO}}, 1) = 0\n8775  write(8, \"[Fri Oct 21 15:34:18.031056 2016\"..., 210 <unfinished ...>\n8775  <... write resumed> )             = 210\n8775  write(8, \"[Fri Oct 21 15:34:18.031262 2016\"..., 210 <unfinished ...>\n8775  <... write resumed> )             = 210\n8775  semop(3702787, {{0, -1, SEM_UNDO}}, 1 <unfinished ...>\n8775  <... semop resumed> )             = 0\n8775  semop(3702787, {{0, 1, SEM_UNDO}}, 1) = 0\n8775  mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0x7f6512935000\n8775  mprotect(0x7f6512935000, 4096, PROT_NONE <unfinished ...>\n8775  <... mprotect resumed> )          = 0\n8775  clone( <unfinished ...>\n8775  <... clone resumed> child_stack=0x7f6513334ff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7f65133359d0, tls=0x7f6513335700, child_tidptr=0x7f65133359d0) = 8776\n8775  futex(0x23d46d8, FUTEX_WAIT_PRIVATE, 2, NULL <unfinished ...>\n8775  <... futex resumed> )             = -1 EAGAIN (Resource temporarily unavailable)\n8775  futex(0x23d46d8, FUTEX_WAKE_PRIVATE, 1 <unfinished ...>\n8775  <... futex resumed> )             = 0\n8775  rt_sigprocmask(SIG_SETMASK, ~[ILL TRAP ABRT BUS FPE SEGV USR2 PIPE SYS RTMIN RT_1],  <unfinished ...>\n8775  <... rt_sigprocmask resumed> NULL, 8) = 0\n8775  mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>\n8775  <... mmap resumed> )              = 0x7f6511f34000\n8775  mprotect(0x7f6511f34000, 4096, PROT_NONE <unfinished ...>\n8775  <... mprotect resumed> )          = 0\n8775  clone( <unfinished ...>\n8775  <... clone resumed> child_stack=0x7f6512933ff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7f65129349d0, tls=0x7f6512934700, child_tidptr=0x7f65129349d0) = 8781\n8775  rt_sigprocmask(SIG_UNBLOCK, [TERM],  <unfinished ...>\n8775  <... rt_sigprocmask resumed> NULL, 8) = 0\n8775  rt_sigaction(SIGTERM, {0x46b710, [], SA_RESTORER|SA_INTERRUPT, 0x7f65175707e0},  <unfinished ...>\n8775  <... rt_sigaction resumed> {0x46fb10, [], SA_RESTORER|SA_INTERRUPT, 0x7f65175707e0}, 8) = 0\n8775  read(6,  <unfinished ...>\n8786  <... getsockname resumed> {sa_family=AF_NETLINK, pid=8775, groups=00000000}, [12]) = 0\n8782  <... getsockname resumed> {sa_family=AF_NETLINK, pid=8775, groups=00000000}, [12]) = 0\n8843  --- SIGPIPE {si_signo=SIGPIPE, si_code=SI_USER, si_pid=8775, si_uid=2} ---\n8775  +++ killed by SIGSEGV (core dumped) +++\n8439  --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_KILLED, si_pid=8775, si_status=SIGSEGV, si_utime=402, si_stime=7} ---\n8439  wait4(-1, [{WIFSIGNALED(s) && WTERMSIG(s) == SIGSEGV && WCOREDUMP(s)}], WNOHANG|WSTOPPED, NULL) = 8775\n\ngdb:\n(gdb) thread 21\n[Switching to thread 21 (Thread 0x7f651868e700 (LWP 8775))]#0  0x00007f651756f82d in read () from /lib64/libpthread.so.0\n(gdb) print wptr\nNo symbol \"wptr\" in current context.\n\nI don't know how to use gdb.\nEnvironment:\ncentos 6.7 x64\napache 2.4.23 compiled with APR 1.5.2 and APR-UTIL 1.5.4\nopenssl 1.0.1u\npcre 8.39\nmodsecurity-2.9.1 compiled with yajl 2.1.0", "count": 4, "id": 194611, "time": "2016-10-21T08:22:47Z", "bug_id": 60071, "creation_time": "2016-10-21T08:22:47Z", "is_private": false}, {"attachment_id": 34601, "tags": [], "creator": "ylavic.dev@gmail.com", "text": "Created attachment 34601\nFix thread-safety in mod_proxy_hcheck\n\nCould you try this patch to address possible threads concurrency issues?", "count": 5, "id": 195921, "time": "2017-01-09T17:46:32Z", "bug_id": 60071, "creation_time": "2017-01-09T17:46:32Z", "is_private": false}, {"count": 6, "tags": [], "bug_id": 60071, "is_private": false, "text": "*** Bug 60553 has been marked as a duplicate of this bug. ***", "id": 195929, "time": "2017-01-09T18:09:51Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-01-09T18:09:51Z", "attachment_id": null}, {"count": 7, "tags": [], "text": "Seems crazy and inefficient that we are doing the below every single time:\n\n   req = apr_psprintf(ptemp,...\n\nWhy??", "is_private": false, "bug_id": 60071, "id": 195930, "time": "2017-01-09T18:26:08Z", "creator": "jim@apache.org", "creation_time": "2017-01-09T18:26:08Z", "attachment_id": null}, {"count": 8, "tags": [], "text": "Also, from what I can see, the common logic path and usage for hc_determine_connection() simply checks that worker->cp->addr isn't null and then assigns worker->cp->addr to worker->cp->addr... This seems wasteful and inefficient as well.", "is_private": false, "bug_id": 60071, "id": 195931, "time": "2017-01-09T18:39:02Z", "creator": "jim@apache.org", "creation_time": "2017-01-09T18:39:02Z", "attachment_id": null}, {"count": 9, "tags": [], "creator": "ylavic.dev@gmail.com", "attachment_id": null, "is_private": false, "id": 195932, "time": "2017-01-09T18:45:40Z", "bug_id": 60071, "creation_time": "2017-01-09T18:45:40Z", "text": "(In reply to Jim Jagielski from comment #7)\n> Seems crazy and inefficient that we are doing the below every single time:\n> \n>    req = apr_psprintf(ptemp,...\n> \n> Why??\n\nBecause ctx->p is not thread-safe here, and req is the request-line + Host header, not that huge...\n\nIs that a fast path, really?"}, {"count": 10, "tags": [], "bug_id": 60071, "attachment_id": null, "is_private": false, "id": 195933, "time": "2017-01-09T18:48:01Z", "creator": "jim@apache.org", "creation_time": "2017-01-09T18:48:01Z", "text": "We are generating the request for each and every health check, so yeah, it's a fast path."}, {"count": 11, "tags": [], "bug_id": 60071, "attachment_id": null, "is_private": false, "id": 195934, "time": "2017-01-09T18:50:26Z", "creator": "jim@apache.org", "creation_time": "2017-01-09T18:50:26Z", "text": "Plus, it should never change for each HC worker."}, {"count": 12, "tags": [], "text": "(In reply to Jim Jagielski from comment #8)\n> Also, from what I can see, the common logic path and usage for\n> hc_determine_connection() simply checks that worker->cp->addr isn't null and\n> then assigns worker->cp->addr to worker->cp->addr... This seems wasteful and\n> inefficient as well.\n\nThis is the same here AFAICT, hc_init_worker() calls hc_determine_connection() exactly once, otherwise it's called from hc_get_backend() which should either find an existing/valid worker->cp->addr, or NULL (in which case it must create a new (backend->)addr for each connection and be thread-safe too.", "is_private": false, "bug_id": 60071, "id": 195935, "time": "2017-01-09T18:52:59Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-01-09T18:52:59Z", "attachment_id": null}, {"count": 13, "tags": [], "bug_id": 60071, "is_private": false, "text": "(In reply to Jim Jagielski from comment #10)\n> We are generating the request for each and every health check, so yeah, it's\n> a fast path.\n\n(In reply to Jim Jagielski from comment #11)\n> Plus, it should never change for each HC worker.\n\nOK, so let's allocate it once at init time, my first goal here is to verify if it solves the issue(s)...", "id": 195936, "time": "2017-01-09T18:54:20Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-01-09T18:54:20Z", "attachment_id": null}, {"count": 14, "tags": [], "bug_id": 60071, "attachment_id": null, "is_private": false, "id": 195937, "time": "2017-01-09T18:55:12Z", "creator": "jim@apache.org", "creation_time": "2017-01-09T18:55:12Z", "text": "What we need is a pool that each HC worker can use."}, {"count": 15, "tags": [], "bug_id": 60071, "is_private": false, "text": "As a quick check, see if setting\n\n   ProxyHCTPsize 0\n\nresolves the issue. This avoids thread pools for the hcheck workers", "id": 195939, "time": "2017-01-09T19:34:15Z", "creator": "jim@apache.org", "creation_time": "2017-01-09T19:34:15Z", "attachment_id": null}, {"count": 16, "tags": [], "bug_id": 60071, "attachment_id": null, "text": "Hello,\n\nWe have applied the patch and apparently it works fine!!!\n\nAfter making jmeter tests during 27 hours and 485500 request the Segmentation Fault error hasn`t appeared and all request have gone ok.\n\nThanks and Good work!!!", "id": 196023, "time": "2017-01-12T13:55:34Z", "creator": "egandarias@gfi.es", "creation_time": "2017-01-12T13:55:34Z", "is_private": false}, {"count": 17, "tags": [], "text": "Created attachment 34618\nUpdated thread patch", "is_private": false, "bug_id": 60071, "id": 196029, "time": "2017-01-12T14:37:11Z", "creator": "jim@apache.org", "creation_time": "2017-01-12T14:37:11Z", "attachment_id": 34618}, {"count": 18, "tags": [], "text": "Can you check out the updated patch?\n\nThx again!", "is_private": false, "bug_id": 60071, "id": 196030, "time": "2017-01-12T14:37:41Z", "creator": "jim@apache.org", "creation_time": "2017-01-12T14:37:41Z", "attachment_id": null}, {"count": 19, "tags": [], "bug_id": 60071, "attachment_id": null, "is_private": false, "id": 196033, "time": "2017-01-12T15:04:31Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-01-12T15:04:31Z", "text": "(In reply to Jim Jagielski from comment #17)\n> Created attachment 34618 [details]\n> Updated thread patch\n\nNice, maybe put the method string in wctx too (at init time)?"}, {"count": 20, "tags": [], "text": "Great idea!", "attachment_id": null, "bug_id": 60071, "id": 196034, "time": "2017-01-12T15:20:40Z", "creator": "jim@apache.org", "creation_time": "2017-01-12T15:20:40Z", "is_private": false}, {"count": 21, "tags": [], "creator": "egandarias@gfi.es", "attachment_id": null, "id": 196035, "time": "2017-01-12T15:27:35Z", "bug_id": 60071, "creation_time": "2017-01-12T15:27:35Z", "is_private": false, "text": "ok....so, i'll wait to the last change"}, {"count": 22, "tags": [], "text": "(In reply to Jim Jagielski from comment #20)\n> Great idea!\n\nAlso it seems that we should get rid of ctx->ba, and probably use r->connection->bucket_alloc instead.\n\nWe could also probably avoid a subpool for r, ptemp is fine.", "attachment_id": null, "bug_id": 60071, "id": 196036, "time": "2017-01-12T15:39:56Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-01-12T15:39:56Z", "is_private": false}, {"count": 23, "tags": [], "creator": "jim@apache.org", "text": "Created attachment 34619\nUpdated patch w/ method\n\nUpdated thread-safety patch w/ method and req pre-prepared", "id": 196037, "time": "2017-01-12T15:54:31Z", "bug_id": 60071, "creation_time": "2017-01-12T15:54:31Z", "is_private": false, "attachment_id": 34619}, {"count": 24, "tags": [], "creator": "ylavic.dev@gmail.com", "attachment_id": 34620, "id": 196048, "time": "2017-01-12T22:36:16Z", "bug_id": 60071, "creation_time": "2017-01-12T22:36:16Z", "is_private": false, "text": "Created attachment 34620\nUpdated patch w/ method + single pool and bucket_alloc"}, {"count": 25, "tags": [], "text": "Can you try w/ the latest patch and let us know?\n\nThanks!!", "is_private": false, "bug_id": 60071, "id": 196122, "time": "2017-01-17T18:27:11Z", "creator": "jim@apache.org", "creation_time": "2017-01-17T18:27:11Z", "attachment_id": null}, {"attachment_id": null, "tags": [], "creator": "egandarias@gfi.es", "text": "Hello,\nWe have tested the patch today and in less than a hour we've obtained 8 Segmentation Faults...\n\nLogs in error log:\n\n[Wed Jan 18 11:02:01.358056 2017] [proxy:debug] [pid 32592:tid 140559909840640] proxy_util.c(2209): [client 10.136.101.140:62325] AH00944: connecting http://ejld1131.ejgvdns:8170/webhdfs/v1/data?op=GETFILESTATUS&user.name=root to ejld1131.ejgvdns:8170\n[Wed Jan 18 11:02:01.358067 2017] [proxy:debug] [pid 32592:tid 140559909840640] proxy_util.c(2418): [client 10.136.101.140:62325] AH00947: connected /webhdfs/v1/data?op=GETFILESTATUS&user.name=root to ejld1131.ejgvdns:8170\n[Wed Jan 18 11:02:01.359566 2017] [proxy:debug] [pid 32592:tid 140559909840640] proxy_util.c(2171): AH00943: http: has released connection for (ejld1131.ejgvdns)\n[Wed Jan 18 11:02:01.359590 2017] [proxy_balancer:debug] [pid 32592:tid 140559909840640] mod_proxy_balancer.c(688): [client 10.136.101.140:62325] AH01176: proxy_balancer_post_request for (balancer://cluster_namenode)\n[Wed Jan 18 11:02:01.373675 2017] [core:notice] [pid 29786:tid 140560333223744] AH00051: child pid 4685 exit signal Segmentation fault (11), possible coredump in /tmp\n[Wed Jan 18 11:02:10.776046 2017] [proxy_balancer:debug] [pid 9243:tid 140560027338496] mod_proxy_balancer.c(688): [client 10.136.101\n.140:62342] AH01176: proxy_balancer_post_request for (balancer://cluster_namenode)\n[Wed Jan 18 11:02:10.794876 2017] [watchdog:debug] [pid 3222:tid 140560165148416] mod_watchdog.c(159): AH02972: Singleton Watchdog (_proxy_hcheck_) running\n[Wed Jan 18 11:02:10.794991 2017] [proxy_hcheck:debug] [pid 3222:tid 140560165148416] mod_proxy_hcheck.c(888): AH03258: _proxy_hcheck_ watchdog started.\n[Wed Jan 18 11:02:10.796926 2017] [proxy_hcheck:debug] [pid 3222:tid 140560165148416] mod_proxy_hcheck.c(902): AH03313: apr_thread_pool_create() with 16 threads succeeded\n[Wed Jan 18 11:02:10.979604 2017] [authz_core:debug] [pid 9243:tid 140560027338496] mod_authz_core.c(835): [client 10.136.101.140:62342] AH01628: authorization result: granted (no directives)\n[Wed Jan 18 11:02:12.800263 2017] [proxy:debug] [pid 3222:tid 140560165148416] proxy_util.c(1856): AH00930: initialized pool in child\n 3222 for (ejld1132.ejgvdns) min=0 max=25 smax=25\n[Wed Jan 18 11:02:12.800440 2017] [proxy_hcheck:debug] [pid 3222:tid 140560165148416] mod_proxy_hcheck.c(457): AH03248: Creating hc worker 20a0690 for http://ejld1132.ejgvdns:8170\n[Wed Jan 18 11:02:12.800479 2017] [proxy:debug] [pid 3222:tid 140560165148416] proxy_util.c(1779): AH00925: initializing worker 20a0690 shared\n[Wed Jan 18 11:02:12.800488 2017] [proxy:debug] [pid 3222:tid 140560165148416] proxy_util.c(1821): AH00927: initializing worker 20a0690 local\n[Wed Jan 18 11:02:12.800513 2017] [proxy:debug] [pid 3222:tid 140560165148416] proxy_util.c(1856): AH00930: initialized pool in child 3222 for (ejld1132.ejgvdns) min=0 max=25 smax=25\n[Wed Jan 18 11:02:12.800571 2017] [proxy_hcheck:debug] [pid 3222:tid 140560165148416] mod_proxy_hcheck.c(457): AH03248: Creating hc worker 20a0c70 for http://ejld1131.ejgvdns:8170\n[Wed Jan 18 11:02:12.800595 2017] [proxy:debug] [pid 3222:tid 140560165148416] proxy_util.c(1779): AH00925: initializing worker 20a0c70 shared\n[Wed Jan 18 11:02:12.800603 2017] [proxy:debug] [pid 3222:tid 140560165148416] proxy_util.c(1821): AH00927: initializing worker 20a0c70 local\n[Wed Jan 18 11:02:12.800619 2017] [proxy:debug] [pid 3222:tid 140560165148416] proxy_util.c(1856): AH00930: initialized pool in child 3222 for (ejld1131.ejgvdns) min=0 max=25 smax=25\n[Wed Jan 18 11:02:12.800623 2017] [proxy_hcheck:debug] [pid 3222:tid 140559523972864] mod_proxy_hcheck.c(819): AH03256: Threaded Health checking http://ejld1132.ejgvdns:8170\n[Wed Jan 18 11:02:12.800710 2017] [proxy_hcheck:debug] [pid 3222:tid 140559641470720] mod_proxy_hcheck.c(819): AH03256: Threaded Health checking http://ejld1131.ejgvdns:8170\n[Wed Jan 18 11:02:12.800874 2017] [proxy:debug] [pid 3222:tid 140559523972864] proxy_util.c(2156): AH00942: HCOH: has acquired connection for (ejld1132.ejgvdns)\n[Wed Jan 18 11:02:12.800900 2017] [proxy:debug] [pid 3222:tid 140559641470720] proxy_util.c(2156): AH00942: HCOH: has acquired connection for (ejld1131.ejgvdns)\n[Wed Jan 18 11:02:12.801249 2017] [proxy:debug] [pid 3222:tid 140559523972864] proxy_util.c(2884): AH02824: HCOH: connection established with 10.190.40.107:8170 (ejld1132.ejgvdns)\n[Wed Jan 18 11:02:12.801293 2017] [proxy:debug] [pid 3222:tid 140559523972864] proxy_util.c(3051): AH00962: HCOH: connection complete to 10.190.40.107:8170 (ejld1132.ejgvdns)\n[Wed Jan 18 11:02:12.801412 2017] [proxy:debug] [pid 3222:tid 140559641470720] proxy_util.c(2884): AH02824: HCOH: connection established with 10.190.40.106:8170 (ejld1131.ejgvdns)\n[Wed Jan 18 11:02:12.801445 2017] [proxy:debug] [pid 3222:tid 140559641470720] proxy_util.c(3051): AH00962: HCOH: connection complete to 10.190.40.106:8170 (ejld1131.ejgvdns)\n[Wed Jan 18 11:02:12.805093 2017] [proxy_hcheck:debug] [pid 3222:tid 140559523972864] mod_proxy_hcheck.c(626): AH03254: HTTP/1.1 403 Forbidden\n[Wed Jan 18 11:02:12.805173 2017] [proxy:debug] [pid 3222:tid 140559523972864] proxy_util.c(2171): AH00943: HCOH: has released connection for (ejld1132.ejgvdns)\n[Wed Jan 18 11:02:12.805278 2017] [proxy_hcheck:debug] [pid 3222:tid 140559523972864] mod_proxy_hcheck.c(560): AH03251: Health check GET Status (1) for 20a0690.\n[Wed Jan 18 11:02:12.806221 2017] [proxy_hcheck:debug] [pid 3222:tid 140559641470720] mod_proxy_hcheck.c(626): AH03254: HTTP/1.1 200 OK\n[Wed Jan 18 11:02:12.806274 2017] [proxy:debug] [pid 3222:tid 140559641470720] proxy_util.c(2171): AH00943: HCOH: has released connection for (ejld1131.ejgvdns)\n[Wed Jan 18 11:02:12.806331 2017] [proxy_hcheck:debug] [pid 3222:tid 140559641470720] mod_proxy_hcheck.c(560): AH03251: Health check GET Status (0) for 20a0c70.\n[Wed Jan 18 11:02:12.822328 2017] [authz_core:debug] [pid 9243:tid 140560027338496] mod_authz_core.c(835): [client 10.136.101.140:62342] AH01628: authorization result: granted (no directives)\n[Wed Jan 18 11:02:12.822448 2017] [lbmethod_byrequests:debug] [pid 9243:tid 140560027338496] mod_lbmethod_byrequests.c(97): AH01207: proxy: Entering byrequests for BALANCER (balancer://cluster_namenode)\n[Wed Jan 18 11:02:12.822458 2017] [lbmethod_byrequests:debug] [pid 9243:tid 140560027338496] mod_lbmethod_byrequests.c(144): AH01208: proxy: byrequests selected worker \"http://ejld1131.ejgvdns:8170\" : busy 0 : lbstatus 0\n[Wed Jan 18 11:02:12.822517 2017] [proxy_balancer:debug] [pid 9243:tid 140560027338496] mod_proxy_balancer.c(632): [client 10.136.101.140:62342] AH01172: balancer://cluster_namenode: worker (http://ejld1131.ejgvdns:8170) rewritten to http://ejld1131.ejgvdns:8170/webhdfs/v1/data?op=GETFILESTATUS&user.name=root\n\n\n\nCore dump\n\n(gdb) bt full\n#0  ap_core_output_filter (f=0x7fd6800090f8, new_bb=0x7fd688000a60) at core_filters.c:491\n        c = 0x7fd680008bf0\n        net = 0x7fd6800090b0\n        ctx = 0x7fd680009140\n        bb = 0x7fd688000a60\n        bucket = 0x1\n        next = <optimized out>\n        flush_upto = 0x1\n        bytes_in_brigade = 0\n        non_file_bytes_in_brigade = 0\n        eor_buckets_in_brigade = 0\n        morphing_bucket_in_brigade = 0\n        rv = <optimized out>\n#1  0x00007fd6b7976c03 in hc_send (ctx=0x2085678, ctx=0x2085678, backend=0x7fd69c026e80, out=<optimized out>, ptemp=0x7fd6880008e8)\n    at mod_proxy_hcheck.c:612\n        tmp_bb = 0x7fd688000a60\n#2  hc_check_http (baton=0x7fd688000960) at mod_proxy_hcheck.c:771\n        backend = 0x7fd69c026e80\n        ctx = 0x2085678\n        status = <optimized out>\n        ptemp = 0x7fd6880008e8\n        method = 0x7fd6b7978438 \"GET\"\n        hc = 0x2088528\n        worker = 0x20a0690\n        c = {pool = 0x0, base_server = 0x0, vhost_lookup_data = 0x0, local_addr = 0x0, client_addr = 0x0, client_ip = 0x0,\n          remote_host = 0x0, remote_logname = 0x0, local_ip = 0x7fd69c020cb0 \"\\370\\f\\002\\234\\326\\177\",\n          local_host = 0x3f1713a <Address 0x3f1713a out of bounds>, id = 140559784145016, conn_config = 0x7fd69c020ce0,\n          notes = 0x7fd69c020c10, input_filters = 0x7fd6c0119876 <find_entry+134>, output_filters = 0x7fd6a000f0d8, sbh = 0x14,\n          bucket_alloc = 0x0, cs = 0x7fd69c020c10, data_in_input_filters = -1610551080, data_in_output_filters = 32726,\n          clogging_input_filters = 0, double_reverse = -1, aborted = 32726, keepalive = (unknown: 2684416216), keepalives = 32726,\n          log = 0x7fd6907f0700, log_id = 0x1 <Address 0x1 out of bounds>, current_thread = 0x7fd6c0119c2f <apr_hash_set+15>,\n          master = 0x7fd69c020b98}\n        wctx = <optimized out>\n        cond = <optimized out>\n#3  hc_check (thread=0x7fd6a000edc8, b=0x7fd688000960) at mod_proxy_hcheck.c:829\n---Type <return> to continue, or q <return> to quit---\n        baton = 0x7fd688000960\n        s = 0x203b5e8\n        worker = 0x20a0690\n        now = 1484733718004809\n        rv = <optimized out>\n#4  0x00007fd6c057aedd in thread_pool_func (t=0x7fd6a000edc8, param=0x2088390) at misc/apr_thread_pool.c:266\n        me = 0x2088390\n        task = 0x7fd6a000f0d8\n        wait = <optimized out>\n        __PRETTY_FUNCTION__ = \"thread_pool_func\"\n#5  0x00007fd6bfab3dc5 in start_thread () from /lib64/libpthread.so.0\nNo symbol table info available.\n#6  0x00007fd6bf5dd28d in clone () from /lib64/libc.so.6\n\nThanks", "count": 26, "id": 196134, "time": "2017-01-18T10:18:21Z", "bug_id": 60071, "creation_time": "2017-01-18T10:18:21Z", "is_private": false}, {"count": 27, "tags": [], "bug_id": 60071, "attachment_id": null, "text": "(In reply to Endika from comment #26)\n> We have tested the patch today and in less than a hour we've obtained 8\n> Segmentation Faults...\n\nI'm not sure which patch you tested, attachment 34620 ?\n\n> \n> \n> Core dump\n> \n> (gdb) bt full\n[...]\n> #1  0x00007fd6b7976c03 in hc_send (ctx=0x2085678, ctx=0x2085678,\n> backend=0x7fd69c026e80, out=<optimized out>, ptemp=0x7fd6880008e8)\n>     at mod_proxy_hcheck.c:612\n>         tmp_bb = 0x7fd688000a60\n\nThere is no tmp_bb is the latest patch, which precisely addresses possible issues with ctx->ba.\n\nAny chance this test was with attachment 34619 or earlier (now obsolete/hidden above) ?", "id": 196141, "time": "2017-01-18T11:22:51Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-01-18T11:22:51Z", "is_private": false}, {"count": 28, "tags": [], "bug_id": 60071, "attachment_id": null, "text": "Sorry, my error...i've patched with the wrong code. \n\nI'll try again\n\nEndika", "id": 196144, "time": "2017-01-18T14:35:07Z", "creator": "egandarias@gfi.es", "creation_time": "2017-01-18T14:35:07Z", "is_private": false}, {"count": 29, "tags": [], "text": "ok, just for clearify....\n\nI\u00b4m patching the original version of .c file (version 2.4.25). \norig.c + attach 34601\n\nTrying to do comulative patches, with the second one i obtained an error \norig.c + attach 34601 + attach 34618\n\nso i patched the original file with the second patch file...\n\norig.c + attach 34618.\n\nFrom that moment i've always made the same proccess. I've patched the original .c file with the next patch.\n\nWhat is the exact proccess to follow for patching the files?\n\nThanks \n\nEndika", "attachment_id": null, "bug_id": 60071, "id": 196145, "time": "2017-01-18T14:42:25Z", "creator": "egandarias@gfi.es", "creation_time": "2017-01-18T14:42:25Z", "is_private": false}, {"count": 30, "tags": [], "bug_id": 60071, "is_private": false, "text": "Applying the last patch both workers of the balancer are in status (1), but one should be ok!!!!\n\n----> HTTP/1.1 200 OK\n\n\n\n[Wed Jan 18 15:43:51.225390 2017] [proxy:debug] [pid 28953:tid 140683774117632] proxy_util.c(2884): AH02824: HCOH: connection established with 10.190.40.107:8170 (ejld1132.ejgvdns)\n[Wed Jan 18 15:43:51.225389 2017] [proxy_hcheck:debug] [pid 28953:tid 140683782510336] mod_proxy_hcheck.c(802): AH03256: Threaded Health checking http://ejld1131.ejgvdns:8170\n[Wed Jan 18 15:43:51.225440 2017] [proxy:debug] [pid 28953:tid 140683782510336] proxy_util.c(2156): AH00942: HCOH: has acquired connection for (ejld1131.ejgvdns)\n[Wed Jan 18 15:43:51.225445 2017] [proxy:debug] [pid 28953:tid 140683774117632] proxy_util.c(3051): AH00962: HCOH: connection complete to 10.190.40.107:8170 (ejld1132.ejgvdns)\n[Wed Jan 18 15:43:51.226028 2017] [proxy:debug] [pid 28953:tid 140683782510336] proxy_util.c(2884): AH02824: HCOH: connection established with 10.190.40.106:8170 (ejld1131.ejgvdns)\n[Wed Jan 18 15:43:51.226047 2017] [proxy:debug] [pid 28953:tid 140683782510336] proxy_util.c(3051): AH00962: HCOH: connection complete to 10.190.40.106:8170 (ejld1131.ejgvdns)\n[Wed Jan 18 15:43:51.228620 2017] [proxy_hcheck:debug] [pid 28953:tid 140683774117632] mod_proxy_hcheck.c(628): AH03254: HTTP/1.1 403 Forbidden\n[Wed Jan 18 15:43:51.228662 2017] [proxy:debug] [pid 28953:tid 140683774117632] proxy_util.c(2171): AH00943: HCOH: has released connection for (ejld1132.ejgvdns)\n[Wed Jan 18 15:43:51.228719 2017] [proxy_hcheck:debug] [pid 28953:tid 140683774117632] mod_proxy_hcheck.c(559): AH03251: Health check GET Status (1) for 226c690.\n[Wed Jan 18 15:43:51.230431 2017] [proxy_hcheck:debug] [pid 28953:tid 140683782510336] mod_proxy_hcheck.c(628): AH03254: HTTP/1.1 200 OK\n[Wed Jan 18 15:43:51.230481 2017] [proxy:debug] [pid 28953:tid 140683782510336] proxy_util.c(2171): AH00943: HCOH: has released connection for (ejld1131.ejgvdns)\n[Wed Jan 18 15:43:51.230508 2017] [proxy_hcheck:debug] [pid 28953:tid 140683782510336] mod_proxy_hcheck.c(559): AH03251: Health check GET Status (1) for 226cc70.\n\nThanks\n\nEndika", "id": 196146, "time": "2017-01-18T14:56:43Z", "creator": "egandarias@gfi.es", "creation_time": "2017-01-18T14:56:43Z", "attachment_id": null}, {"count": 31, "tags": [], "creator": "ylavic.dev@gmail.com", "attachment_id": null, "id": 196148, "time": "2017-01-18T15:15:21Z", "bug_id": 60071, "creation_time": "2017-01-18T15:15:21Z", "is_private": false, "text": "(In reply to Endika from comment #30)\n> Applying the last patch \n\nThis attachment https://bz.apache.org/bugzilla/attachment.cgi?id=34620 right?\nIt's not cumulative, applies directly on 2.4.25's modules/proxy/mod_proxy_hcheck.c.\n\n> both workers of the balancer are in status (1), but\n> one should be ok!!!!\n\nCould you set \"LogLevel trace7\" please?"}, {"count": 32, "tags": [], "bug_id": 60071, "attachment_id": null, "text": "Double checked the patch applied and it's the correct one\n\nThese are the logs with LogLevel trace7\n\n[Wed Jan 18 16:20:58.439498 2017] [proxy_hcheck:trace2] [pid 20265:tid 139992545253120] mod_proxy_hcheck.c(912): Checking balancer://cluster_namenode worker: http://ejld1131.ejgvdns:8170  [4] (18e4c70)\n[Wed Jan 18 16:20:58.439541 2017] [proxy_hcheck:debug] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(802): AH03256: Threaded Health checking http://ejld1132.ejgvdns:8170\n[Wed Jan 18 16:20:58.439552 2017] [proxy:debug] [pid 20265:tid 139992445744896] proxy_util.c(2156): AH00942: HCOH: has acquired connection for (ejld1132.ejgvdns)\n[Wed Jan 18 16:20:58.439724 2017] [proxy:trace2] [pid 20265:tid 139992445744896] proxy_util.c(2850): HCOH: fam 2 socket created to connect to ejld1132.ejgvdns\n[Wed Jan 18 16:20:58.439773 2017] [proxy_hcheck:debug] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(802): AH03256: Threaded Health checking http://ejld1131.ejgvdns:8170\n[Wed Jan 18 16:20:58.439819 2017] [proxy:debug] [pid 20265:tid 139992445744896] proxy_util.c(2884): AH02824: HCOH: connection established with 10.190.40.107:8170 (ejld1132.ejgvdns)\n[Wed Jan 18 16:20:58.439827 2017] [proxy:debug] [pid 20265:tid 139992437352192] proxy_util.c(2156): AH00942: HCOH: has acquired connection for (ejld1131.ejgvdns)\n[Wed Jan 18 16:20:58.439841 2017] [proxy:debug] [pid 20265:tid 139992445744896] proxy_util.c(3051): AH00962: HCOH: connection complete to 10.190.40.107:8170 (ejld1132.ejgvdns)\n[Wed Jan 18 16:20:58.439859 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(609): GET /webhdfs/v1/data?op=GETFILESTATUS HTTP/1.0\\r\\nHost: ejld1132.ejgvdns:8170\\r\\n\\r\\n\n[Wed Jan 18 16:20:58.439870 2017] [core:trace6] [pid 20265:tid 139992445744896] core_filters.c(525): [remote 10.190.40.107:8170] core_output_filter: flushing because of FLUSH bucket\n[Wed Jan 18 16:20:58.439933 2017] [proxy:trace2] [pid 20265:tid 139992437352192] proxy_util.c(2850): HCOH: fam 2 socket created to connect to ejld1131.ejgvdns\n[Wed Jan 18 16:20:58.440223 2017] [proxy:debug] [pid 20265:tid 139992437352192] proxy_util.c(2884): AH02824: HCOH: connection established with 10.190.40.106:8170 (ejld1131.ejgvdns)\n[Wed Jan 18 16:20:58.440260 2017] [proxy:debug] [pid 20265:tid 139992437352192] proxy_util.c(3051): AH00962: HCOH: connection complete to 10.190.40.106:8170 (ejld1131.ejgvdns)\n[Wed Jan 18 16:20:58.440276 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(609): GET /webhdfs/v1/data?op=GETFILESTATUS HTTP/1.0\\r\\nHost: ejld1131.ejgvdns:8170\\r\\n\\r\\n\n[Wed Jan 18 16:20:58.440282 2017] [core:trace6] [pid 20265:tid 139992437352192] core_filters.c(525): [remote 10.190.40.106:8170] core_output_filter: flushing because of FLUSH bucket\n[Wed Jan 18 16:20:58.443916 2017] [proxy_hcheck:debug] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(628): AH03254: HTTP/1.1 403 Forbidden\n[Wed Jan 18 16:20:58.443962 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Cache-Control: no-cache\n[Wed Jan 18 16:20:58.443975 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Expires: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.443983 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Date: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.443990 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Pragma: no-cache\n[Wed Jan 18 16:20:58.444020 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Expires: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.444030 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Date: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.444034 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Pragma: no-cache\n[Wed Jan 18 16:20:58.444045 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Content-Type: application/json\n[Wed Jan 18 16:20:58.444085 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(662): Server: Jetty(6.1.26.hwx)\n[Wed Jan 18 16:20:58.444103 2017] [proxy:debug] [pid 20265:tid 139992445744896] proxy_util.c(2171): AH00943: HCOH: has released connection for (ejld1132.ejgvdns)\n[Wed Jan 18 16:20:58.444165 2017] [proxy_hcheck:debug] [pid 20265:tid 139992445744896] mod_proxy_hcheck.c(559): AH03251: Health check GET Status (1) for 18e4690.\n[Wed Jan 18 16:20:58.445331 2017] [proxy_hcheck:debug] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(628): AH03254: HTTP/1.1 200 OK\n[Wed Jan 18 16:20:58.445353 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Cache-Control: no-cache\n[Wed Jan 18 16:20:58.445359 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Expires: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.445370 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Date: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.445375 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Pragma: no-cache\n[Wed Jan 18 16:20:58.445379 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Expires: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.445383 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Date: Wed, 18 Jan 2017 15:20:58 GMT\n[Wed Jan 18 16:20:58.445387 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Pragma: no-cache\n[Wed Jan 18 16:20:58.445390 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Content-Type: application/json\n[Wed Jan 18 16:20:58.445394 2017] [proxy_hcheck:trace7] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(662): Server: Jetty(6.1.26.hwx)\n[Wed Jan 18 16:20:58.445402 2017] [proxy:debug] [pid 20265:tid 139992437352192] proxy_util.c(2171): AH00943: HCOH: has released connection for (ejld1131.ejgvdns)\n[Wed Jan 18 16:20:58.445436 2017] [proxy_hcheck:debug] [pid 20265:tid 139992437352192] mod_proxy_hcheck.c(559): AH03251: Health check GET Status (1) for 18e4c70.\n[Wed Jan 18 16:21:00.442224 2017] [proxy_hcheck:trace2] [pid 20265:tid 139992545253120] mod_proxy_hcheck.c(891): Run of _proxy_hcheck_ watchdog.", "id": 196149, "time": "2017-01-18T15:24:06Z", "creator": "egandarias@gfi.es", "creation_time": "2017-01-18T15:24:06Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "jim@apache.org", "text": "Hold on... These patches are wrong since they prevent the health checks from being changed via the balancer-manager application (because hc_get_hcworker is only called once per start...\n\nI think we need to rethink this...\n\nUsing the orig code as a start, can we just create a mutex to make the allocs thread-safe now? That seems like a \"cleaner\" solution, assuming that that is the root cause.", "count": 33, "id": 196150, "time": "2017-01-18T15:37:25Z", "bug_id": 60071, "creation_time": "2017-01-18T15:37:25Z", "is_private": false}, {"count": 34, "tags": [], "creator": "jim@apache.org", "attachment_id": null, "id": 196151, "time": "2017-01-18T15:44:13Z", "bug_id": 60071, "creation_time": "2017-01-18T15:44:13Z", "is_private": false, "text": "If possible, go back to the original, unmatched version of the module and see if setting ProxyHCTPsize 0 resolves the problem.\n\nIf so, then that conclusively indicates a thread concurrent issue.\n\nThanks!"}, {"count": 35, "tags": [], "creator": "ylavic.dev@gmail.com", "attachment_id": null, "is_private": false, "id": 196152, "time": "2017-01-18T15:44:56Z", "bug_id": 60071, "creation_time": "2017-01-18T15:44:56Z", "text": "(In reply to Jim Jagielski from comment #33)\n> Hold on... These patches are wrong since they prevent the health checks from\n> being changed via the balancer-manager application (because hc_get_hcworker\n> is only called once per start...\n\nHmm, could you elaborate?\n\nhc_get_hcworker() is called for each worker on each watchdog run, this part did not change AFAICT.\n\nBTW, it looks like the \"method\" only can be changed with the manager, always been so..."}, {"count": 36, "tags": [], "text": "OK... I see that it was moved from hc_check_http and hc_check_tcp to the actual baton creation.\n\nI still wonder if we are overcomplicating things by such a drastic restructuring... having a hard time groking the various changes :)", "is_private": false, "bug_id": 60071, "id": 196153, "time": "2017-01-18T15:57:05Z", "creator": "jim@apache.org", "creation_time": "2017-01-18T15:57:05Z", "attachment_id": null}, {"count": 37, "tags": [], "bug_id": 60071, "attachment_id": null, "text": "Ok, i am making tests with original module and \n\nProxyHCTPsize 0\n\nThanks", "id": 196155, "time": "2017-01-18T16:10:16Z", "creator": "egandarias@gfi.es", "creation_time": "2017-01-18T16:10:16Z", "is_private": false}, {"count": 38, "tags": [], "text": "Created attachment 34642\nThreads safety + single pool/bucket_alloc\n\nThe previous patch did not handle APR_EOF correctly while reading response body, now fixed.\n\nAfter the test with unpatched 2.4.25 and \"ProxyHCTPsize 0\" (proposed by Jim) to valide that multithreading is the issue, could you please try this new patch (with default/no ProxyHCTPsize this time) to verify that multithreading would work (without locking)?\n\nMultithreading is nice when (some) backends start to respond slowly/timeout.\nBTW, we probably shouldn't start a new check for a backend that did not finish with the previous one (should its timeout be above the watchdog's pediod), but that's a different story we won't ask you to test here ;)\n\nThanks Endika for all this testing.", "attachment_id": 34642, "bug_id": 60071, "id": 196157, "time": "2017-01-18T18:43:56Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-01-18T18:43:56Z", "is_private": false}, {"count": 39, "tags": [], "creator": "jim@apache.org", "attachment_id": null, "id": 196158, "time": "2017-01-18T19:27:53Z", "bug_id": 60071, "creation_time": "2017-01-18T19:27:53Z", "is_private": false, "text": "My VERY simple testing looks good!"}, {"count": 40, "tags": [], "creator": "egandarias@gfi.es", "text": "Hi!!\n\nAfter 18 hours and 305000 requests i haven't obtained any error with original module and \"ProxyHCTPsize 0\"\n\nNow I'll prepare the test with the last patch\n\nThanks!!!", "id": 196193, "time": "2017-01-19T09:24:01Z", "bug_id": 60071, "creation_time": "2017-01-19T09:24:01Z", "is_private": false, "attachment_id": null}, {"count": 41, "tags": [], "text": "Hello!\n\nGreat news!!! With the last patch after 21 hours and 380000 tests i've got no errors!!!\n\nSo it seems to be fixed!!\n\nGood work!!!\n\nThanks", "is_private": false, "bug_id": 60071, "id": 196232, "time": "2017-01-20T07:21:32Z", "creator": "egandarias@gfi.es", "creation_time": "2017-01-20T07:21:32Z", "attachment_id": null}, {"count": 42, "tags": [], "creator": "egandarias@gfi.es", "attachment_id": null, "id": 197483, "time": "2017-03-06T16:02:03Z", "bug_id": 60071, "creation_time": "2017-03-06T16:02:03Z", "is_private": false, "text": "Hello,\n\nWhen will the solution be included in a release?\n\nThanks"}, {"count": 43, "tags": [], "bug_id": 60071, "is_private": false, "text": "Hello,\n\nSomething new about when this patch will be included in the next release?\n\nBest regards", "id": 197869, "time": "2017-03-21T14:44:38Z", "creator": "egandarias@gfi.es", "creation_time": "2017-03-21T14:44:38Z", "attachment_id": null}, {"count": 44, "tags": [], "bug_id": 60071, "attachment_id": null, "is_private": false, "id": 198407, "time": "2017-04-20T13:20:07Z", "creator": "jim@apache.org", "creation_time": "2017-04-20T13:20:07Z", "text": "Fixed in truck and will be in 2.4.26"}]