[{"count": 0, "tags": [], "text": "Whenever I try to load a simple, 500k file, I find that my CPU usage goes up to\n98% and my memory slowly gets eaten... very slowly.  The file never loads.  But\nwhen I try to do this with a much smaller file (between 50-100k), I have no\nproblems.  This happens regardless of whether I use the usermodel or the\neventmodel.  Here is the code I am using for the usermodel:\n\npublic class LoadTest {\n\n  public static void main(String[] args) {\n\n    HSSFWorkbook workbook;\n    File inputFile = new File(\"big_test.xls\");\n    try {\n    InputStream fileIn = new FileInputStream(inputFile);\n    fileIn = new BufferedInputStream(fileIn);\n    workbook = new HSSFWorkbook(fileIn);\n    fileIn.close();\n    } catch (FileNotFoundException ex) {\n    } catch (IOException ex) { }\n  }\n\n}\n\nAnd for the eventmodel, I am using the example from the HSSF howto homepage.\nIt gets this far:\n\nEncountered workbook\nNew sheet named: Sheet1\nNew sheet named: Sheet2\nNew sheet named: Sheet3\n\nAnd then it hangs the same way the usermodel did.\nI have also had this problem on a Windows XP box, and I've tried it with what\nappears to be a recent CVS build (poi-2.1-20050221).  It never works.  What is\ngoing on here?", "attachment_id": null, "bug_id": 33681, "id": 71330, "time": "2005-02-22T00:41:53Z", "creator": "mserra@iwu.edu", "creation_time": "2005-02-22T00:41:53Z", "is_private": false}, {"count": 1, "tags": [], "bug_id": 33681, "attachment_id": 14339, "id": 71331, "time": "2005-02-22T00:52:24Z", "creator": "mserra@iwu.edu", "creation_time": "2005-02-22T00:52:24Z", "is_private": false, "text": "Created attachment 14339\nThe file that won't open"}, {"count": 2, "tags": [], "bug_id": 33681, "text": "I should add that I think the determining factor is not the file size, it is the\nnumber of rows.  I can easily load a file with more total kb but fewer rows.\nThe file that will not load has over 1000 rows.\n\n(In reply to comment #0)\n> Whenever I try to load a simple, 500k file, I find that my CPU usage goes up to\n> 98% and my memory slowly gets eaten... very slowly.  The file never loads.  But\n> when I try to do this with a much smaller file (between 50-100k), I have no\n> problems.  This happens regardless of whether I use the usermodel or the\n> eventmodel.  Here is the code I am using for the usermodel:\n> \n> public class LoadTest {\n> \n>   public static void main(String[] args) {\n> \n>     HSSFWorkbook workbook;\n>     File inputFile = new File(\"big_test.xls\");\n>     try {\n>     InputStream fileIn = new FileInputStream(inputFile);\n>     fileIn = new BufferedInputStream(fileIn);\n>     workbook = new HSSFWorkbook(fileIn);\n>     fileIn.close();\n>     } catch (FileNotFoundException ex) {\n>     } catch (IOException ex) { }\n>   }\n> \n> }\n> \n> And for the eventmodel, I am using the example from the HSSF howto homepage.\n> It gets this far:\n> \n> Encountered workbook\n> New sheet named: Sheet1\n> New sheet named: Sheet2\n> New sheet named: Sheet3\n> \n> And then it hangs the same way the usermodel did.\n> I have also had this problem on a Windows XP box, and I've tried it with what\n> appears to be a recent CVS build (poi-2.1-20050221).  It never works.  What is\n> going on here?\n\n", "id": 71332, "time": "2005-02-22T00:53:59Z", "creator": "mserra@iwu.edu", "creation_time": "2005-02-22T00:53:59Z", "is_private": false, "attachment_id": null}, {"count": 3, "attachment_id": null, "creator": "avik@apache.org", "text": "I can verify that this is indeed the case with the attached file. ", "id": 75375, "time": "2005-05-25T20:25:03Z", "bug_id": 33681, "creation_time": "2005-05-25T20:25:03Z", "tags": [], "is_private": false}, {"count": 4, "tags": [], "bug_id": 33681, "attachment_id": null, "id": 75377, "time": "2005-05-25T20:56:18Z", "creator": "mserra@iwu.edu", "creation_time": "2005-05-25T20:56:18Z", "is_private": false, "text": "(In reply to comment #3)\n> I can verify that this is indeed the case with the attached file. \n\nAfter reporting this bug, I added that I thought the problem was having too many\nrows in the file, rather than simply too large of a file.\n\nSince then I've found that creating a file with an equal number of rows, but\nwith numeric data rather than string data, causes no problem.  So, I'm pretty\nsure the issue here is having a file with too many strings."}, {"count": 5, "tags": [], "text": "(In reply to comment #4)\n> (In reply to comment #3)\n> > I can verify that this is indeed the case with the attached file. \n> \n> After reporting this bug, I added that I thought the problem was having too many\n> rows in the file, rather than simply too large of a file.\n> \n> Since then I've found that creating a file with an equal number of rows, but\n> with numeric data rather than string data, causes no problem.  So, I'm pretty\n> sure the issue here is having a file with too many strings.\n\n(In reply to comment #4)\n> (In reply to comment #3)\n> > I can verify that this is indeed the case with the attached file. \n> \n> After reporting this bug, I added that I thought the problem was having too many\n> rows in the file, rather than simply too large of a file.\n> \n> Since then I've found that creating a file with an equal number of rows, but\n> with numeric data rather than string data, causes no problem.  So, I'm pretty\n> sure the issue here is having a file with too many strings.\n\nI have verified this problem under the cited scenario as well.  Tracing with JDB\nshows the hang occurs when calling the constructor\nHSSFWorkbook(java.io.InputStream) [ example: workbook = new\nHSSFWorkbook(streamIn); ].  The trace never returns from the constructor call\nalthough I have not yet traced any deeper than the call to the constructor\nitself at the time of this writing.", "attachment_id": null, "id": 75577, "creator": "corrupted_kernel@yahoo.com", "time": "2005-05-29T22:07:13Z", "bug_id": 33681, "creation_time": "2005-05-29T22:07:13Z", "is_private": false}, {"count": 6, "tags": [], "bug_id": 33681, "attachment_id": null, "text": "(In reply to comment #4)\n> (In reply to comment #3)\n> > I can verify that this is indeed the case with the attached file. \n> \n> After reporting this bug, I added that I thought the problem was having too many\n> rows in the file, rather than simply too large of a file.\n> \n> Since then I've found that creating a file with an equal number of rows, but\n> with numeric data rather than string data, causes no problem.  So, I'm pretty\n> sure the issue here is having a file with too many strings.\n\n(In reply to comment #4)\n> (In reply to comment #3)\n> > I can verify that this is indeed the case with the attached file. \n> \n> After reporting this bug, I added that I thought the problem was having too many\n> rows in the file, rather than simply too large of a file.\n> \n> Since then I've found that creating a file with an equal number of rows, but\n> with numeric data rather than string data, causes no problem.  So, I'm pretty\n> sure the issue here is having a file with too many strings.\n\nI have verified this problem under the cited scenario as well.  Tracing with JDB\nshows the hang occurs when calling the constructor\nHSSFWorkbook(java.io.InputStream) [ example: workbook = new\nHSSFWorkbook(streamIn); ].  The trace never returns from the constructor call\nalthough I have not yet traced any deeper than the call to the constructor\nitself at the time of this writing.", "id": 75578, "time": "2005-05-29T22:08:00Z", "creator": "corrupted_kernel@yahoo.com", "creation_time": "2005-05-29T22:08:00Z", "is_private": false}, {"count": 7, "tags": [], "text": "The error occurs in SSTDeserialize .. it seems to spend all its time in\nSSTDeserializer.addToStringTable (calling into BinaryTree.put)\n\nDebugging, it looks like the issue is due to duplicate strings. the algorithm in\nthis method is probably polynomial in the number of duplicates of any string.\nWhen a file contains many instances of the same string, this becomes a bottleneck!\n\nI'm confused... I thought we had problems with duplicate strings only because of\nrich text. Are the strings in this file rich text? ", "attachment_id": null, "id": 77033, "creator": "avik@apache.org", "time": "2005-07-06T13:13:06Z", "bug_id": 33681, "creation_time": "2005-07-06T13:13:06Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 33681, "attachment_id": null, "id": 77621, "time": "2005-07-25T18:23:55Z", "creator": "amolweb@yahoo.com", "creation_time": "2005-07-25T18:23:55Z", "is_private": false, "text": "*** Bug 35849 has been marked as a duplicate of this bug. ***"}, {"count": 9, "tags": [], "bug_id": 33681, "attachment_id": null, "id": 77742, "time": "2005-07-28T06:24:54Z", "creator": "jheight@apache.org", "creation_time": "2005-07-28T06:24:54Z", "is_private": false, "text": "Ok i know what is happening here. SST records it seems *CAN* have duplicate \nentries @#^&%!\n\nThis means that the current implementation which adds spaces onto the string to\nprevent duplicate entries being put into the Binary Tree is called continually!\nWhich of course is going to raise an exceptionally large number of exceptions &\nre-create a large number of strings, effectively killing the whole performance.\n\nAs part of the patch to http://issues.apache.org/bugzilla/show_bug.cgi?id=31906\nI have implemented the RichText stuff in UnicodeRecord, including comparision\nfor uniqueness based on the rich text stuff.  After taking out the code in the\nSSTDeserializer.addToStringTable which ensured uniqueness, i now get exceptions\nbeing raised with duplicates.\n\nSo in actual fact we should allow for duplicates in the SST record. I will look\ninto this over the next few days.\n\nJason"}, {"count": 10, "tags": [], "bug_id": 33681, "attachment_id": null, "text": "Are you POSITIVE?  I could have sworn we put that in to prevent dupes because\nthey caused problems.  If this IS true then part of me wants to say\n\"de-duplciate\" and give the referencing records the other index...while that\nwould help file size -- I guess it wouldn't help write times (depending on the\nlength of the string I guess)...  \n\nWe should still reference existing strings when creating new cells with the same\nstring...", "id": 77744, "time": "2005-07-28T08:16:54Z", "creator": "poi-support@buni.org", "creation_time": "2005-07-28T08:16:54Z", "is_private": false}, {"count": 11, "tags": [], "bug_id": 33681, "attachment_id": null, "text": "Well to assure myself, i opened the file in excel re-saved it as a different\nfile and used the MS BIFFVIEW. Sure enough the SST record contains the same\nrepeated string information.\n\nSo it seems that Excel doesnt mind duplicates. So YES i am POSITIVE...95% UNLESS\nI have missed something here! Chalk this up to yet-another-excel-oddity.\n\nYes we will continue to avoid duplicates via the API, but we will need to allow\nfor duplicates in \"template\" files read from excel.\n\nI will look into the difficulty of de-duplicating when i try to get this all to\nwork.\n\nJason", "id": 77746, "time": "2005-07-28T08:41:06Z", "creator": "jheight@apache.org", "creation_time": "2005-07-28T08:41:06Z", "is_private": false}, {"count": 12, "tags": [], "bug_id": 33681, "attachment_id": null, "id": 77749, "time": "2005-07-28T08:47:30Z", "creator": "poi-support@buni.org", "creation_time": "2005-07-28T08:47:30Z", "is_private": false, "text": "And this is a file written by Excel and not some defective implementation?  \n\nOkay...I'm glad you're working on this :-)  We've needed rich strings for a\nlooong time."}, {"count": 13, "tags": [], "bug_id": 33681, "text": "Ok so i opened the file, saved it as csv, then opened the csv and re-saved as excel.\n\nThe \"pure\" excel version at the end of this process did not cause errors ie\nthere were no duplicates in the sst record. Interestingly the file size was\nlarger than the initial file that contained duplicates!!!\n\nI have sent emails to the reporters asking whether they are sure that the\nattached files were created by excel. At this stage ill hold off making any\nchanges until i hear back from them.\n\nJason", "id": 77806, "time": "2005-07-29T00:16:24Z", "creator": "jheight@apache.org", "creation_time": "2005-07-29T00:16:24Z", "is_private": false, "attachment_id": null}, {"count": 14, "tags": [], "bug_id": 33681, "text": "Opps. Saved the file with the excel 95 version as well thats why it was larger.\nIf i only save excel 97 then file size is reduced but still no duplicates.", "id": 77807, "time": "2005-07-29T00:18:42Z", "creator": "jheight@apache.org", "creation_time": "2005-07-29T00:18:42Z", "is_private": false, "attachment_id": null}, {"count": 15, "tags": [], "bug_id": 33681, "attachment_id": null, "text": "Ok i am 100% sure that SST record can contain duplicates.\n\nThere are even a number of excel files in our test files for the unit tests that\ncontain duplicates in the SST record.\n\nThis makes some sense, look at the fields of the SST record, one is the number\nof strings in the record, another is the number of *unique* strings in the record.\n\nThe good news is Im almost done getting this all solved.\n\nJason\n", "id": 78040, "time": "2005-08-04T01:20:49Z", "creator": "jheight@apache.org", "creation_time": "2005-08-04T01:20:49Z", "is_private": false}, {"count": 16, "tags": [], "bug_id": 33681, "attachment_id": null, "id": 78783, "time": "2005-08-22T05:23:48Z", "creator": "jheight@apache.org", "creation_time": "2005-08-22T05:23:48Z", "is_private": false, "text": "Now works with the HEAD of CVS, albiet slowly (takes about 30sec on my machine\nto read the file and write it back out.\n\nInterestingly 90% of the time is on the writing out side of things.. Hmm time to\nlook into performance.\n\nJason"}]