[{"count": 0, "tags": [], "creator": "dkruglyak@gmail.com", "is_private": false, "text": "I have been stress-testing fault tolerance of an Apache load balancer with Jetty backend being restarted under heavy load.\n\nOur test requests involve downloading of large XML documents (in HTTP request bodies) with heavy custom processing before response - which means the full requests take between 100-1000ms depending of the system load.\n\nInitially, I have been getting errors similar to the ones described in Bug 37770, so I applied all the proposed fixes in all possible combinations (HTTP 1.0, no keepalive, initial not pooled, etc) - however the same errors as in Bug 37770 kept re-appearing from time to time.\n\nThen I noticed that our Jetty servlet occasionally fails to read HTTP input stream - encountering and logging an early EOF. Seems to me the ONLY possible cause of this condition would be if httpd forcefully closed the connection for currently executing requests. This could occur once Jetty has stopped accepting new requests, but have not yet finished processing existing ones - it would return an error to httpd, which would mark the worker as failed and kill existing connections. This condition would be noticeable with large requests under heavy load.\n\nI wonder if this theory makes sense and if a fix could be developed.", "id": 180694, "time": "2015-02-02T01:50:42Z", "bug_id": 57520, "creation_time": "2015-02-02T01:50:42Z", "attachment_id": null}, {"count": 1, "attachment_id": null, "bug_id": 57520, "is_private": false, "id": 180695, "time": "2015-02-02T08:54:28Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-02-02T08:54:28Z", "tags": [], "text": "(In reply to dk from comment #0)\n> I have been stress-testing fault tolerance of an Apache load balancer with\n> Jetty backend being restarted under heavy load.\n\nIs Jetty gracefully restarted here?\nIf not, you probably can't expect zero error here, say a request (including body) has been forwarded by mod_proxy while Jetty issues an abortive restart, there is nothing that can be recovered, an error will occur on the proxy side while \"reading status line from remote server\".\n\nWhat is the exact error line printed?\n\n> \n> Our test requests involve downloading of large XML documents (in HTTP\n> request bodies) with heavy custom processing before response - which means\n> the full requests take between 100-1000ms depending of the system load.\n\nYou mean *uploading* a large XML document to Jetty right?\nAnd then Jetty takes up to 1s to generate a response or is the request taking that time to be forwarded to Jetty (not including response)?\n\n> \n> Initially, I have been getting errors similar to the ones described in Bug\n> 37770, so I applied all the proposed fixes in all possible combinations\n> (HTTP 1.0, no keepalive, initial not pooled, etc) - however the same errors\n> as in Bug 37770 kept re-appearing from time to time.\n> \n> Then I noticed that our Jetty servlet occasionally fails to read HTTP input\n> stream - encountering and logging an early EOF. Seems to me the ONLY\n> possible cause of this condition would be if httpd forcefully closed the\n> connection for currently executing requests.\n\nThis could also happen if the read failed on the client side while mod_proxy forwards the (large) request.\n\n> This could occur once Jetty has\n> stopped accepting new requests, but have not yet finished processing\n> existing ones - it would return an error to httpd, which would mark the\n> worker as failed and kill existing connections. This condition would be\n> noticeable with large requests under heavy load.\n\nIf that happens, you should see \"disabling worker\" messages in the error log, is it the case?\n\n> \n> I wonder if this theory makes sense and if a fix could be developed.\n\nI think we need at least the error log and configuration file on the httpd side, as well as the KeepAlive timeout value (and connection related ones) on the Jetty side."}, {"count": 2, "tags": [], "text": "Bug 37770 is closed, there is nothing to depend on from there right now.", "is_private": false, "id": 180696, "creator": "ylavic.dev@gmail.com", "time": "2015-02-02T08:58:11Z", "bug_id": 57520, "creation_time": "2015-02-02T08:58:11Z", "attachment_id": null}, {"count": 3, "tags": [], "text": "1) Yes, Jetty is being restarted gracefully. I am calling server.stop() and then server.join() to ensure the JVM does not exit till all current requests finish. All Java-side issues have been sorted out by now\n\n2) The error that reveals the problem here is a Java exception with \"EOF while reading\" HTTP input stream. This is only possible if the connection is closed by Apache - JVM would not do this since Jetty is being stopped gracefully and waiting for requests to finish, per above.\n\n3) Yes, XML is being uploaded into Jetty as HTTP body. It is read by servlet as raw input stream fed into XML parser. Per above, the stream reading sporadically fails with a \"premature EOF exception\" if Jetty is being stopped/restarted under heavy load \n\n4) The HTTP body read never fails unless Jetty is restarted. Both our test client and Jetty servlet have been confirmed to function correctly under all circumstances, except the stress test of Jetty restart under heavy load\n\n5) Of course I see \"disabling worker\" messages. That's the point of the test - to ensure that restarting Jetty under heavy load is graceful and does not mangle requests in process. Additionally \"proxy: error reading status line from remote server\" messages occasionally appear which may also be consistent with the possibility of httpd killing in-process connections when a worker is disabled.\n\n6) I tried this with or without keepalives (per instructions in Bug 37770) and the behavior is consistent. Specifically the tested cases were: 1) 'SetEnv proxy-nokeepalive 1' 2) keepalive=On on all balancer workers 3) 'KeepAlive 600' within vhost config\n\nHere are the sample lines from httpd error log -\n\n[Sun Feb 01 16:11:48 2015] [error] [client xx.xx.xx.xx] (20014)Internal error: proxy: error reading status line from remote server host1:xxxx\n[Sun Feb 01 16:11:48 2015] [error] [client xx.xx.xx.xx] proxy: Error reading from remote server returned by /my_uri\n\n[Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] (104)Connection reset by peer: proxy: error reading status line from remote server host1:xxxx\n[Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] proxy: Error reading from remote server returned by /my_uri\n[Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] (104)Connection reset by peer: proxy: error reading status line from remote server host2:xxxx\n[Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] proxy: Error reading from remote server returned by /my_uri\n[Sun Feb 01 16:40:06 2015] [error] (111)Connection refused: proxy: HTTP: attempt to connect to xx.xx.xx.xx:xxxx (host1) failed\n[Sun Feb 01 16:40:06 2015] [error] ap_proxy_connect_backend disabling worker for (host1)\n[Sun Feb 01 16:40:26 2015] [error] (111)Connection refused: proxy: HTTP: attempt to connect to xx.xx.xx.xx:xxxx (host2) failed\n[Sun Feb 01 16:40:26 2015] [error] ap_proxy_connect_backend disabling worker for (host2)", "attachment_id": null, "id": 180697, "creator": "dkruglyak@gmail.com", "time": "2015-02-02T09:27:29Z", "bug_id": 57520, "creation_time": "2015-02-02T09:27:29Z", "is_private": false}, {"count": 4, "attachment_id": null, "bug_id": 57520, "is_private": false, "id": 180703, "time": "2015-02-02T12:25:05Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-02-02T12:25:05Z", "tags": [], "text": "(In reply to dk from comment #3)\n> 1) Yes, Jetty is being restarted gracefully. I am calling server.stop() and\n> then server.join() to ensure the JVM does not exit till all current requests\n> finish. All Java-side issues have been sorted out by now\n\nI don't no much about Jetty but, does it accept new connections while restarting gracefully? The \"Connection refused\" in the error log seems to indicate it does not.\n\n> \n> 2) The error that reveals the problem here is a Java exception with \"EOF\n> while reading\" HTTP input stream. This is only possible if the connection is\n> closed by Apache - JVM would not do this since Jetty is being stopped\n> gracefully and waiting for requests to finish, per above.\n\nCould you provide some network (pcap) traces between the proxy and jetty which would capture the restart (so that we can figure out what's going on and which part is closing/resetting the connections)?\n\n\n> 5) Of course I see \"disabling worker\" messages. That's the point of the test\n> - to ensure that restarting Jetty under heavy load is graceful and does not\n> mangle requests in process. Additionally \"proxy: error reading status line\n> from remote server\" messages occasionally appear which may also be\n> consistent with the possibility of httpd killing in-process connections when\n> a worker is disabled.\n> \n> 6) I tried this with or without keepalives (per instructions in Bug 37770)\n> and the behavior is consistent. Specifically the tested cases were: 1)\n> 'SetEnv proxy-nokeepalive 1' 2) keepalive=On on all balancer workers 3)\n> 'KeepAlive 600' within vhost config\n\nUnless \"proxy-nokeepalive\" (SetEnv) or \"disablereuse=on\" (ProxyPass' parameter) are used, the backend (jetty) connections will be reused for multiple/successive requests.\nThe time these connections are kept alive between requests (idle) is given by the parameter \"ttl=<seconds>\" (ProxyPass'), whereas \"keepalive=on\" relates to the TCP-keepalive (not HTTP-keepalive), and KeepAliveTimeout (assuming this is what you mean with 'KeepAlive 600') applies to client connections (not to backend's).\n\nBy using a ttl= lower than the configured KeepAliveTimeout on the jetty side, it would prevent the case where the backend closes the connection while the proxy is reusing it for a successive request, which would lead to the same \"proxy: error reading status line\" error since the proxy wouldn't realize before the connection is read (write would succeed).\n\nThis cannot happen with \"proxy-nokeepalive\" though, backend connections are not reused and hence a new one is created for each request.\nFrom mod_proxy point of view, an \"error reading\" on a newly established connection means it has been closed/reset on the backend side (whereas a response is expected).\nThat's why I think we need network traces to figure out..."}, {"count": 5, "tags": [], "creator": "dkruglyak@gmail.com", "attachment_id": null, "text": "\"Connection refused\" messages in the error log are expected since after Jetty JVM exits it won't be listening on ports while it is restarting. While Jetty is still shutting down and waiting for existing requests to finish it responds with 503. In httpd I have failonstatus set to handle that.\n\nCould you recommend the specific pcap or tcpdump settings to capture the connection info you need? I believe capturing the entire traffic payload would be way too heavy and I am not sure if/how I can even post huge logs into this issue tracker. I have 3 boxes in the cluster/balancer with Jetty/httpd on each and one client box issuing round robin stress test requests - 30 threads at the time. Could you specify exactly what you want me to do?\n\nAs far as ttl/keepalives - yes this reported behavior is seen under \"proxy-nokeepalive\" too. TTL is the only setting you mentioned that I have not tried, but I presume it is ignored under nokeepalive regime?\n\nPerhaps you could suggest a couple mod_proxy configurations for me to test with the traffic recorder on? I should note, it might take a few runs (generating pretty big logs) since the error behavior is quite flaky and occurs sporadically - depending on how the Jetty restart is exactly timed.", "id": 180747, "time": "2015-02-04T06:32:31Z", "bug_id": 57520, "creation_time": "2015-02-04T06:32:31Z", "is_private": false}, {"count": 6, "attachment_id": null, "bug_id": 57520, "is_private": false, "id": 180765, "time": "2015-02-05T00:06:53Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-02-05T00:06:53Z", "tags": [], "text": "(In reply to dk from comment #5)\n> Perhaps you could suggest a couple mod_proxy configurations for me to test\n> with the traffic recorder on? I should note, it might take a few runs\n> (generating pretty big logs) since the error behavior is quite flaky and\n> occurs sporadically - depending on how the Jetty restart is exactly timed.\n\nGiven your load scenario and \"sporadic error behavior\", it is probably not a good idea to use a network capture (would indeed produce a big file, but mainly huge time to analyse it).\n\nYou could restrict captured packets' size with tcpdump -s or just filter SYN/FIN/RST (and then find connections terminated prematuraly, ie. those with low sequence numbers at the end, correlated with error log timestamps), still probably quite painful to analyse with high traffic.\n\nBefore that, maybe we can think more about what's happening...\n\nFirst, please note (as already requested) that the relevant part of your configuration (<Proxy balancer:...></Proxy>, ProxyPass, ..., or the complete file) would help determine the expected httpd's behaviour.\n\nOtherwise, can you see \"child pid <pid> exit signal <sig>...\" messages in the global log file (the one pointed to by the main ErrorLog directive, out of any VirtualHost)?\nThose would mean a crash in httpd (children) and could explain why some connections are closed forcibly (by the system) before the request is sent, as detected on the jetty side.\n\nOther than that, mod_proxy will *never* close any connection without even trying to send the request it was created for (but read failures on the client side), even if the backend (LoadBalancerMember) has switched to recovery state in the meantime (because of some simultaneous request's failure).\nSo this is not something that should/could be fixed (per comment #1), it just should not happen.\n\nThe only reason (I see) for which jetty could get an EOF (without data) is a race condition between a connect() timeout on the proxy side and a (simultaneous) accept() on the jetty side (that would cause \"The timeout specified has expired\" instead of \"Connection refused\" in the log, though).\n\n> \"Connection refused\" messages in the error log are expected since after\n> Jetty JVM exits it won't be listening on ports while it is restarting. While\n> Jetty is still shutting down and waiting for existing requests to finish it\n> responds with 503. In httpd I have failonstatus set to handle that.\n\nBTW, in both cases (\"connection refused\" and 503) mod_proxy will put the backend in recovery state for a period of retry=<seconds> (60 by default), which is what is intended (IIUC).\nSo, all the \"connection refused\" messages should only appear once jetty is completely down and until jetty is up again, and only when the backend is being retried (the number of lines should then be the number of simultaneous requests elected for that backend at that time).\nSo far, so good.\n\nBut in this scenario there shouldn't be any \"Connection reset by peer: proxy: error reading status line from remote server\", which indicates that an established connection was *reset* by jetty with no response at all, no 200 nor 503...\nThis error won't put the bachend in recovery state (the connect()ion succeed and there is no status to fail on), but with proxy-initial-not-pooled and a normal browser as client, this one will resend the request without notifying the user.\n\nFrom the log lines you provided in comment #3 :\n> [Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] (104)Connection reset by peer: proxy: error reading status line from remote server host1:xxxx\n> [Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] proxy: Error reading from remote server returned by /my_uri\n> [Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] (104)Connection reset by peer: proxy: error reading status line from remote server host2:xxxx\n> [Sun Feb 01 16:40:06 2015] [error] [client xx.xx.xx.xx] proxy: Error reading from remote server returned by /my_uri\n> [Sun Feb 01 16:40:06 2015] [error] (111)Connection refused: proxy: HTTP: attempt to connect to xx.xx.xx.xx:xxxx (host1) failed\n> [Sun Feb 01 16:40:06 2015] [error] ap_proxy_connect_backend disabling worker for (host1)\nit suggests that reading errors can arise just before jetty is down (not connectable anymore), so it seems that the graceful shutdown is missing some already established connections...\nIn fact this is also racy, between the time when jetty detects/decides there is no pending connection (no more 403) and the one the listening socket is really closed, there may be new connection handshaken (TCP speaking) by the OS (up to the listen backlog size).\nThese connections will typically be reset once the listening socket is really closed.\n\n> \n> As far as ttl/keepalives - yes this reported behavior is seen under\n> \"proxy-nokeepalive\" too. TTL is the only setting you mentioned that I have\n> not tried, but I presume it is ignored under nokeepalive regime?\n\nCorrect, I just mentioned this based on your \"keepalive=On\" + \"Keepalive 600\" configuration from comment #3, and only to notice that ttl lower than backend's KeepAliveTimeout is probably a better alternative with regard to performances/resources (since connections to jetty would be reused), and possibly also with regard to fault tolerance (see below).\n\nThat would depend on how jetty handles kept alive (idle) connections on gracefull restart (in your scenario).\nIf those are closed immediatly the issue remain that mod_proxy may reuse connections closed before they can be detected as such on its side.\nBut if they are closed only above KeepAliveTimeout while still answered with 503 when some request arrives (on time), everything is fine.\n\nThis \"mode\" may also be better for fault tolerance too (provided \"everything is fine\" above), because now mod_proxy will likely reuse already established connections and should receive 403s on all of them (for all simultaneous requests), until the recovery state.\n\nAlthough this is just (my) theoretical thought..."}, {"count": 7, "tags": [], "text": "Since there is nothing we can do on httpd side, I'm closing this report.\n\nIf some investigation turns up a httpd bug, please reopen.\nOtherwise, I suggest users@httpd.apache.org for further (support) discussion.", "attachment_id": null, "id": 180766, "creator": "ylavic.dev@gmail.com", "time": "2015-02-05T00:34:45Z", "bug_id": 57520, "creation_time": "2015-02-05T00:34:45Z", "is_private": false}, {"count": 8, "tags": [], "text": "(In reply to Yann Ylavic from comment #6)\n> But in this scenario there shouldn't be any \"Connection reset by peer:\n> proxy: error reading status line from remote server\", which indicates that\n> an established connection was *reset* by jetty with no response at all, no\n> 200 nor 503...\n> This error won't put the bachend in recovery state (the connect()ion succeed\n> and there is no status to fail on), but with proxy-initial-not-pooled and a\n> normal browser as client, this one will resend the request without notifying\n> the user.\n\nThis is wrong, proxy-initial-not-pooled does not help the browser here, it just prevent mod_proxy from reusing a kept alive connection on the backend side when the request comes from new client connection (first one on this connection), precisely because the client does not expect an error is this case (and reusing a connection whose ttl is above backend's KeepAliveTimeTimeout may produce the error).\nParticularly, proxy-initial-not-pooled should not be used when ttl is lower than backend's KeepAliveTimeTimeout since reusing backend connections is not an issue and may even help fault tolerance as seen is previous message.\n\nI mixed up with mod_proxy's behaviour (hard coded) on the client side when this error occurs.\nEither the request comes from a kept alive connection and hence the browser is expecting that kind of error, so mod_proxy will respond with a closed connection (no HTTP response at all) so that the browser can recover (resend the request) without notifying the user.\nOr the request is the first one on the client connection and httpd will respond with a 502 because the browser can't recover (unexpected error).\nUnfortunately this is not something which is controllable on httpd, hence this errors are not *always* transparent for the user.", "attachment_id": null, "id": 180776, "creator": "ylavic.dev@gmail.com", "time": "2015-02-05T09:11:23Z", "bug_id": 57520, "creation_time": "2015-02-05T09:11:23Z", "is_private": false}, {"count": 9, "tags": [], "text": "Based on your insistence that the problem must be on Jetty side I did in-depth investigation.\n\nIndeed, turned out that Jetty requires a very specific configuration for graceful shutdown to work - specifically a working StatisticHandler. Originally, I did have this handler created but it was not properly invoked / integrated into the handler chain. After fixing this problem (and adding logging of the StatisticHandler stats at the request start/stop and server shutdown start/stop times) I found that all exceptions / errors go away after thorough testing. In the logs I can observe the case of server shutdown invoked in the middle of multiple requests processing and then seeing them through to graceful completion, before server shutdown completion.\n\nHere is the reference for the Jetty issue: https://bugs.eclipse.org/bugs/show_bug.cgi?id=420142\n\nJust for the record this final test was completed under the following settings in the VirtualHost:\n\n    SetEnv force-proxy-request-1.0 1\n    SetEnv proxy-nokeepalive 1\n    SetEnv proxy-initial-not-pooled 1\n    RequestHeader unset Expect early\n\n... and this balancer setup:\n\n<Proxy balancer://mybalancer>\n    ProxySet failonstatus=502,503 maxattempts=1000\n    BalancerMember http://host1:9999 timeout=3600 retry=0 ping=30 disablereuse=on keepalive=on\n    BalancerMember http://host2:9999 timeout=3600 retry=0 ping=30 disablereuse=on keepalive=on\n    BalancerMember http://host3:9999 timeout=3600 retry=0 ping=30 disablereuse=on keepalive=on\n</Proxy>\n\nI intend to leave these settings \"as is\" for our production system, unless there is a good reason (and low risk) to restore keep alives or if you might advise change any other changes... I suppose if all requests have large payloads, the keepalives won't be adding much benefit?", "is_private": false, "id": 180779, "creator": "dkruglyak@gmail.com", "time": "2015-02-05T09:20:38Z", "bug_id": 57520, "creation_time": "2015-02-05T09:20:38Z", "attachment_id": null}, {"count": 10, "tags": [], "text": "(In reply to dk from comment #9)\n> Based on your insistence that the problem must be on Jetty side I did\n> in-depth investigation.\n\nSorry if it appeared insistent, it was not my intent.\n\nMy goal was more about giving an overview on the issues regarding connections between a proxy and a backend (I may have been confusing in that goal though, based on a real use case), because these questions keep coming regularly either on bugzilla or mailing lists (or personnal experience).\n\nI think this is is racy by nature, under heavy stress no one should expect zero error.\nAnd jetty's graceful shutdown is not an exception (IMHO), it has to decide when to stop listenning/accepting clients, but the proxy has no idea about this and still asks for connections (especially with retry=0)...\n\n> \n> Just for the record this final test was completed under the following\n> settings in the VirtualHost:\n> \n>     SetEnv force-proxy-request-1.0 1\n>     SetEnv proxy-nokeepalive 1\n>     SetEnv proxy-initial-not-pooled 1\n\nThose 3 are redundant, you could leave the second only.\nBeside redundancy, I'd suggest not using force-proxy-request-1.0, unless the backend really expects HTTP/1.0, or requests with chunked encoding is not supported on jetty.\nIt will cause chunked requests' bodies to be spooled on the disk before being forwarded (as Content-Length).\n\n> \n>     RequestHeader unset Expect early\n> \n> ... and this balancer setup:\n> \n> <Proxy balancer://mybalancer>\n>     ProxySet failonstatus=502,503 maxattempts=1000\n>     BalancerMember http://host1:9999 timeout=3600 retry=0 ping=30\n> disablereuse=on keepalive=on\n>     BalancerMember http://host2:9999 timeout=3600 retry=0 ping=30\n> disablereuse=on keepalive=on\n>     BalancerMember http://host3:9999 timeout=3600 retry=0 ping=30\n> disablereuse=on keepalive=on\n> </Proxy>\n\ndisablereuse is also redundant with SetEnvs above.\nping is ajp:// specific in 2.2.x, it is implemented in 2.4.x for http://, so you can leave it as is from a migration perspective...\nBut if the goal is to let jetty handle the Expect header (with RequestHeader unset above), it won't work in 2.2.x, and since httpd won't handle it either (still per RequestHeader rule), no one will statisfy the client's expectation...\n\n> \n> I intend to leave these settings \"as is\" for our production system, unless\n> there is a good reason (and low risk) to restore keep alives or if you might\n> advise change any other changes... I suppose if all requests have large\n> payloads, the keepalives won't be adding much benefit?\n\nProbably, especially if it works \"as is\"...\nI'd personnaly run an heavier stress test and try to get things race ( but that's *my* theory ;) and then if it happens, try the other way...", "attachment_id": null, "id": 180786, "creator": "ylavic.dev@gmail.com", "time": "2015-02-05T11:33:19Z", "bug_id": 57520, "creation_time": "2015-02-05T11:33:19Z", "is_private": false}, {"count": 11, "tags": [], "text": "Your insistence was very much appreciated. It led me to re-check Jetty backend.\n\nYes, I am not expecting zero error, just want to make sure the most obvious things to increase stability are implemented. In my case I believe retry=0 (with maxattempts=1000) is very proper since our Jetty restart script \"staggers\" restarts to ensure that only 1/3 of all servers may be down at a time (we take IP address modulo 3 to calculate the restart delay multiplier) - so there is always a server to fall back on. Just to confirm - if httpd encounters 503 from Jetty under this regime, would it retry a different balancer or return 503 to client (with failonstatus set)? The docs are not very clear on whether failonstatus would cause the requests to be retried and/or if this behavior could be configured...\n\nI am going to clean up our proxy settings per your advice. The ones I have now were result of trial and error with various suggestions from https://issues.apache.org/bugzilla/show_bug.cgi?id=37770 (e.g. I do not care about Expect header or HTTP 1.0). I am curious how much of the advice in that discussion thread is still applicable with 2.2.29 vs. may have been fixed / worked around?", "is_private": false, "id": 180788, "creator": "dkruglyak@gmail.com", "time": "2015-02-05T12:11:23Z", "bug_id": 57520, "creation_time": "2015-02-05T12:11:23Z", "attachment_id": null}, {"count": 12, "attachment_id": null, "bug_id": 57520, "is_private": false, "id": 180789, "time": "2015-02-05T12:30:13Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-02-05T12:30:13Z", "tags": [], "text": "So that a link is done between the two (other than \"depends on\").\n\n*** This bug has been marked as a duplicate of bug 37770 ***"}, {"count": 13, "tags": [], "bug_id": 57520, "attachment_id": null, "is_private": false, "id": 180792, "time": "2015-02-05T13:26:59Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-02-05T13:26:59Z", "text": "(In reply to dk from comment #11)\n> Just to confirm - if httpd\n> encounters 503 from Jetty under this regime, would it retry a different\n> balancer or return 503 to client (with failonstatus set)? The docs are not\n> very clear on whether failonstatus would cause the requests to be retried\n> and/or if this behavior could be configured...\n\nYes, any 503 is retried on another BalancerMember, provided at least one is not in recovery state (forcerecovery=on may be used to force recovery of all the members otherwise).\n\nHowever failonstatus plays no role on retry, it only sets the statuses for which the backend is put into recovery state, but only 503 (or connect() error) will be retried (moreover 503 is implicit, so failonstatus=503 is redundant too).\nWith 502 for example (as per your configuration), it will cause the member to be put into recovery state for retry seconds but the request will *not* be sent to another one, and the 502 will reach the client.\nKeep in mind that mod_proxy can't decide whether a request can be resent, particularly non-idempotent ones (as per RFC), you wouldn't want a bank order to be applied twice (it depends on which way though :) and hence mod_proxy generally let the browser/user decide.\n\nFinally retry=0 is probably playing a bad role here since when retried, all the balancer members will be taken into account (and checked against the recovery period for eligibility), hence retry=0 will force eligibility during the retry, which may lead to the same member being elected.\n\n> \n> I am going to clean up our proxy settings per your advice. The ones I have\n> now were result of trial and error with various suggestions from\n> https://issues.apache.org/bugzilla/show_bug.cgi?id=37770 (e.g. I do not care\n> about Expect header or HTTP 1.0). I am curious how much of the advice in\n> that discussion thread is still applicable with 2.2.29 vs. may have been\n> fixed / worked around?\n\nLink between the two tickets is done now.\n\nIf needed, this discussion should be continued on the users@ mailing-list.\nBugzilla if for reporting bugs only, not for support.\nI'll be happy to contribute, but we should stop here (I realize this is not what I've done so far, by I guess it's enough)."}, {"count": 14, "tags": [], "bug_id": 57520, "attachment_id": null, "id": 180795, "time": "2015-02-05T13:54:23Z", "creator": "dkruglyak@gmail.com", "creation_time": "2015-02-05T13:54:23Z", "is_private": false, "text": "Thanks for all your help!\n\nJust completed the final stress-test under the revised settings and everything goes through without any errors. So for now I'll consider the issue solved for us..."}]