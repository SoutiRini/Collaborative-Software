[{"count": 0, "tags": [], "creator": "david.pentzlin@commscope.com", "attachment_id": null, "text": "Problem: \nSetup Tomcat (8.0.28) with default settings. Copy a file in a webapp e.g. webapps/root and download the file with limited speed to simulate a slow modem download.\nwget http://xxx.xxx.xxx.xxx:8080/xxxxxxx/7.zip --limit-rate=1k\n\nConnection is closed always after 6minutes and 24seconds\ne.g:\nSaving to: \u20187.zip\u2019\n7.zip               0%[                      ] 767.99K  1.00KB/s   in 6m 24s\n2015-10-28 17:27:39 (1024 B/s) - Connection closed at byte 786426. Retrying.\n\nif i change the protocol to an old blocking protocol:\nprotocol=\"org.apache.coyote.http11.Http11Protocol\"\neverything is fine and the download will complete with slow speed.\n\nIf the change the speed (with protocol HTTP/1.1) the time after the connection is closed is different (e.g. for 7kb/sec it will always close after 3minutes and 21sec).\n\nIf the speed is 8kb/sec or higher the download is stable with both protocols.", "id": 186089, "time": "2015-10-29T14:29:35Z", "bug_id": 58565, "creation_time": "2015-10-29T14:29:35Z", "is_private": false}, {"count": 1, "tags": [], "creator": "remm@apache.org", "attachment_id": null, "text": "Ok. Internally things are very different with NIOx: lots of buffering in the network stack and non blocking calls. It is possible this needs OS level configuration if available, and Tomcat itself doesn't do anything wrong. This will be investigated.\n\nDowngrading severity since in a way it's an anti DoS feature ...", "id": 186091, "time": "2015-10-29T15:57:51Z", "bug_id": 58565, "creation_time": "2015-10-29T15:57:51Z", "is_private": false}, {"count": 2, "tags": [], "creator": "david.pentzlin@commscope.com", "attachment_id": null, "text": "just as a note. i tried it with windows and linux and got the same results/problems.", "id": 186092, "time": "2015-10-29T16:00:18Z", "bug_id": 58565, "creation_time": "2015-10-29T16:00:18Z", "is_private": false}, {"count": 3, "tags": [], "bug_id": 58565, "attachment_id": null, "id": 186093, "time": "2015-10-29T16:28:31Z", "creator": "remm@apache.org", "creation_time": "2015-10-29T16:28:31Z", "is_private": false, "text": "After debugging, it looks like it's a combination of factors.\n\nWhat happens is that the network stack buffers things a lot without blocking. At some point it will block until everything gets flushed. Unfortunately, the flushing takes forever due to the client speed limit, and a legitimate write connection timeout occurs in Tomcat. Then the network stack will continue trickling bytes to the client until the connection is closed due to the error (which occurred a long time before).\n\nI would recommend to *not* increase the timeout for the connection [that would be a workaround], and I doubt this can be fixed effectively. So this could end up as a WONTFIX."}, {"count": 4, "tags": [], "creator": "david.pentzlin@commscope.com", "attachment_id": null, "text": "so what do you recommend? our application is accessed by many of our customers via gsm modem.\nIn some cases the speed is lower than 8kb/sec (gprs).\nWhich timeout do i have to change?", "id": 186094, "time": "2015-10-29T16:32:52Z", "bug_id": 58565, "creation_time": "2015-10-29T16:32:52Z", "is_private": false}, {"count": 5, "tags": [], "creator": "chris@christopherschultz.net", "attachment_id": null, "text": "Bufferbloat strikes again. :(\n\nDoes NIOx have enough control over the underlying socket/connection to reduce buffer sizes so that this problem is minimized?", "id": 186095, "time": "2015-10-29T17:18:21Z", "bug_id": 58565, "creation_time": "2015-10-29T17:18:21Z", "is_private": false}, {"count": 6, "tags": [], "creator": "remm@apache.org", "attachment_id": null, "text": "Well, it's a network stack thing so it is transparent. I suppose it is dynamic depending on the memory condition. The network stack has an incentive to make buffers huge since it will vastly improve performance as everything is non blocking.\n\nLuckily, it is exposed in NIOx (NIO2 has fewer socket options compared to NIO1, but it has that one) and I could find socket.txBufSize allows configuration in Tomcat, and probably there's something at the OS level as well. So a low value for socket.txBufSize allows this to run, I just verified it. By default I found out it is 1313280 for me [AKA memory is cheap :) ].\n\nAnyway, thanks for the hint to get me to look at the socket options, it is now determined it is only a configuration issue. Given the low speeds needed for this to happen and the possible performance impact, I would be -1 for trying to override the network stack default without explicit user configuration.", "id": 186097, "time": "2015-10-29T17:50:59Z", "bug_id": 58565, "creation_time": "2015-10-29T17:50:59Z", "is_private": false}, {"count": 7, "tags": [], "bug_id": 58565, "attachment_id": null, "text": "It seems like less buffer for NIO makes sense in general, though I agree that changing the default is probably not necessary, here. It might be worth mentioning in the documentation that NIO + slow speed = increased change of end-to-end timeouts.\n\nIt seems like there is also the possibility of maybe having some kind of QoS setting that causes flushes to occur with more regularity, even if the buffer is large enough to handle more data. Something like once every few seconds maybe? That way, the (slow) writes won't pile-up and cause a large flush that takes too much time.\n\nI don't know enough about the underlying NIO and threading strategy to know whether this would be easy or difficult, or yet another unnecessary complication in an already complicated machine.\n\nBut theoretically, there are always some values for which connections will be dropped due to this problem. Unless buffers are eliminated, which degrades NIO -> BIO which is of course not what we want. How does a user balance throughput and performance against fault-tolerance?\n\n(also, should this resolution be INVALID/NOTABUG or WORKSFORME?)", "id": 186098, "time": "2015-10-29T18:09:37Z", "creator": "chris@christopherschultz.net", "creation_time": "2015-10-29T18:09:37Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 58565, "attachment_id": null, "id": 186099, "time": "2015-10-29T18:31:54Z", "creator": "remm@apache.org", "creation_time": "2015-10-29T18:31:54Z", "is_private": false, "text": "It works for me with the right configuration, but not the default one, so I chose WORKSFORME but I don't mind if someone wants to change it.\n\nIt would seem with the default buffer on Linux + localhost and the default 20s timeout, the client read speed would have to be over 64KB/s to avoid a timeout. Increasing the timeout from 20s to 60s would allow slower speeds."}, {"count": 9, "tags": [], "bug_id": 58565, "attachment_id": null, "id": 186100, "time": "2015-10-29T19:16:18Z", "creator": "knst.kolinko@gmail.com", "creation_time": "2015-10-29T19:16:18Z", "is_private": false, "text": "(In reply to comment #8)\n> It would seem with the default buffer on Linux + localhost ...\n\n1. If I understand correctly, real networks have some quality-of-service mechanisms. A well known one is MTU size. For a localhost connection the MTU size is enormous.\n\n2. I do not know how wget implements its --limit-rate feature. Maybe all those MBs of data have already arrived and are in it's own (client's) buffer.\n\n3. Maybe in some configurations it makes sense to specify timeouts measured by throughput, instead of a time unit.\n\n(E.g. it may be easier to configure requirement of minimum throughput of X kb/s instead of a read timeout of Y sec.) It means that the actual timeout (that will be used as an argument to APIs) needs to be calculated dynamically based on amount of data that have been transfered earlier over this connection."}, {"count": 10, "tags": [], "bug_id": 58565, "attachment_id": null, "id": 186192, "time": "2015-11-02T13:33:02Z", "creator": "remm@apache.org", "creation_time": "2015-11-02T13:33:02Z", "is_private": false, "text": "IMO a direct \"speed\" configuration would be quite convoluted and users would have a hard time understanding it. It sounds better to me to keep buffer size and timeout instead.\n\nAre there really many who still want to support \"large\" files and dialup [without retry, since for example wget just happily resumes] ? In that situation, an admin should first increase the timeout, since 20s sounds too short for dialup even if not counting this issue. I'd say 60s in that situation at least, or maybe longer and using a separate shorter keepalive timeout."}]