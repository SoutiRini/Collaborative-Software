[{"attachment_id": null, "tags": [], "bug_id": 26446, "is_private": false, "count": 0, "id": 51210, "time": "2004-01-26T21:52:42Z", "creator": "Alan_Krueger@g1.com", "creation_time": "2004-01-26T21:52:42Z", "text": "When an HTTP/1.1 connection to an ISAPI extension uses Keep-Alive and the \noutput is written in Chunked Transfer-Encoding, multiple last-chunk markers \n(http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.6.1) appear in the \nresponse.\n\nThis appears to be due to the isapi_handler appending both a flush bucket and \nan eos bucket on the brigade.  Another follows that, created by \nend_output_stream.  Since the eos bucket appears to drive the last-chunk \nmarker, two are output in the response.\n\nThe symptoms for this vary by User-Agent, but in my situation the ISAPI \nextension is returning XML, where the extra last-chunk appears to be more \nnoticeable.  Specifically, MSIE on Win2k and Mozilla on RH9 sometimes display \nthe XML properly, other times they display the chunked response verbatim \nincluding previous leading last-chunk and the following response header as part \nof the response.  (MSIE actually just displays a 0 in those cases.)"}, {"count": 1, "tags": [], "creator": "robo@khelekore.org", "attachment_id": null, "text": "I have seen multiple last-chunks from other web servers, not sure if it the same\nproblem. This came from a php-page. \n\nMultiple last-chunks causes my web proxy, rabbit, to read the extra last chunk\nas the next http header in the keepalive stream. This results in broken images.\nIf I do not use a proxy mozilla seems to silently remove the extra last-chunk. \n\nI have a ethereal trace when this happens if anyone wants it. \n\nSeems to happen often, but non-deterministacally on http://www.eet.gr/\n", "id": 69439, "time": "2005-01-08T20:12:40Z", "bug_id": 26446, "creation_time": "2005-01-08T20:12:40Z", "is_private": false}, {"count": 2, "tags": [], "creator": "wrowe@apache.org", "attachment_id": null, "id": 90472, "time": "2006-06-22T07:20:09Z", "bug_id": 26446, "creation_time": "2006-06-22T07:20:09Z", "is_private": false, "text": "Reclassified this because it's not always going to be possible for a module\nauthor to know that a flush is ending the entire stream, depending on the\nprotocol they are trying to implement.\n\nThe eos bucket handler is gonna have to get a brain on this point, or else\nthe flush bucket handler is gonna have to learn not to flush nothing.\n\nEither way the flaw is probably in the chunk protocol filter itself."}, {"attachment_id": null, "tags": [], "bug_id": 26446, "is_private": false, "count": 3, "id": 90473, "time": "2006-06-22T07:26:36Z", "creator": "wrowe@apache.org", "creation_time": "2006-06-22T07:26:36Z", "text": "Whoops, off to core with this."}, {"count": 4, "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "is_private": false, "id": 90504, "time": "2006-06-22T21:27:20Z", "bug_id": 26446, "creation_time": "2006-06-22T21:27:20Z", "text": "From my point of view this problem is independant of the flush bucket. I guess\nwe see this problem if pass the eos bucket twice thru the chunk filter in\ndifferent invocations (=> and in different brigades). I guess the possible\napproach to solve this in the chunk filter is to remove the chunk filter from\nthe filter chain once we see an eos bucket in the chunk filter."}, {"count": 5, "tags": [], "text": "Why is this not simply a bug in whatever handler generated multiple EOS buckets?", "is_private": false, "bug_id": 26446, "id": 134071, "time": "2010-01-31T06:10:30Z", "creator": "jorton@redhat.com", "creation_time": "2010-01-31T06:10:30Z", "attachment_id": null}]