[{"count": 0, "tags": [], "text": "We're running Apache 2.0.54 on multiple Solaris 8 machines, using the prefork\nMPM, mmap and sendfile turned off - sharing a NFS mounted docroot hosted by a\nNetApp. Occasionally we are getting child processes die with either a bus error\nor segmentation fault - it seems to be taking the order of 1-2 hours of\nhigh-load production traffic to happen. From the cores I've inspected, they all\nseem to be when a request is being parsed by mod_include (so I've marked that as\nthe component - though something else may be at fault).\n\nHaving searched around, bug 34708 and bug 19325 look as if they might be related\nto this. Certainly the files in our docroot do get modified. The process would\nbe for the file to be removed, and then recreated by another system. So Apache\nmight be getting upset by the file disappearing mid request. It's unlikely\n(though I guess possible) that a file would be removed and recreated in time for\nApache to have one request on the original file and the subsequent request on\nthe new file (ie: without accessing it in the period of \"no file\"). My attempts\nat trying to recreate this scenario without production traffic have failed. So\nit's been difficult to rule out particular system components (though we have\nunloaded as much of our code as possible). Additionally, we are seeing some\nerrors on a couple of servers that have a different docroot (though still on\nNFS) where files seldom change (or at least not at the times we've seen errors).\n\nBased on comments in one of the bugs above, and looking at the head revision of\napr-util-trunk, I've also patched apr_buckets_file.c (two liner - so it checks\nfor APR_EOF) - I'll attach the diff. We still get errors (though it doesn't\nappear to hurt so I've left it in for now)...\n\nI've got two backtraces which I'll attach. Unfortunately we weren't initially\ncompiling with debugging symbols, though we have now enabled that. Since\nenabling, we've only had one error - otherwise I'd attach more.\n\nI'd offer to upload the core file but it's 100Mb as we have a huge chunk of\nshared memory in use. Note that I don't think that is to blame because the other\nservers don't use it (waiting for a segfault there so I can upload that\ninstead), and because the points of failure don't implicate it at all.\n\nAny questions or things you'd like me to try - please ask. Really keen to get to\nthe bottom of this.", "attachment_id": null, "id": 77945, "creation_time": "2005-08-02T17:11:33Z", "time": "2005-08-02T17:11:33Z", "creator": "stuart@terminus.co.uk", "bug_id": 35974, "is_private": false}, {"count": 1, "tags": [], "creator": "stuart@terminus.co.uk", "attachment_id": 15850, "id": 77948, "time": "2005-08-02T17:16:53Z", "bug_id": 35974, "creation_time": "2005-08-02T17:16:53Z", "is_private": false, "text": "Created attachment 15850\npatch in use against srclib/apr-util/buckets/apr_buckets_file.c"}, {"count": 2, "tags": [], "bug_id": 35974, "attachment_id": 15851, "id": 77949, "time": "2005-08-02T17:18:16Z", "creator": "stuart@terminus.co.uk", "creation_time": "2005-08-02T17:18:16Z", "is_private": false, "text": "Created attachment 15851\nfirst backtrace - no debugging symbols"}, {"count": 3, "tags": [], "bug_id": 35974, "attachment_id": 15852, "id": 77950, "time": "2005-08-02T17:20:38Z", "creator": "stuart@terminus.co.uk", "creation_time": "2005-08-02T17:20:38Z", "is_private": false, "text": "Created attachment 15852\n'full' backtrace - with debugging symbols"}, {"count": 4, "tags": [], "text": "#1  0xff3676a0 in apr_brigade_create (p=0x0, list=0x3c7ef0) at apr_brigade.c:67\n        b = (apr_bucket_brigade *) 0x3c8c80\n#2  0x000d17e0 in core_output_filter (f=0x3c3f10, b=0x3f4130) at core.c:4047\n\nthe p = NULL bit looks like the problem, unless that's a side-effect of stack\ncorruption.  In gdb if you \"up 2\" into core_output_filter, what does\n\n  p *f\n  p *f->conn\n\ngive?\n\nIf the pool really is NULL this could just be a symptom of hitting an\nout-of-memory condition - is that plausible for this system?  (httpd should be\ncalling abort() in that case but actually doesn't)", "attachment_id": null, "id": 77951, "creator": "jorton@redhat.com", "time": "2005-08-02T17:28:25Z", "bug_id": 35974, "creation_time": "2005-08-02T17:28:25Z", "is_private": false}, {"count": 5, "tags": [], "bug_id": 35974, "attachment_id": null, "text": "Quick response - thanks. :)\n\n(In reply to comment #4)\n> #1  0xff3676a0 in apr_brigade_create (p=0x0, list=0x3c7ef0) at apr_brigade.c:67\n>         b = (apr_bucket_brigade *) 0x3c8c80\n> #2  0x000d17e0 in core_output_filter (f=0x3c3f10, b=0x3f4130) at core.c:4047\n> \n> the p = NULL bit looks like the problem, unless that's a side-effect of stack\n> corruption.  In gdb if you \"up 2\" into core_output_filter, what does\n> \n>   p *f\n>   p *f->conn\n\nIndeed - meant to mention that's what seems to be the issue (though how it's\nhappened...). Anyway:\n\n(gdb) up 2\n#4  0x0007c494 in chunk_filter (f=0x3df960, b=0x3f0b70) at http_core.c:218\n218             rv = ap_pass_brigade(f->next, b);\n(gdb) p *f\n$1 = {frec = 0x1cd638, ctx = 0x0, next = 0x3c3f10, r = 0x3c9f30, c = 0x3c3b80}\n(gdb) p *f->conn\nThere is no member named conn.\n\nBut ITYM:\n\n(gdb) p *f->c\n$2 = {pool = 0x0, base_server = 0x0, vhost_lookup_data = 0x0, local_addr =\n0x3c3ae0, remote_addr = 0x0, \n  remote_ip = 0x3c3ed8 \"85.164.244.5\", remote_host = 0x0, remote_logname = 0x0,\naborted = 0, \n  keepalive = AP_CONN_CLOSE, double_reverse = 0, keepalives = 0, local_ip =\n0x3c3ec8 \"212.187.153.30\", \n  local_host = 0x0, id = 14, conn_config = 0x3c3bd8, notes = 0x3c3d70,\ninput_filters = 0x3c3ef8, \n  output_filters = 0x3c3f10, sbh = 0x3c1ac8, bucket_alloc = 0x3c7ef0}\n\n> If the pool really is NULL this could just be a symptom of hitting an\n> out-of-memory condition - is that plausible for this system?  (httpd should be\n> calling abort() in that case but actually doesn't)\n\nIncredibly unlikely. top claims:\n\nload averages:  1.29,  1.38,  1.21                                     16:37:17\n124 processes: 123 sleeping, 1 on cpu\nCPU states: 50.0% idle,  0.0% user, 50.0% kernel,  0.0% iowait,  0.0% swap\nMemory: 2048M real, 908M free, 768M swap in use, 1727M swap free\n\nvmstat gives similar numbers. The system is so busy because we're deliberately\noverloading that server in an attempt to reproduce the problem.\n\nOne other thing that's worth mentioning is another server that has the same\napache binary and mostly the same modules loaded has had no errors - but that\ndoes not have a NFS docroot (it serves purely dynamic requests).\n\nCheers", "id": 77952, "time": "2005-08-02T17:39:57Z", "creator": "stuart@terminus.co.uk", "creation_time": "2005-08-02T17:39:57Z", "is_private": false}, {"count": 6, "tags": [], "bug_id": 35974, "text": "It would be useful to monitor memory use over time to be able to correlate the\nchild crashes with memory exhaustion.  (using sar or just vmstat, for example)\n\nTo progress, otherwise, some ideas:\n\n- try an \"--enable-pool-debug\" build, if possible; such a build will be much\nslower and consume much more memory overall but might help tracking the issue down\n\n- unsubtle debugging hacks like \"if (!f->c->pool) abort()\" in strategic places\nlike the top of core_output_filter, if that particular failure mode recurs\n\n- check the r->uri in case this an issue specific to a particular resource, or\nat least, to localize testing against some specific resources.", "id": 77961, "time": "2005-08-02T18:48:20Z", "creator": "jorton@redhat.com", "creation_time": "2005-08-02T18:48:20Z", "is_private": false, "attachment_id": null}, {"count": 7, "tags": [], "text": "> It would be useful to monitor memory use over time to be able to correlate the\n> child crashes with memory exhaustion.  (using sar or just vmstat, for example)\n\nWe do have monitoring systems which should pick that kind of event up, but I now\nhave sar running as well.\n\n> - try an \"--enable-pool-debug\" build, if possible; such a build will be much\n> slower and consume much more memory overall but might help tracking the issue\n> down\n\nThis is being done. Will have to check the memory usage, but slower shouldn't\nmatter as we're only testing on one server out of a group.\n\n> - check the r->uri in case this an issue specific to a particular resource, or\n> at least, to localize testing against some specific resources.\n\nYeah, we tried this before. We managed to recreate a couple of requests from the\nnon-debugging enabled cores (using adb and some knowledge of the structures\ninvolved), and replay those without problem. Having inspected the more recent\ncores (we had a few more overnight) there's still no pattern. We suspected\ncookies at one point (as that's often where testing doesn't mirror real\ntraffic), but again they look normal. A couple seem to have happened outside\nrequest handling (eg: in apr_pool_clear called from the loop in child_main).\n\nAs I mentioned, we've got some more backtraces now. There hasn't been a repeat\nof the NULL pool error - they're more similar to the first backtrace I attached.\nOne common thing I've noticed is that whilst the errors are happening in a few\nplaces (though mostly within apr_pools.c or apr_buckets_alloc.c), they mostly\nseem to involve the value of a pointer to a apr_memnode_t being a bad address.\nFor example:\n\n(gdb) bt\n#0  0xff1dedb4 in allocator_alloc (allocator=0x3c0ef0, size=8192) at apr_pools.c:219\n#1  0xff1dd6d8 in apr_pool_create_ex (newpool=0x3c4044, parent=0x3c3a70,\nabort_fn=0, allocator=0x3c0ef0) at apr_pools.c:804\n#2  0x000d1f18 in core_output_filter (f=0x3c3f10, b=0x3dc8f0) at core.c:4182\n#3  0x000c14bc in ap_pass_brigade (next=0x3c3f10, bb=0x3dc8f0) at util_filter.c:512\n#4  0x00081308 in ap_http_header_filter (f=0x3d2838, b=0x3dc868) at\nhttp_protocol.c:1668\n#5  0x000c14bc in ap_pass_brigade (next=0x3d2838, bb=0x3dc868) at util_filter.c:512\n...\n\nLine 219 of apr_pools.c being:\n    if ((*ref = node->next) == NULL && i >= max_index) {\nSo:\n\n(gdb) p node\n$3 = (apr_memnode_t *) 0x4f6a5536\n(gdb) p *node\nCannot access memory at address 0x4f6a5536\n\nIf you'd like the full backtrace of that, or indeed the other new backtraces,\njust let me know.", "attachment_id": null, "id": 78004, "creator": "stuart@terminus.co.uk", "time": "2005-08-03T14:23:54Z", "bug_id": 35974, "creation_time": "2005-08-03T14:23:54Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 35974, "attachment_id": null, "id": 78220, "time": "2005-08-08T11:57:51Z", "creator": "stuart@terminus.co.uk", "creation_time": "2005-08-08T11:57:51Z", "is_private": false, "text": "We've now run with --enable-pool-debug... and got no errors at all! That server\nhas been running for over four days without a single abnormal exit. Meanwhile we\nset another server up without pool debugging - but otherwise an identical\nconfiguration and set of modules - and that contiues to have bus errors and seg\nfaults (a few a day).\n\nSo it would seem that something in the debugging code is either catching the\nproblem, or making it go away. At a quick glance, it would appear the debug code\nabort()s when it sees a problem - which we haven't seen... so I don't think\nthat's happening. In which case it's either doing something correct where the\nnon-debugging code goes wrong, or it's changing the program behaviour in such a\nway that the error conditions don't arise.\n\nI'm going to have a look at how pool initialisation/destruction differs between\nthe debugging and non-debugging code paths, but if you've got any insights of\nwhat to try or to look into, it would be much appreciated.\n"}, {"count": 9, "tags": [], "bug_id": 35974, "attachment_id": null, "text": "Just an update to say that we eventually ran the pool-debug build for a week\nwithout any errors. Regular builds continue to see a small number of errors per day.\n\nI also now have a coredump from one of our other servers which has the same\nApache binary, but only our simplest modules loaded. We see fewer errors there -\nthough they have a lot less traffic to deal with. As those machines don't use\nshared memory the file is only 2Mb - so I can upload it no problem if it would\nbe of any use. The backtrace appears to be identical to the one outlined in\ncomment 7.", "id": 79188, "time": "2005-08-30T17:18:56Z", "creator": "stuart@terminus.co.uk", "creation_time": "2005-08-30T17:18:56Z", "is_private": false}, {"count": 10, "tags": [], "bug_id": 35974, "text": "This problem is I believe still being seen. However, occurring in such small\nnumbers (given the number of machines and requests involved) to not be of\nsufficient importance to warrant ongoing investigation. As you can see, there\nhasn't been an update to this in two years. I see this as unlikely to change.\n\nAdditionally, I am no longer involved with that system, so can make no further\npersonal contribution. Hence, I am closing this bug.\n\nIn any case, the servers in question are undergoing a series of updates (OS,\nconfiguration, major apache version, MPM, etc) which will make comparisons\nimpossible. If the appropriate people still see problems after that, they can\nreopen/file new bugs as appropriate.", "id": 107597, "time": "2007-09-01T16:03:13Z", "creator": "stuart@terminus.co.uk", "creation_time": "2007-09-01T16:03:13Z", "is_private": false, "attachment_id": null}]