[{"count": 0, "tags": [], "creator": "nikke@acc.umu.se", "attachment_id": null, "id": 88363, "time": "2006-04-21T20:49:19Z", "bug_id": 39380, "creation_time": "2006-04-21T20:49:19Z", "is_private": false, "text": "The attached patch addresses the following issues:\n\n* Implement Large File Support (LFS) in mod_disk_cache.\n* Try to check if allowed to cache the file before caching it too, first\n  caching bits of a huge file and then toss it makes little sense.\n* When caching a file, copy it using the file descriptor in the brigade instead\n  of using apr_bucket_read which forces the file into memory. This produced a\n  segfault if trying to cache a file larger than the available amount of memory.\n* When having cached a file, replace the brigade referring to the source file\n  with our cached copy. This makes a huge difference when the file is larger\n  than your memory and thus not in cache, given that your cache filesystem\n  is faster than your backend (a natural assumption, why cache otherwise?).\n* When caching a file, keep the cache file even if the connection was aborted.\n  There is no reason to toss it, and the penalty for doing so when caching\n  DVD images is really huge.\n* When multiple downloads of an uncached file is initiated, only allow one of\n  them to cache the file and let the others wait for the result. It's not a\n  theoretically perfect solution, but in practice it seems to work well.\n* Consequently use disk_cache: in error log strings.\n* In mod_cache, restore r->filename so %f in LogFormat strings work. This\n  really should be solved by saving r->filename with the headers and restore\n  it in mod_disk_cache et al, but this at least provides something.\n\nThis allows us (http://ftp.acc.umu.se/) to use mod_disk_cache to cache DVD\nimages on a 32bit machine with \"only\" 3GB of memory, with the thing behaving\nin a sane way and our LogFormat xferlog style emulation working.\n\nAn annoying issue remains: When caching a file, mod_disk_cache copies the\nentire file before data is being sent to the client. This gets really annoying\nif you have large files (say 4.3GB DVD images) and a slow backend. If there\nis work in progress to solve this, please point us in that direction so we can\nget it finished.\n\nI'm not on any Apache mailing lists, so please CC me any feedback on this."}, {"count": 1, "tags": [], "bug_id": 39380, "is_private": false, "id": 88364, "creation_time": "2006-04-21T20:50:36Z", "time": "2006-04-21T20:50:36Z", "creator": "nikke@acc.umu.se", "text": "Created attachment 18152\nFIxes for mod_disk_cache: LFS-support, don't eat all your memory, etc.", "attachment_id": 18152}, {"count": 2, "tags": [], "bug_id": 39380, "is_private": false, "id": 88368, "creation_time": "2006-04-21T23:13:08Z", "time": "2006-04-21T23:13:08Z", "creator": "rpluem@apache.org", "text": "Many thanks for your patch. I will go through the points and the patch as soon\nas I have time to. In order to ease the discussion, review and integration into\nthe trunk please split your patch into a separate patch for each issue you\naddress and attach them to this report. So there should be 8 patch files instead\nof one.", "attachment_id": null}, {"count": 3, "tags": [], "bug_id": 39380, "is_private": false, "text": "OK. I'll start with two patches that are small and separated.\n\nHow do you propose to handle the rest? The problem is that since almost all\nchanges affect store_body() the only way for all of them to apply cleanly is\nto make them depend on eachother. Is this OK?", "id": 88374, "time": "2006-04-22T09:45:02Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-04-22T09:45:02Z", "attachment_id": null}, {"count": 4, "tags": [], "bug_id": 39380, "is_private": false, "text": "Created attachment 18154\nImplement Large File  Support in mod_disk_cache", "id": 88375, "time": "2006-04-22T09:46:57Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-04-22T09:46:57Z", "attachment_id": 18154}, {"count": 5, "text": "Created attachment 18155\nmod_cache: Provide r->filename so %f in LogFormat works\n\n* In mod_cache, restore r->filename so %f in LogFormat strings work. This\n  really should be solved by saving r->filename with the headers and restore\n  it in mod_disk_cache et al, but this at least provides something.\n\n%f is most often used when emulating the xferlog format (ie. when serving\nfiles), so even if it's not 100% accurate at all times the side effects of it\nbeing wrong isn't deadly.", "bug_id": 39380, "attachment_id": 18155, "id": 88376, "time": "2006-04-22T09:52:18Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-04-22T09:52:18Z", "tags": [], "is_private": false}, {"count": 6, "tags": [], "bug_id": 39380, "attachment_id": null, "id": 88377, "time": "2006-04-22T15:13:09Z", "creator": "rpluem@apache.org", "creation_time": "2006-04-22T15:13:09Z", "is_private": false, "text": "(In reply to comment #3)\n> OK. I'll start with two patches that are small and separated.\n> \n> How do you propose to handle the rest? The problem is that since almost all\n> changes affect store_body() the only way for all of them to apply cleanly is\n> to make them depend on eachother. Is this OK?\n\nYes, this is ok. Sorry that I have not been more precise on this subject. I\nwould like to see the low hanging fruits first, so that we get in the easiest\nthings first. By easy I also mean with possible least discussion on the mailing\nlist. So I would propose the following order for the patches:\n\n* Consequently use disk_cache: in error log strings.\n  (Yes, I am serious. I like to see this as a separate patch that goes in first,\nas it actually does not really touch code.)\n* Try to check if allowed to cache the file before caching it too, first\n  caching bits of a huge file and then toss it makes little sense.\n* Implement Large File Support (LFS) in mod_disk_cache.\n* When caching a file, copy it using the file descriptor in the brigade instead\n  of using apr_bucket_read which forces the file into memory. This produced a\n  segfault if trying to cache a file larger than the available amount of memory.\n* When having cached a file, replace the brigade referring to the source file\n  with our cached copy. This makes a huge difference when the file is larger\n  than your memory and thus not in cache, given that your cache filesystem\n  is faster than your backend (a natural assumption, why cache otherwise?).\n* In mod_cache, restore r->filename so %f in LogFormat strings work. This\n  really should be solved by saving r->filename with the headers and restore\n  it in mod_disk_cache et al, but this at least provides something.\n* When caching a file, keep the cache file even if the connection was aborted.\n  There is no reason to toss it, and the penalty for doing so when caching\n  DVD images is really huge.\n* When multiple downloads of an uncached file is initiated, only allow one of\n  them to cache the file and let the others wait for the result. It's not a\n  theoretically perfect solution, but in practice it seems to work well.\n\nDon't feel discouraged if answers to single patches take longer. This is no lack\nof interest, but in most cases lack of time or the need for a discussion on the\ndeveloper list. Feel free to give a ping here if you think that things stalled\ncompletely. Thanks for your understanding and patience.\n"}, {"count": 7, "tags": [], "creator": "nikke@acc.umu.se", "attachment_id": 18157, "id": 88394, "time": "2006-04-23T08:34:45Z", "bug_id": 39380, "creation_time": "2006-04-23T08:34:45Z", "is_private": false, "text": "Created attachment 18157\nmod_disk_cache: Consequently use disk_cache: in error log strings"}, {"count": 8, "text": "Created attachment 18158\nmod_disk_cache: check if allowed to cache the file before caching\n\n* Try to check if allowed to cache the file before caching it too, first\n  caching bits of a huge file and then toss it makes little sense.", "bug_id": 39380, "attachment_id": 18158, "id": 88395, "time": "2006-04-23T08:36:09Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-04-23T08:36:09Z", "tags": [], "is_private": false}, {"count": 9, "tags": [], "bug_id": 39380, "is_private": false, "text": "Created attachment 18159\nmod_disk_cache: Implement Large File Support (LFS)", "id": 88396, "time": "2006-04-23T08:37:50Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-04-23T08:37:50Z", "attachment_id": 18159}, {"count": 10, "tags": [], "bug_id": 39380, "attachment_id": null, "id": 88397, "time": "2006-04-23T08:43:50Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-04-23T08:43:50Z", "is_private": false, "text": "I'll start with the first three patches so you can start processing stuff at\nyour end, and if there are things I have to do differently I don't have to\nrespin so many patches.\n\nThe mod_cache patch is totally separated from the rest, so that one can also\nbe processed even though it's further down in your preferred order :)"}, {"count": 11, "text": "(In reply to comment #10)\n> I'll start with the first three patches so you can start processing stuff at\n> your end, and if there are things I have to do differently I don't have to\n> respin so many patches.\n\nMakes sense. Thanks for doing so. I will go through them. I already committed\nthe first one to trunk as r396252\n(http://svn.apache.org/viewcvs?rev=396252&view=rev)\n\n", "bug_id": 39380, "attachment_id": null, "id": 88402, "time": "2006-04-23T10:54:37Z", "creator": "rpluem@apache.org", "creation_time": "2006-04-23T10:54:37Z", "tags": [], "is_private": false}, {"count": 12, "text": "The LFS support can be done by just using apr_brigade_insert_file() - that code\nwas refactored out already.", "bug_id": 39380, "attachment_id": null, "id": 88427, "time": "2006-04-24T13:13:17Z", "creator": "jorton@redhat.com", "creation_time": "2006-04-24T13:13:17Z", "tags": [], "is_private": false}, {"count": 13, "tags": [], "bug_id": 39380, "is_private": false, "id": 88428, "creation_time": "2006-04-24T13:15:26Z", "time": "2006-04-24T13:15:26Z", "creator": "jorton@redhat.com", "text": "Plus the string->off_t conversion can be done using apr_strtoff() rather than\nsscanf.", "attachment_id": null}, {"count": 14, "tags": [], "creator": "nikke@acc.umu.se", "attachment_id": null, "is_private": false, "id": 88429, "time": "2006-04-24T13:21:26Z", "bug_id": 39380, "creation_time": "2006-04-24T13:21:26Z", "text": "(In reply to comment #12,#13)\n> The LFS support can be done by just using apr_brigade_insert_file() -\n> that code was refactored out already.\n\nAh. And I thought that server/*.c was a good example of how to do things,\nsilly me ;)\n\n> Plus the string->off_t conversion can be done using apr_strtoff() rather than\n> sscanf.\n\nGood point. That code was cut&paste from mod_mem_cache.\n\nIn general, it seems to me that there are a lot of code in httpd that would\nbenefit from an update to use current/recommended functions/API/etc ..."}, {"count": 15, "tags": [], "bug_id": 39380, "attachment_id": null, "id": 88552, "time": "2006-04-27T10:56:21Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-04-27T10:56:21Z", "is_private": false, "text": "The more I look at mod_disk_cache the more I find that needs revamping to solve\nthe larger problems.\n\nI'll take the whole mod_disk_cache-discussion to the devel maillist when we have\nironed out all problems, and then we'll se how to proceed regarding merging patches.\n\nHowever, the r->filename issue still has to be fixed. We have fixed it in\nmod_disk_cache, but someone should take a look at fixing it in mod_mem_cache\ntoo, it should be a pretty trivial fix."}, {"count": 16, "text": "Created attachment 18860\nhttpd 2.2.3 - mod_disk_cache jumbo patch - lfs/diskformat/read-while-caching etc.\n\nThe condensed version of what this patch does is the following:\n* Implement Large File Support (LFS).\n* Realise that files are files, by:\n   - copy files instead of reading them into memory or eat your mmap\n     space (depending on whether you have mmap enabled or not). This\n     caused segfaults on 32bit machines when caching LFS-files.\n   - cache files and URLs pointing to files separately, so if you have\n     a bunch of vhosts allowing access to a DVD image you'll only get\n     one copy in cache.\n* Only initiate one caching session per file, originally all sessions\n   initiated separate caching sessions until the file was cached.\n* When a file is being cached, other clients are served the data that has\n   been cached so far.\n* Don't throw away a painfully cached file just because the connection\n   was aborted, since it's a file the result is valid anyway.\n* Allow the caching of a file to happen in the background, the\n   original behaviour is to cache the whole file and then return data\n   to the client. This is bad for DVD-isos and other large files.\n* Rerarrange the on-disk structure to allow the above items. The big\n   improvement here is to cache both header and body in the same file,\n   the original has separate files which are a hassle to do atomic\n   operations on without locks and also makes cleaning the cache\n   unneccesarily hard.\n* As a side effect of the disk format change, %f in LogFormat now\n   works.\n* Add option to not try to remove directories in the cache structure.\n* Lots of code reorganisation and error handling to make it all\n   possible.", "bug_id": 39380, "attachment_id": 18860, "id": 93567, "time": "2006-09-14T08:59:34Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-09-14T08:59:34Z", "tags": [], "is_private": false}, {"count": 17, "tags": [], "bug_id": 39380, "attachment_id": 18861, "is_private": false, "id": 93568, "time": "2006-09-14T09:01:07Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-09-14T09:01:07Z", "text": "Created attachment 18861\nmod_disk_cache LFS-aware config\n\nLFS-config part of the jumbo patch.\n\nThis patch makes it possible to configure mod_disk_cache to cache\nfiles that are larger than the LFS limit. While at it, I implemented\nerror handling so it doesn't accept things like \"CacheMinFileSize\nbarf\" anymore."}, {"attachment_id": null, "tags": [], "creator": "minfrin@sharp.fm", "is_private": false, "count": 18, "id": 93573, "time": "2006-09-14T12:37:20Z", "bug_id": 39380, "creation_time": "2006-09-14T12:37:20Z", "text": "Ok, the mod_disk_cache LFS-aware config patch is easily digestible. The jumbo\npatch is too big - is the jumbo patch the sum of the patches that were posted in\nApril, or is the jumbo patch something new?"}, {"count": 19, "tags": [], "bug_id": 39380, "is_private": false, "id": 93581, "creation_time": "2006-09-14T13:33:13Z", "time": "2006-09-14T13:33:13Z", "creator": "nikke@acc.umu.se", "text": "The jumbo patch is what our production code looks like vs. 2.2.3. It's intended\nas reference and for those who wants to know where I'm heading with all this.\nIt's not intended for merging as is.\n\nThe old attachments were work in progress before I realised how sad the state of\nmod_disk_cache were.\n\nThe \"LFS aware config\"-patch is for trunk, identical to the one posted to the\ndev mailing list and should be suitable for merging. It doesn't break any\nexisting valid configurations.", "attachment_id": null}, {"count": 20, "tags": [], "bug_id": 39380, "is_private": false, "id": 94104, "creation_time": "2006-09-26T08:52:01Z", "time": "2006-09-26T08:52:01Z", "creator": "nikke@acc.umu.se", "text": "Created attachment 18916\nmod_disk_cache working LFS (filecopy)\n\nThis patch depends on \"mod_disk_cache LFS-aware config\" and is for trunk. It's\na subset of our jumbo-patch.\n\nIt makes caching of large files possible on 32bit machines by:\n\n* Realising that a file is a file and can be copied as such, without\n  reading the whole thing into memory first.\n* When a file is cached by copying, replace the brigade with a new one\n  refering to the cached file so we don't have to read the file from\n  the backend again when sending a response to the client.\n* When a file is cached by copying, keep the file even if the client\n  aborts the connection since we know that the response is valid.\n* Check a few more return values to be able to add \"successfully\" in\n  the appropriate places above.", "attachment_id": 18916}, {"count": 21, "text": "Attachments 18916 and 18861 have been applied.", "bug_id": 39380, "attachment_id": null, "id": 94129, "time": "2006-09-26T16:30:31Z", "creator": "minfrin@sharp.fm", "creation_time": "2006-09-26T16:30:31Z", "tags": [], "is_private": false}, {"attachment_id": 18968, "tags": [], "bug_id": 39380, "is_private": false, "count": 22, "id": 94555, "time": "2006-10-05T14:05:38Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-10-05T14:05:38Z", "text": "Created attachment 18968\nRearrange load/store to not depend on temporary files.\n\nThe goal of this patch is to do away with the\nwrite-to-file-then-move-in-place mentality.\n\nThis is done by rearranging the load (open_entity) and store (store_headers)\ninto multiple functions to get better overview, and thus enable the\npossibillity to detect partial reads and other issues that can happen.\n\nThis also introduces a disk format change, since we need the size of the\nbody in the header to be able to know what's happening.\n\nThe patch is for trunk."}, {"count": 23, "tags": [], "bug_id": 39380, "attachment_id": 18969, "id": 94556, "time": "2006-10-05T14:08:36Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-10-05T14:08:36Z", "is_private": false, "text": "Created attachment 18969\nRead While Caching - don't stall clients accessing a file being cached.\n\nImplements Read While Caching by introducing a DISKCACHE bucket which morphs\ninto FILE buckets as the file is being cached.\n\nThe setaside-function is incomplete, but those cases doesn't seem to happen in\nnormal production use for us.\n\nThis is copied as is from our production code, and is rock solid for us."}, {"attachment_id": 18979, "tags": [], "bug_id": 39380, "is_private": false, "count": 24, "id": 94632, "time": "2006-10-08T10:37:29Z", "creator": "nikke@acc.umu.se", "creation_time": "2006-10-08T10:37:29Z", "text": "Created attachment 18979\nFixups for the load/store-patch.\n\nI discovered a few misses, mostly not NULL:ing fd pointers when closing\nthem, missing close/flush, and some unneccessary code duplication instead of\ncalling the right helper in replace_brigade_with_cache().\n\nThe misses are in the load/store-patch, so I would recommend applying this\nbefore reviewing the results even though it's generated from a file with the\nread-while-caching patch applied."}, {"count": 25, "tags": [], "bug_id": 39380, "is_private": false, "id": 94633, "creation_time": "2006-10-08T11:03:34Z", "time": "2006-10-08T11:03:34Z", "creator": "nikke@acc.umu.se", "text": "Created attachment 18980\nDo background copy of a file while caching\n\nThis patch implements copying a file in the background so the client\ninitiating the caching can get the file delivered by read-while-caching\ninstead of having to wait for the file to finish.", "attachment_id": 18980}, {"count": 26, "text": "Created attachment 19418\nhttpd 2.2.4 - mod_disk_cache jumbo patch - lfs/diskformat/read-while-caching etc.\n\nThis is an update of the mod_disk_cache jumbo patch.\n\nIt's been running for a couple of months on ftp.acc.umu.se now, so it's fairly\nstable.\n\nHighlights from previous patch:\n* Reverted to separate files for header and data, there were too many corner\ncases and having the data file separate allows us to reuse the cached data for\nother purposes (for example rsync).\n* Fixed on disk headers to be stored in easily machine parseable format which\nallows for error checking instead human readable form that doesn't.\n* Attaching the background thread to the connection instead of request pool\nallows for restarts to work, the thing doesn't crash when you do apachectl\ngraceful anymore :)\n* Lots of error handling fixes and corner cases, we triggered most of them when\nour backend went bezerk-go-slow-mode.\n* Deletes cached files when cache decides that the object is stale for real,\npreviously it only NULL:ed the data structure in memory causing other requests\noto read headers etc.\n\nKnown issues:\n* htcacheclean builds but is broken.", "bug_id": 39380, "attachment_id": 19418, "id": 98166, "time": "2007-01-17T01:56:34Z", "creator": "nikke@acc.umu.se", "creation_time": "2007-01-17T01:56:34Z", "tags": [], "is_private": false}, {"count": 27, "tags": [], "bug_id": 39380, "is_private": false, "id": 106006, "creation_time": "2007-07-27T14:06:44Z", "time": "2007-07-27T14:06:44Z", "creator": "nikke@acc.umu.se", "text": "Created attachment 20558\nhttpd-2.2.4 - mod_disk_cache jumbo patch - lfs/diskformat/read-while-caching etc.\n\nhttpd 2.2.4 - mod_disk_cache jumbo patch - lfs/diskformat/read-while-caching\netc.\n\nA snapshot from 20070727 of our mod_disk_cache jumbo patch and some assorted\nadditional patches that's needed for stability. We've been running this for a\ncouple of months on ftp.acc.umu.se, they have survived Debian/Ubuntu/Mozilla\nreleases gracefully.\n\nThis version plays well with other entities using/updating the cache. We are\nusing a open()-wrapper in combination with rsync which lets rsync utilise the\ncached bodies, and also cache files.\n\nThis patch is provided mostly as a one-patch solution for other sites that\nwishes to use these mod_disk_cache modifications.\n\nHighlights from previous patch:\n* More corner case error fixes, most of them triggered by Mozilla releases.\n* Greatly reduced duplicated data in the cache when using an NFS backend by\nhashing the body on the source files device and inode when available. HTTPD has\nalready done the stat() of the file for us, so it's essentially free.\n* Tidied up the handling of updated files, only delete files in cache if\nthey're\nreally obsolete.", "attachment_id": 20558}, {"attachment_id": 21016, "tags": [], "creator": "nikke@acc.umu.se", "is_private": false, "count": 28, "id": 109519, "time": "2007-10-21T01:42:55Z", "bug_id": 39380, "creation_time": "2007-10-21T01:42:55Z", "text": "Created attachment 21016\nhttpd-2.2.6 - mod_disk_cache jumbo patch - 2.2.6 version\n\nAdaptation of the patch to httpd 2.2.6. Includes the fixes made to the original\nversion of mod_disk_cache made in httpd 2.2.6.\n\nIt has survived a Ubuntu-release, so it's fairly stable."}, {"count": 29, "tags": [], "bug_id": 39380, "attachment_id": 21519, "is_private": false, "id": 113707, "time": "2008-02-13T05:52:57Z", "creator": "rafaelspereira@gmail.com", "creation_time": "2008-02-13T05:52:57Z", "text": "Created attachment 21519\nFIX htcacheclean for mod_disk_cache jumbo patch - 2.2.6 version\n\nThis patch fixes htcacheclean, to be compatible with LFS cache structure. It\nmust be applied after patch 21016: httpd-2.2.6 - mod_disk_cache jumbo patch -\n2.2.6 version\nThe -i option still not working."}, {"count": 30, "tags": [], "bug_id": 39380, "attachment_id": 21715, "id": 114960, "time": "2008-03-25T13:32:02Z", "creator": "kbeevers@voxel.net", "creation_time": "2008-03-25T13:32:02Z", "is_private": false, "text": "Created attachment 21715\nFIX corruption in near-simultaneous requests of uncached files for mod_disk_cache jumbo patch - 2.2.6 version\n\nThis patch addresses an issue when (a) mod_proxy is in use; and (b) two requests for the same uncached file arrive almost simultaneously.  Data is proxied initially, but upon attempting to cache the headers, a conflicting header cache is discovered; this results in a later call to replace_brigade_with_cache.  This patch modifies the behavior of replace_brigade_with_cache (via changes to recall_body) to account for bytes already proxied to the client."}, {"count": 31, "text": "Created attachment 22127\nhttpd-2.2.9 - mod_disk_cache jumbo patch - 2.2.9 version\n\nAdaptation of the patch to httpd-2.2.9.\n\nIncludes the submitted fixes for:\n\"FIX htcacheclean\" - Since we use a script that looks at atime we don't use htcacheclean. The fix looks sane though.\n\n\"FIX corruption in near-simultaneous requests of uncached files\" - We're not hitting this, so it probably only affects usage in combination with proxies.\n\nMajor additional fixes:\n- Adapt to recent APR sub-second file timestamps, meaning that we have to truncate to whole-second granularity when comparing http timestamps with file timestamps.\n- Be a tad more clever when trying to detect corrupted files (commonly caused by a machine using xfs crashing). It's perfectly valid to have the consumed size smaller than the actual size, for example filesystems with compression (SUN ZFS). Also, don't check consumed size on new files since for example ZFS only updates it when data is commited to disk.", "bug_id": 39380, "attachment_id": 22127, "id": 117692, "time": "2008-06-14T10:38:41Z", "creator": "nikke@acc.umu.se", "creation_time": "2008-06-14T10:38:41Z", "tags": [], "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 39380, "is_private": false, "count": 32, "id": 166974, "time": "2013-04-30T16:33:42Z", "creator": "minfrin@sharp.fm", "creation_time": "2013-04-30T16:33:42Z", "text": "Is there any news about an updated patch for httpd v2.4?"}]