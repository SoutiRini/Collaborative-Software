[{"count": 0, "text": "To ensure that our RollingFileAppender would not start deleting files soon, we \nset maxbackupindex=99999999.\n\nThis seemed to work, but we experienced that this would make log4j halt to a \nstandstill at each time the rollover should happen.\n\nThe culprit is that RollingFileAppender at line 123 counts DOWN from \nmaxbackupindex and try to check if a file exists with that number.\n\nThis is VERY ineffective when maxbackupindex is a large number.\n\nWhy don't it start form 0 (zero) and try to find the first missing file ? \nThat would make it complete the search in a more reasonable time.\n\nOr even better - do a File.list() and do it all in memory instead of accessing \nthe file system for each iteration.\n\nIf you don't find this a solution, then at least warn about this in the docs of \nmaxbackupindex.", "bug_id": 24407, "attachment_id": null, "id": 46812, "time": "2003-11-04T20:35:34Z", "creator": "max@eos.dk", "creation_time": "2003-11-04T20:35:34Z", "tags": [], "is_private": false}, {"count": 1, "text": "Good call. However, I'd rather see it use a binary search for the first\nnon-existent file, not a File.list().", "bug_id": 24407, "attachment_id": null, "id": 46815, "time": "2003-11-04T20:50:12Z", "creator": "toby.butzon@ilc.com", "creation_time": "2003-11-04T20:50:12Z", "tags": [], "is_private": false}, {"count": 2, "tags": [], "creator": "max@eos.dk", "attachment_id": null, "is_private": false, "id": 46816, "time": "2003-11-04T20:55:15Z", "bug_id": 24407, "creation_time": "2003-11-04T20:55:15Z", "text": "well - it need to load the list of already genereate files, rigt ? \n\nThat involves File.list(), right ? ;)\n\nor more precise:\n\nFile[] listFiles(FileFilter filter) and then a binary search or someting ;)"}, {"count": 3, "tags": [], "bug_id": 24407, "attachment_id": null, "is_private": false, "id": 46818, "time": "2003-11-04T21:32:59Z", "creator": "toby.butzon@ilc.com", "creation_time": "2003-11-04T21:32:59Z", "text": "No. A binary search could simply check File.exists() and very little would need\nto be loaded into memory. Even for 99,999,999 files, this is under 30 calls (in\nthe worse case) to File.exists(). The performance tradeoff for a small number of\nalready-present files would be negligable -- you could do it either way;\nhowever, the memory lost in doing a File.list() compared to the unchanging 30\nFile.exists() or less when there are many existing log files that match the\nfilter makes it worthwhile to stick to the simple binary search using File.exists().\n\nNote that for a more reasonable MaxBackupIndex, the number of File.exists()\ncalls gets vastly smaller.\n\nI realize this difference is somewhat petty -- there are probably only a handful\nof people out there who would ever have that many logfiles in the same\ndirectory. While we're on it though, we might as well do something that scales\nreasonably well; there's no compelling reason to load the entire File.list()\ninstead of doing a few File.exists().."}, {"count": 4, "tags": [], "bug_id": 24407, "attachment_id": null, "id": 46819, "time": "2003-11-04T21:37:34Z", "creator": "max@eos.dk", "creation_time": "2003-11-04T21:37:34Z", "is_private": false, "text": "ah - of course...misunderstood it the first time ;)\n\nYes - binary search will be the most effective (even for small numbers ;)\n"}, {"count": 5, "tags": [], "bug_id": 24407, "attachment_id": 8939, "is_private": false, "id": 46837, "time": "2003-11-05T03:18:26Z", "creator": "toby.butzon@ilc.com", "creation_time": "2003-11-05T03:18:26Z", "text": "Created attachment 8939\nHere's an untested patch -- I'll test it tomorrow and let you know."}, {"count": 6, "tags": [], "bug_id": 24407, "attachment_id": null, "id": 68507, "time": "2004-12-14T21:12:47Z", "creator": "yoavs@computer.org", "creation_time": "2004-12-14T21:12:47Z", "is_private": false, "text": "If you're still interested and can submit a patch to fit the new RollingPolicy \nand related classes in log4j 1.3, that'd be great.  The code has changed \nsignificantly enough that this patch is not a trivial conversion in the new \nframework, so as it is, we can't commit it."}]