[{"count": 0, "tags": [], "creator": "stuart@terminus.co.uk", "attachment_id": null, "text": "A child process on one of our servers has got stuck eating all available CPU.\nWe've seen this a couple of times now maybe (not very often given the number of\nrequests we do). Now we have -g builds as standard, I was able to attach gdb to\nit this time and discovered the following:\n\n[Switching to Thread 1 (LWP 1)]\n0xff1de3f4 in apr_pool_cleanup_kill (p=0x3cd3c0, data=0x3bf9e8,\ncleanup_fn=0xff355588 <pool_bucket_cleanup>) at apr_pools.c:1911\n1911    apr_pools.c: No such file or directory.\n        in apr_pools.c\n(gdb) bt\n#0  0xff1de3f4 in apr_pool_cleanup_kill (p=0x3cd3c0, data=0x3bf9e8,\ncleanup_fn=0xff355588 <pool_bucket_cleanup>) at apr_pools.c:1911\n#1  0xff35571c in pool_bucket_destroy (data=0x3bf9e8) at apr_buckets_pool.c:79\n#2  0xff3575c8 in apr_brigade_cleanup (data=0x3deb80) at apr_brigade.c:48\n#3  0x0004fabc in send_parsed_content (f=0x3c2488, bb=0x3ed238) at\nmod_include.c:3311\n#4  0x00050a30 in includes_filter (f=0x3c2488, b=0x3ed1c8) at mod_include.c:3591\n#5  0x000c14bc in ap_pass_brigade (next=0x3c2488, bb=0x3ed1c8) at util_filter.c:512\n#6  0xfee12548 in ap_headers_output_filter (f=0x3ec078, in=0x3ed1c8) at\nmod_headers.c:538\n#7  0x000c14bc in ap_pass_brigade (next=0x3ec078, bb=0x3ed1c8) at util_filter.c:512\n#8  0xfe365b50 in gu_vignette_ape_out_filter (f=0x3eb608, in_brigade=0x3baff8)\nat mod_gu_vignette.c:881\n#9  0x000c14bc in ap_pass_brigade (next=0x3eb608, bb=0x3baff8) at util_filter.c:512\n#10 0xfed75050 in ap_proxy_http_process_response (p=0x3bab00, r=0x3eb468,\np_conn=0x3bb048, origin=0x3bb230, backend=0x3bb060, conf=0x21a228, bb=0x3baff8, \n    server_portstr=0xffbef1d0 \"\") at proxy_http.c:937\n#11 0xfed757e4 in ap_proxy_http_handler (r=0x3eb468, conf=0x21a228, url=0x3bb150\n\"/\", proxyname=0x0, proxyport=0) at proxy_http.c:1107\n#12 0xfedb5e54 in proxy_run_scheme_handler (r=0x3eb468, conf=0x21a228,\nurl=0x3ec04e \"http://ape-liveprod.gul3.gnl:4738/\", proxyhost=0x0, proxyport=0)\n    at mod_proxy.c:1115\n#13 0xfedb3ac0 in proxy_handler (r=0x3eb468) at mod_proxy.c:420\n#14 0x000a6b0c in ap_run_handler (r=0x3eb468) at config.c:152\n#15 0x000a776c in ap_invoke_handler (r=0x3eb468) at config.c:364\n#16 0x000d7db8 in ap_run_sub_req (r=0x3eb468) at request.c:1855\n#17 0xfe3640d0 in gu_vignette_handler (r=0x3cd3f8) at mod_gu_vignette.c:471\n#18 0x000a6b0c in ap_run_handler (r=0x3cd3f8) at config.c:152\n#19 0x000a776c in ap_invoke_handler (r=0x3cd3f8) at config.c:364\n#20 0x00085198 in ap_process_request (r=0x3cd3f8) at http_request.c:249\n#21 0x0007c5c4 in ap_process_http_connection (c=0x3bac10) at http_core.c:251\n#22 0x000bc684 in ap_run_process_connection (c=0x3bac10) at connection.c:43\n#23 0x000bccb4 in ap_process_connection (c=0x3bac10, csd=0x3bab38) at\nconnection.c:176\n#24 0x000a4280 in child_main (child_num_arg=83) at prefork.c:610\n#25 0x000a44b8 in make_child (s=0x1ce020, slot=83) at prefork.c:704\n#26 0x000a48e0 in perform_idle_server_maintenance (p=0x1c90f8) at prefork.c:839\n#27 0x000a5000 in ap_mpm_run (_pconf=0x1c90f8, plog=0x2011d8, s=0x1ce020) at\nprefork.c:1040\n#28 0x000af9d0 in main (argc=3, argv=0xffbefccc) at main.c:618\n\nSteping through, it's just looping round this code:\n\n    while (c) {\n        if (c->data == data && c->plain_cleanup_fn == cleanup_fn) {\n            *lastp = c->next;\n            break;\n        }\n\n        lastp = &c->next;\n        c = c->next;\n    }\n\nSo, let's have a look the data we've got:\n\n(gdb) p c\n$1 = (cleanup_t *) 0x3ed420\n(gdb) p *c\n$2 = {next = 0x3c1378, data = 0x3ef428, plain_cleanup_fn = 0x1, child_cleanup_fn\n= 0}\n(gdb) p *c->next\n$3 = {next = 0x3cd3a8, data = 0x3ed420, plain_cleanup_fn = 0x1, child_cleanup_fn\n= 0}\n(gdb) p *c->next->next\n$4 = {next = 0x3ef428, data = 0x3c1378, plain_cleanup_fn = 0x1, child_cleanup_fn\n= 0}\n(gdb) p *c->next->next->next\n$5 = {next = 0x3ed420, data = 0x3cd3a8, plain_cleanup_fn = 0xff355588\n<pool_bucket_cleanup>, child_cleanup_fn = 0}\n\nWhoops - the list has turned back on itself, so we're stuck in the loop.\nPresumably (from the NULL check), this shouldn't be the case.\n\nThis looks related to bug 35974 where we're seeing the address of memory nodes\nbeing corrupt (pointing to invalid addresses which of course result in seg\nfaults/bus errors on attempted access).\n\nI've ask our systems team to leave that server blocked and the errant process\nrunning, so if you'd like to know the state of anything else please let me know.", "id": 78883, "time": "2005-08-23T18:40:01Z", "bug_id": 36324, "creation_time": "2005-08-23T18:40:01Z", "is_private": false}, {"count": 1, "tags": [], "creator": "jwoolley@apache.org", "attachment_id": null, "text": "I see that there's a 3rd party module in the call stack.  It's hard to know\nwhether the data structure corruption is coming from the 3rd party module or\nfrom within Apache.  I realize this is on production machines so this might not\nbe possible, but if you could enable pool debugging and bucket debugging in a\nbuild at least long enough to repro the problem, that would be very helpful in\ntracking down the source of the corruption.", "id": 78888, "time": "2005-08-23T22:20:47Z", "bug_id": 36324, "creation_time": "2005-08-23T22:20:47Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "stuart@terminus.co.uk", "is_private": false, "count": 2, "id": 78911, "time": "2005-08-24T12:16:15Z", "bug_id": 36324, "creation_time": "2005-08-24T12:16:15Z", "text": "Heya\n\n> I see that there's a 3rd party module in the call stack.  It's hard to know\n> whether the data structure corruption is coming from the 3rd party module or\n> from within Apache.\n\nIt's our own module. Naturally this is suspect! :) I have tried to find things\nin it that could cause this, but haven't been successful. Sadly we can't run in\nproduction without that particular module so testing with it unloaded is impossible.\n\n> I realize this is on production machines so this might not\n> be possible, but if you could enable pool debugging and bucket debugging in a\n> build at least long enough to repro the problem, that would be very helpful in\n> tracking down the source of the corruption.\n\nWe are quite happy to run one machine with those on (is bucket debugging a\nseperate configure option to --enable-pool-debug?). However, the occurence of\nthe runaway child processes is so low (say 3-4 over 12 machines in 3 weeks) that\nwe could run for weeks without reproducing it. I'm loath to try something that\nwill be very hard to draw a conclusion from (has it actually solved it, or have\nwe simply not hit the problem yet).\n\nAdditionally, it's worth noting that when we turned on pool debugging to\ninvestigate bug 35974 the problem there (which happens a handful of times a day)\ndisappeared. The only idea I came up from looking at other bugs was whether our\nproblems are down to the lifetime of data created in subrequests - and that\nperhaps this is different under a pool debug build (we tried the patch in bug\n12655 without success)."}, {"attachment_id": null, "tags": [], "creator": "nick@webthing.com", "text": "I wonder if this is another manifestation of the subrequest+filter issues \ndiscussed re: Bug 17629?  Are you mixing main- and sub-request output streams \nvia different filter configs?  (mod_diagnostics might help find out, if that's \na difficult question). ", "count": 3, "id": 79001, "time": "2005-08-25T16:39:44Z", "bug_id": 36324, "creation_time": "2005-08-25T16:39:44Z", "is_private": false}, {"count": 4, "tags": [], "bug_id": 36324, "is_private": false, "text": "Apologies for the slight delay replying - I've been away over a long weekend.\n\n> I wonder if this is another manifestation of the subrequest+filter issues \n> discussed re: Bug 17629?  Are you mixing main- and sub-request output streams \n> via different filter configs?  (mod_diagnostics might help find out, if that's \n> a difficult question).\n\nNot sure exactly how you mean \"different filter configs\", but I suspect yes. We\nuse mod_include heavily, but our own code sometimes creates additional\nsubrequests (the ones you see going through mod_proxy in the backtrace above).\nThese subrequests have a custom input filter (which creates their \"request\") and\noutput filter (which may or may not modify the response data) added using\nap_add_input_filter/ap_add_output_filter. The output from those is passed back\nup to the originating request (which could be either the main one, or a SSI\ngenerated subrequest, and doesn't have the custom filters).", "id": 79162, "time": "2005-08-30T13:05:34Z", "creator": "stuart@terminus.co.uk", "creation_time": "2005-08-30T13:05:34Z", "attachment_id": null}, {"count": 5, "tags": [], "text": "Without eliminating the third-party module dependency here I'm afraid there's\nnot much more anyone can do to help track this down.", "attachment_id": null, "bug_id": 36324, "id": 81903, "time": "2005-10-28T17:53:10Z", "creator": "jorton@redhat.com", "creation_time": "2005-10-28T17:53:10Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "jim@apache.org", "is_private": false, "count": 6, "id": 106287, "time": "2007-08-03T08:08:13Z", "bug_id": 36324, "creation_time": "2007-08-03T08:08:13Z", "text": "This is quite old. Please test/confirm with more recent versions and reopen if needed."}]