[{"count": 0, "tags": [], "bug_id": 36290, "attachment_id": null, "id": 78766, "time": "2005-08-21T12:48:38Z", "creator": "anathaniel@apache.org", "creation_time": "2005-08-21T12:48:38Z", "is_private": false, "text": "It is well documented that filtering may corrupt binary files.  I now found \nthat a similar issue exists with text files.\n\nWhen a text file with LATIN1 encoding is read assuming UTF-8 encoding, then a-\numlaut and other non-ASCII characters are replaced by '?' because these LATIN1 \nbyte values are not valid UTF-8 sequences.\n\nNow this is what happens if this task\n\n  <copy filtering=\"on\" todir=\"bar\">\n    <fileset dir=\"foo\">\n      <include name=\"**/*.xml\"/>\n    </fileset>\n  </copy>\n\nis applied to XML files with encoding=\"iso-8859-1\" on a platform with UTF-8 as \ndefault encoding.\n\nThe easy workaround is to set explicitly <copy filtering=\"on\" todir=\"bar\" \nencoding=\"iso-8859-1\">.  This also copies correctly UTF-8 encoded files \ncontaining multi-byte character sequences.  Token replacement of ASCII strings \nalso works correctly independent of the encoding.\n\nMy proposal is now to make this the default behaviour for the <copy> task:\nIf no explicit encoding is specified, do not use the platform dependent default \nencoding (which may be UTF-8) but always use iso-8859-1."}, {"count": 1, "text": "why would we make the default latin1? Why not Big5? If there has to be a default\nwould not some sort of universal charset such as utf-8 not be a better choice.\nWhy not leave this as the platform default?\n\nAm I missing something?", "creator": "conor@apache.org", "attachment_id": null, "id": 78862, "time": "2005-08-23T03:16:10Z", "bug_id": 36290, "creation_time": "2005-08-23T03:16:10Z", "tags": [], "is_private": false}, {"count": 2, "tags": [], "bug_id": 36290, "attachment_id": null, "is_private": false, "id": 78874, "time": "2005-08-23T10:14:32Z", "creator": "anathaniel@apache.org", "creation_time": "2005-08-23T10:14:32Z", "text": "It need not be LATIN1 but it must be an encoding which can be used to read an \narbitrary byte stream into Java chars and write it out again that the new file \nis a genuine copy of the original.  For token replacement to work, ASCII must \nbe a subset.\n\nUTF-8 cannot be used because some of the 256 byte values are invalid.\n\nMulti-byte encodings cannot be used because they would fail odd file lengths.\n\nI came across this problem in the Cocoon build file macro where an unknown set \nof XML files with eithor UTF-8 or ISO-8859-1 encodings by author's choice needs \nto be copied, possibly with token replacement.  The platform default encoding \nis outside our control.\n\nOf course, an even better solution would be if the copy+filtering task looked \nat the <?xml encoding=\"...\"?> to determine the correct encoding for XML files."}, {"count": 3, "tags": [], "creator": "stevel@apache.org", "attachment_id": null, "text": "Sounds like you have a special problem, in which the encoding of the doc is only\nknown inside the doc itself. We cannot be expected to patch <copy> to handle\nsuch complexity, and there is no \"right\" choice for default encoding other than\n\"platform\" or \"UTF8\"; the latter not being an option for backwards\ncompatibility. What may make sense is a filterencoding attribute, to declare\nwhat encoding you need.\n\nthe question is then: how to solve your particular problem. Perhaps in the\nhypothetical xmltasks antlib we intermittently discuss, something to do encoding\nconversion of XML docs would be one of the tasks. \n\nOf course, you may be albe to use the <xslt> task to do exactly that, before you\ndo any property expansion.", "id": 78879, "time": "2005-08-23T12:26:46Z", "bug_id": 36290, "creation_time": "2005-08-23T12:26:46Z", "is_private": false}, {"count": 4, "tags": [], "creator": "anathaniel@apache.org", "attachment_id": null, "text": "My immediate problem is solved by using <copy filtering=\"on\" encoding=\"iso-8859-\n1\"> since that handles fine also UTF-8 encoded files.  I have no reason to \nassume that other more exotic encodings will be used any time soon.\n\n<xslt> may be a solution of XML files but in our case there could also be non-\nXML text files in the fileset.\n\nI find it very confusing that something called <copy> does unintended \nmodifications.  It would be clearer to deprecate <copy filtering=\"on\"> and \ncreate an new <filter> task to replace it.\n\nMay I just suggest to add a sentence to the note on the documentation page:\n \nNOTE: If you employ filters in your copy operation, you should limit the copy \nto text files. Binary files will be corrupted by the copy operation. ***Text \nfiles may also be corrupted by the copy operation unless the encoding attribute \nmatches the file content.*** This applies whether the filters are implicitly \ndefined by the filter task or explicitly provided to the copy operation as \nfiltersets.\n", "id": 78907, "time": "2005-08-24T09:40:41Z", "bug_id": 36290, "creation_time": "2005-08-24T09:40:41Z", "is_private": false}, {"count": 5, "tags": [], "bug_id": 36290, "attachment_id": null, "id": 93191, "time": "2006-09-08T08:38:13Z", "creator": "peterreilly@apache.org", "creation_time": "2006-09-08T08:38:13Z", "is_private": false, "text": "I have added a long winded readme to the <copy>\ntask regarding file encoding:\n\n  <p>\n    <strong><a name=\"encoding\">Important Encoding Note:</a></strong>\n    The reason that binary files when filtered get corrupted is that\n    filtering involves reading in the file using a Reader class. This\n    has an encoding specifing how files are encoded. There are a number\n    of different types of encoding - UTF-8, UTF-16, Cp1252, ISO-8859-1,\n    US-ASCII and (lots) others. On Windows the default character encoding\n    is Cp1252, on Unix it is usually UTF-8. For both of these encoding\n    there are illegal byte sequences (more in UTF-8 than for Cp1252).\n  </p>\n  <p>\n    How the Reader class deals with these illegal sequences is up to the\n    implementation\n    of the character decoder. The current Sun Java implemenation is to\n    map them to legal characters. Previous Sun Java (1.3 and lower) threw\n    a MalformedInputException. IBM Java 1.4 also thows this exception.\n    It is the mapping of the characters that cause the corruption.\n  </p>\n  <p>\n    On Unix, where the default is normally UTF-8, this is a <em>big</em>\n    problem, as it is easy to edit a file to contain non US Ascii characters\n    from ISO-8859-1, for example the Danish oe character. When this is\n    copied (with filtering) by Ant, the character get converted to a\n    question mark (or some such thing).\n  </p>\n  <p>\n    There is not much that Ant can do. It cannot figure out which\n    files are binary - a UTF-8 version of Korean will have lots of\n    bytes with the top bit set. It is not informed about illegal\n    character sequences by current Sun Java implementions.\n  </p>\n  <p>\n    One trick for filtering containing only US-ASCII is to\n    use the ISO-8859-1 encoding. This does not seem to contain\n    illegal character sequences, and the lower 7 bits are US-ASCII.\n    Another trick is to change the LANG environment variable from\n    something like \"us.utf8\" to \"us\".\n  </p>\n"}]