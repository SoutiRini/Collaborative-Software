[{"count": 0, "tags": [], "bug_id": 61222, "text": "When an CGI program generates huge amount of output (magnitude of many gigabytes),\nthe system becomes unresponsive due to memory consumption by Apache worker process\nwhich executed that CGI program. The Apache worker process should not consume too\nmuch memory trying to spool the output from the CGI program. Even if spooling is\nnecessary for some reason, it should not consume too much memory and it should\nfree the memory after current request completed.\n\nOriginal report (including steps to reproduce) is at https://bugzilla.redhat.com/show_bug.cgi?id=1464406 .\n\nI report here because I confirmed that this problem remains using httpd-2.4.26.tar.bz2 .", "id": 199413, "time": "2017-06-26T03:19:42Z", "creator": "penguin-kernel@i-love.sakura.ne.jp", "creation_time": "2017-06-26T03:19:42Z", "is_private": false, "attachment_id": null}, {"count": 1, "tags": [], "text": "Ping? This bug can easily trigger out of memory denial of service.", "is_private": false, "bug_id": 61222, "id": 200743, "time": "2017-09-08T03:39:01Z", "creator": "penguin-kernel@i-love.sakura.ne.jp", "creation_time": "2017-09-08T03:39:01Z", "attachment_id": null}, {"count": 2, "tags": [], "bug_id": 61222, "attachment_id": null, "is_private": false, "id": 200804, "time": "2017-09-11T17:06:41Z", "creator": "jorton@redhat.com", "creation_time": "2017-09-11T17:06:41Z", "text": "I can't really understand how this has regressed, since we fixed this bug several times in the 2.0/2.2 era!\n\nap_content_length_filter seems completely wrong here, and does not try to limit the number of buckets it morphs into HEAP where e->length == -1.  With a fast enough CGI script where the CGI buckets can morph into HEAP as fast as you read them (i.e. you never hit EAGAIN), it'll try to consume as much RAM as it can.  I'm not sure how this ever worked."}, {"count": 3, "tags": [], "text": "Created attachment 35316\nrip out buffering in content length filter\n\nI feel like this code should work harder to justify its existence, or else we should rip it out.\n\nIf the server should buffer some output specifically to try to compute a C-L and avoid chunked output for streamy content generators (e.g. CGI), OK, there's surely an argument for doing that.  But clearly you need to think hard about limits, which was never done here and somehow we got away with it.\n\nTrying to push buffering into arbitrary filters is annoying and probably wrong unless we are carefully looking at performance trade-offs.\n\nAnyway, alternative is to rip out the buffering here, and fixes the bug.", "attachment_id": 35316, "id": 200805, "creation_time": "2017-09-11T17:21:22Z", "time": "2017-09-11T17:21:22Z", "creator": "jorton@redhat.com", "bug_id": 61222, "is_private": false}, {"count": 4, "tags": [], "creator": "rpluem@apache.org", "text": "(In reply to Joe Orton from comment #3)\n> Created attachment 35316 [details]\n> rip out buffering in content length filter\n> \n> I feel like this code should work harder to justify its existence, or else\n> we should rip it out.\n> \n> If the server should buffer some output specifically to try to compute a C-L\n> and avoid chunked output for streamy content generators (e.g. CGI), OK,\n> there's surely an argument for doing that.  But clearly you need to think\n> hard about limits, which was never done here and somehow we got away with it.\n> \n> Trying to push buffering into arbitrary filters is annoying and probably\n> wrong unless we are carefully looking at performance trade-offs.\n> \n> Anyway, alternative is to rip out the buffering here, and fixes the bug.\n\nUnderstand that you are upset and I am quite surprised how old this code is and that we never got hit by it. Just a comment to the patch: It breaks the stats of r->bytes_sent.\nHow about passing everything down the chain once we did an apr_bucket_read no matter if  it was APR_SUCCESS or or EAGAIN?", "id": 200812, "time": "2017-09-12T07:20:55Z", "bug_id": 61222, "creation_time": "2017-09-12T07:20:55Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 61222, "text": "Created attachment 35317\nSend all read data down the chain\n\nLike the following?", "id": 200813, "time": "2017-09-12T07:21:58Z", "creator": "rpluem@apache.org", "creation_time": "2017-09-12T07:21:58Z", "is_private": false, "attachment_id": 35317}, {"count": 6, "tags": [], "bug_id": 61222, "attachment_id": 35321, "is_private": false, "id": 200819, "time": "2017-09-12T10:29:17Z", "creator": "jorton@redhat.com", "creation_time": "2017-09-12T10:29:17Z", "text": "Created attachment 35321\nModified rpluem's patch with more FLUSH\n\nThanks a lot for looking at this Ruediger!  You're correct of course on r->bytes_sent, which I completely missed...\n\nSo I agree it makes sense to keep the loop, but I think we should keep the FLUSH bucket insertion for the EAGAIN case - or is there a good reason to remove it?  Hence I'd modify your patch slightly to the attached."}, {"count": 7, "tags": [], "bug_id": 61222, "text": "I also added the APR_ECONNABORTED logic here is which orthogonal to this change so could be committed separately.\n\n+                else if (f->c->aborted) {\n+                    return APR_ECONNABORTED;\n+                }", "id": 200820, "time": "2017-09-12T10:31:36Z", "creator": "jorton@redhat.com", "creation_time": "2017-09-12T10:31:36Z", "is_private": false, "attachment_id": null}, {"count": 8, "attachment_id": null, "creator": "ylavic.dev@gmail.com", "text": "(In reply to Ruediger Pluem from comment #5)\n> Created attachment 35317 [details]\n> Send all read data down the chain\n> \n> Like the following?\n\nLooks like we wouldn't pass the \"pipe\" bucket itself down the stack (eg. e == APR_BRIGADE_FIRST(b) and rv == EAGAIN), and then keep reading it until EOS in ap_content_length_filter(), or am I missing something?\nDo we really want C-L that much nowadays?\n\nIf we pass the \"pipe\" down the stack, I think the ssl or core output filter would do the the right thing already, but we need to make sure that ap_save_brigade() isn't called either or the unbounded heap buckets would still be there...", "id": 200821, "time": "2017-09-12T11:15:16Z", "bug_id": 61222, "creation_time": "2017-09-12T11:15:16Z", "tags": [], "is_private": false}, {"count": 9, "tags": [], "text": "(In reply to Yann Ylavic from comment #8)\n> Do we really want C-L that much nowadays?\n\nScratch this, the C-L is not the point whenever we pass anything before EOS.\nStill looks like we duplicate some logic belonging in ssl or core filters here, is it worth it?", "is_private": false, "bug_id": 61222, "id": 200822, "time": "2017-09-12T11:30:43Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-09-12T11:30:43Z", "attachment_id": null}, {"count": 10, "tags": [], "bug_id": 61222, "text": "(In reply to Yann Ylavic from comment #9)\n> (In reply to Yann Ylavic from comment #8)\n> > Do we really want C-L that much nowadays?\n> \n> Scratch this, the C-L is not the point whenever we pass anything before EOS.\n> Still looks like we duplicate some logic belonging in ssl or core filters\n> here, is it worth it?\n\nAgreed. We should find a better way / place to count the bytes stats, but for now this fixes an actual issue.", "id": 200823, "time": "2017-09-12T11:40:50Z", "creator": "rpluem@apache.org", "creation_time": "2017-09-12T11:40:50Z", "is_private": false, "attachment_id": null}, {"count": 11, "tags": [], "text": "Sure, I'm fine with the fix, let's work on this afterwards ;)", "is_private": false, "id": 200824, "creation_time": "2017-09-12T11:48:07Z", "time": "2017-09-12T11:48:07Z", "creator": "ylavic.dev@gmail.com", "bug_id": 61222, "attachment_id": null}, {"count": 12, "tags": [], "bug_id": 61222, "text": "(In reply to Joe Orton from comment #6)\n> Created attachment 35321 [details]\n> Modified rpluem's patch with more FLUSH\n> \n> Thanks a lot for looking at this Ruediger!  You're correct of course on\n> r->bytes_sent, which I completely missed...\n> \n> So I agree it makes sense to keep the loop, but I think we should keep the\n> FLUSH bucket insertion for the EAGAIN case - or is there a good reason to\n> remove it?  Hence I'd modify your patch slightly to the attached.\n\nMy initial thought on the flush was to remove it in order to give the downstream filters some freedom whether they need to flush or can buffer (at least parts) for whatever optimization, but now I agree that it makes sense to flush when we probably block next.\n\nBut looking at my own an your patch additional points come up:\n\n1. Don't we need to handle flush buckets and do flushing nevertheless what happens else?\n2. The if (e->length == (apr_size_t)-1)  seems stupid as it would also handle META_BUCKETS trying to read them. Shouldn't we just skip them in the loop (with the exception of the flush buckets above)?", "id": 200825, "time": "2017-09-12T11:53:04Z", "creator": "rpluem@apache.org", "creation_time": "2017-09-12T11:53:04Z", "is_private": false, "attachment_id": null}, {"count": 13, "tags": [], "creator": "jorton@redhat.com", "attachment_id": 35322, "text": "Created attachment 35322\niteration #4\n\nI think any complete content filter ends up with 99% the same code +/- ten different bugs. :(\n\nMetadata buckets *should* always be length=0 I think, though I'm not sure that's an API contract anywhere.  Anyway it's easy to side-step that.\n\nYou're definitely right we should handle FLUSH.  Another iteration here...", "id": 200826, "time": "2017-09-12T12:25:55Z", "bug_id": 61222, "creation_time": "2017-09-12T12:25:55Z", "is_private": false}, {"count": 14, "tags": [], "text": "Possible micro-optimisation here, if we get a brigade like:\n\n[CGI FLUSH EOR]\n\nand that CGI bucket transforms entirely into a HEAP on first read (i.e. a small fast CGI script) you then get\n\n[HEAP FLUSH EOR]\n\nand we should be able to do that with a single ap_pass_brigade() call, and still compute a C-L.", "attachment_id": null, "id": 200827, "creation_time": "2017-09-12T12:30:38Z", "time": "2017-09-12T12:30:38Z", "creator": "jorton@redhat.com", "bug_id": 61222, "is_private": false}, {"count": 15, "tags": [], "creator": "jorton@redhat.com", "attachment_id": null, "text": "Actually, scratch previous comment, that works already.\n\nap_scan_script_header* will have read from the CGI bucket already before we reach the C-L filter.  So if the output from the CGI script fits into 8K on first entry to the C-L filter we get a brigade like:\n\n[HEAP(response data) CGI EOS]\n\nThe CGI bucket morphs to a zero-length IMMORTAL when you read EOF from it, after passing through the loop twice the brigade is:\n\n[HEAP(response data) IMMORTAL(\"\") EOS]\n\n...and we fall through to the final ap_pass_brigade after fixing C-L.", "id": 200828, "time": "2017-09-12T14:22:37Z", "bug_id": 61222, "creation_time": "2017-09-12T14:22:37Z", "is_private": false}, {"count": 16, "tags": [], "bug_id": 61222, "text": "That's only true because I re-introduced the original bug in iteration #4 :(  Iteration #5 coming up...", "id": 200829, "attachment_id": null, "creator": "jorton@redhat.com", "creation_time": "2017-09-12T14:43:18Z", "time": "2017-09-12T14:43:18Z", "is_private": false}, {"count": 17, "tags": [], "text": "Created attachment 35323\niteration #5\n\nI rewrote the loop logic to be more explicit (I hope) in the handling of different bucket types, at least I can understand it properly now.\n\nThree specific cases I'm testing:\n\n1) a CGI script which writes the header, sleeps, writes the body -- must see headers hit the network before waiting to read the body\n\n2) the bug reported here -- must see no buffering\n\n3) for a small fast CGI script with <8K output -- does get C-L on response\n\n... plus running the test suite.\n\nI'm worried there are specific response brigades with FLUSH this is going to fail to produce C-L where we previously did, e.g. as simple as:\n\nHEAP FLUSH EOS\n\nwill produced chunked output now.\n\nI think we could safely handling this by moving the \"short-cut\" case for EOS right before the apr_brigade_split_ex call?", "attachment_id": 35323, "id": 200830, "creation_time": "2017-09-12T15:49:54Z", "time": "2017-09-12T15:49:54Z", "creator": "jorton@redhat.com", "bug_id": 61222, "is_private": false}, {"count": 18, "tags": [], "bug_id": 61222, "text": "(In reply to Joe Orton from comment #17)\n> Created attachment 35323 [details]\n> iteration #5\n> \n> I rewrote the loop logic to be more explicit (I hope) in the handling of\n> different bucket types, at least I can understand it properly now.\n> \n> Three specific cases I'm testing:\n> \n> 1) a CGI script which writes the header, sleeps, writes the body -- must see\n> headers hit the network before waiting to read the body\n> \n> 2) the bug reported here -- must see no buffering\n> \n> 3) for a small fast CGI script with <8K output -- does get C-L on response\n> \n> ... plus running the test suite.\n> \n> I'm worried there are specific response brigades with FLUSH this is going to\n> fail to produce C-L where we previously did, e.g. as simple as:\n> \n> HEAP FLUSH EOS\n> \n> will produced chunked output now.\n> \n> I think we could safely handling this by moving the \"short-cut\" case for EOS\n> right before the apr_brigade_split_ex call?\n\nAgreed. Cool patch.\n\nNitpick:\nI think we should call apr_brigade_cleanup(ctx->tmpbb) before leaving if the ap_pass_brigade fails in the loop ( (rv != APR_SUCCESS) /  (f->c->aborted)). This should free up memory faster.", "id": 200838, "time": "2017-09-13T06:10:59Z", "creator": "rpluem@apache.org", "creation_time": "2017-09-13T06:10:59Z", "is_private": false, "attachment_id": null}, {"count": 19, "tags": [], "creator": "jorton@redhat.com", "text": "I think the patch which removed 40 lines of code was cooler :(\n\nThe brigade cleanup is done for r->pool anyway, I dislike adding explicit cleanups when we can let the pools do their job!\n\nI committed this in r1808230.\n\nThere are definitely cases we could further optimise here.  e.g. where the entire brigade length is determinable, we could find C-L without an intermediate ap_pass_brigade.  [HEAP FLUSH HEAP EOS] is an example where we'll now chunk when we could determine C-L.\n\nI don't know if it's worth the effort to optimise that; arguably content generators which produce excessive FLUSH buckets should be treated as streamy producers.", "id": 200840, "attachment_id": null, "bug_id": 61222, "creation_time": "2017-09-13T11:04:59Z", "time": "2017-09-13T11:04:59Z", "is_private": false}, {"count": 20, "tags": [], "text": "(In reply to Joe Orton from comment #19)\n> I think the patch which removed 40 lines of code was cooler :(\n> \n> The brigade cleanup is done for r->pool anyway, I dislike adding explicit\n> cleanups when we can let the pools do their job!\n\nFair enough.\n\n> \n> I committed this in r1808230.\n> \n> There are definitely cases we could further optimise here.  e.g. where the\n> entire brigade length is determinable, we could find C-L without an\n> intermediate ap_pass_brigade.  [HEAP FLUSH HEAP EOS] is an example where\n> we'll now chunk when we could determine C-L.\n> \n> I don't know if it's worth the effort to optimise that; arguably content\n> generators which produce excessive FLUSH buckets should be treated as\n> streamy producers.\n\nI don't think that it is worth the effort. These generators should be considered streamy producers as you say. I also doubt that we see [HEAP FLUSH HEAP EOS] in practice. More likely we see [HEAP FLUSH] which resulted in chunked encoding so far as well.", "is_private": false, "id": 200841, "creation_time": "2017-09-13T11:21:06Z", "time": "2017-09-13T11:21:06Z", "creator": "rpluem@apache.org", "bug_id": 61222, "attachment_id": null}, {"count": 21, "tags": [], "bug_id": 61222, "text": "(In reply to Tetsuo Handa from comment #1)\n> Ping? This bug can easily trigger out of memory denial of service.\n\nCan you please try the latest patch and report back?", "id": 200842, "time": "2017-09-13T11:22:13Z", "creator": "rpluem@apache.org", "creation_time": "2017-09-13T11:22:13Z", "is_private": false, "attachment_id": null}, {"count": 22, "tags": [], "creator": "rpluem@apache.org", "is_private": false, "text": "(In reply to Ruediger Pluem from comment #21)\n> (In reply to Tetsuo Handa from comment #1)\n> > Ping? This bug can easily trigger out of memory denial of service.\n> \n> Can you please try the latest patch and report back?\n\nI mean the one from r1808230.", "id": 200843, "time": "2017-09-13T11:23:34Z", "bug_id": 61222, "creation_time": "2017-09-13T11:23:34Z", "attachment_id": null}, {"count": 23, "tags": [], "bug_id": 61222, "text": "(In reply to Ruediger Pluem from comment #22)\n> (In reply to Ruediger Pluem from comment #21)\n> > (In reply to Tetsuo Handa from comment #1)\n> > > Ping? This bug can easily trigger out of memory denial of service.\n> > \n> > Can you please try the latest patch and report back?\n> \n> I mean the one from r1808230.\n\nThe patch solves this bug. Thank you.\n\n\nMy client side program which calls the CGI programs depends on Content-Length:\nfield available in order to show progress bar, and I worried\n\n> but this DOES change behaviour and some responses will end up\n> chunked rather than C-L computed.\n\npart. But as far as I tested, it seems to me that the patch does not change\nbehavior and responses will not end up chunked as long as the CGI programs\nexplicitly emit Content-Length: field. Thus, I'm OK with the patch.", "id": 200861, "time": "2017-09-14T02:29:34Z", "creator": "penguin-kernel@i-love.sakura.ne.jp", "creation_time": "2017-09-14T02:29:34Z", "is_private": false, "attachment_id": null}, {"count": 24, "tags": [], "bug_id": 61222, "attachment_id": null, "is_private": false, "id": 200863, "time": "2017-09-14T07:00:56Z", "creator": "jorton@redhat.com", "creation_time": "2017-09-14T07:00:56Z", "text": "(In reply to Tetsuo Handa from comment #23)\n> But as far as I tested, it seems to me that the patch does not change\n> behavior and responses will not end up chunked as long as the CGI programs\n> explicitly emit Content-Length: field. Thus, I'm OK with the patch.\n\nYes, if C-L is set explicitly that will always get passed through to the client, this patch won't make any difference there.\n\nThanks a lot for testing it out!"}, {"count": 25, "tags": [], "bug_id": 61222, "text": "Merged to 2.4.x -- r1811746", "id": 201393, "attachment_id": null, "creator": "jorton@redhat.com", "creation_time": "2017-10-10T17:59:34Z", "time": "2017-10-10T17:59:34Z", "is_private": false}]