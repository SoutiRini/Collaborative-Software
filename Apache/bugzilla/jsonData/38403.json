[{"count": 0, "tags": [], "bug_id": 38403, "is_private": false, "text": "This bug is probably related to #36951, however the offered solution was for\n2.1.8 and not merged into 2.2.0.\n\nSetup: Apache 2.2.0 is used as a reverse proxy for accelerating an apache1.3+php\non the same host. we are using worker mdm and mod_proxy is configured to use a\npool. Apache is build from current source.\n\nProblem: At least once a day 1 or 2 threads consume 100% of their cpu (dual+ht,\nso system remains responsive). strace says that the process is waiting for the child\n\nfutex(0xb2b10bf8, FUTEX_WAIT, 1847, NULL <unfinished ...>\n\nwhile the child (1847) is in a loop of: \n\nclose(-1)                               = -1 EBADF (Bad file descriptor)\n\n(the other thread did the same)\n\nthe worker-thread and its parent process remain up, even if all other worker\nthreads of this process have been shutdown and apache already uses other\nprocesses to service requests.\n\nps -L -e -F\nUID        PID  PPID   LWP  C NLWP    SZ  RSS PSR STIME TTY          TIME CMD\n...\nnobody    1839 19001  1839  0    3 693301 235192 3 Jan25 ?       00:00:00\n/home/apache22/bin/httpd -k start\nnobody    1839 19001  1847 99    3 693301 235192 1 Jan25 ?       1-03:37:44\n/home/apache22/bin/httpd -k start\nnobody    1839 19001  1928 99    3 693301 235192 2 Jan25 ?       1-03:37:54\n/home/apache22/bin/httpd -k start\n... (these are all threads of PID 1839)\n\nI could not get a backtrace of the crazy thread, gdb hangs after this:\n\ngdb httpd 1928\n...\n[Thread debugging using libthread_db enabled]\n[New Thread -1209694528 (LWP 1839)]\n\nthe threads show up in extended server status:\nSrv\tPID\tAcc\tM\tCPU \tSS\tReq\tConn\tChild\tSlot\tClient\tVHost\tRequest\n0-0\t1839\t8/32/6705\tW \t13.39\t101250\t0\t19.5\t0.14\t40.40\tx.x.x.x\tfrontservername\tGET\n/proxied/url/...\n0-0\t1839\t30/117/6898\tR \t13.59\t101250\t0\t307.9\t0.60\t42.39 \t?\t?\n...\n\nthey survive gracefull restarts and apache restart kills them with SIGKILL\n\nI have the feeling that this bug is triggered by timeouts or partial responses\nof the accelerated server. (We had database problems with the backend server, so\na lot of proxy requests failed, see error_log, however there was no error\nmessage associated with the requested ressource, remote_addr or processids from\nthe extend status (probably because the threads did not yet get to log anything)\n\nproxy: error reading status line from remote server 127.0.0.1\nproxy: Error reading from remote server returned by\nproxy: error reading status line from remote server (null)\n(70007)The timeout specified has expired: proxy: prefetch request body failed to\n127.0.0.1:8081 (127.0.0.1) from \n\nWe log the x-forwarded-for header on the backend, and the ip from the server\nstatus does not show up, so the request headers were never received by the\nbackend server.", "id": 85063, "time": "2006-01-26T19:22:01Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-01-26T19:22:01Z", "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 38403, "attachment_id": null, "text": "> This bug is probably related to #36951, however the offered solution was for\n> 2.1.8 and not merged into 2.2.0.\nsorry wrong guess, \" rv=APR_EOF; \" is in 2.2.0, it's probably a similar thing on\nanother line ", "id": 85064, "time": "2006-01-26T19:30:00Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-01-26T19:30:00Z", "is_private": false}, {"count": 2, "tags": [], "text": "(In reply to comment #1)\n> sorry wrong guess, \" rv=APR_EOF; \" is in 2.2.0, it's probably a similar thing on\n> another line \nYES, about 35 lines above is the same situation as in #36951 only for the\nanother socket (i guess the one to the remote server). (this makes sense, as I\nhad a \"buggy\" backend server, while the other bug hat \"buggy\" client.\n\nI'm trying the patched version and get the info back here.\n\nIf this sounds like a good thing, can someone provide a patch and get this into\nCVS ? Sorry I'm not that good with this", "attachment_id": null, "bug_id": 38403, "id": 85065, "time": "2006-01-26T19:42:25Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-01-26T19:42:25Z", "is_private": false}, {"count": 3, "tags": [], "creator": "rpluem@apache.org", "text": "Could you please let me know where in the code (filename, line) you suppose the\nerror?\nPR #36951 is about mod_proxy_connect. I doubt that it is used in your reverse\nproxy configuration.\nRegarding the problem getting a backtrace:\nYou called gdb with the LWP id:\n\ngdb httpd 1928\n\nTry to call it with the PID of the LWP. That would be\n\ngdb httpd -p 1839\n\nHope that helps.\nFurther debugging tips can be found at:\nhttp://httpd.apache.org/dev/debugging.html", "id": 85070, "time": "2006-01-26T22:17:08Z", "bug_id": 38403, "creation_time": "2006-01-26T22:17:08Z", "is_private": false, "attachment_id": null}, {"count": 4, "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "id": 85072, "time": "2006-01-26T22:18:26Z", "bug_id": 38403, "creation_time": "2006-01-26T22:18:26Z", "is_private": false, "text": "Forgot one thing: Debugging can be problematic if you compiled your httpd with\n--enable-pie on Linux."}, {"text": "its build without pie, however i tried to build everything static .\n\nthx for the gdb hint, heres the output\n\n(gdb) info threads\n  3 Thread -1884591184 (LWP 8768)  0xb7e7f308 in allocator_free () from\n/home/apache22/lib/libapr-1.so.0\n  2 Thread -2052428880 (LWP 8785)  0x0807b039 in ap_core_input_filter ()\n  1 Thread -1209833792 (LWP 8702)  0xb7f89402 in ?? ()\n(gdb) thread 3\n[Switching to thread 3 (Thread -1884591184 (LWP 8768))]#0  0xb7e7f308 in\nallocator_free () from /home/apache22/lib/libapr-1.so.0\n(gdb) bt\n#0  0xb7e7f308 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n#1  0xb7e7f224 in apr_allocator_free () from /home/apache22/lib/libapr-1.so.0\n#2  0xb7f6a81f in apr_bucket_free () from /home/apache22/lib/libaprutil-1.so.0\n#3  0xb7f6b146 in heap_bucket_destroy () from /home/apache22/lib/libaprutil-1.so.0\n#4  0xb7f6b35c in apr_brigade_cleanup () from /home/apache22/lib/libaprutil-1.so.0\n#5  0xb7f6b3b1 in apr_brigade_destroy () from /home/apache22/lib/libaprutil-1.so.0\n#6  0x0806e810 in ap_getline ()\n#7  0x0809b7a0 in ap_proxy_http_process_response ()\n#8  0x0809c878 in proxy_http_handler ()\n#9  0x08095249 in proxy_run_scheme_handler ()\n#10 0x08092ab1 in proxy_handler ()\n#11 0x0807ce41 in ap_run_handler ()\n#12 0x0807d581 in ap_invoke_handler ()\n#13 0x080b4a66 in ap_process_request ()\n#14 0x080b1f7c in ap_process_http_connection ()\n#15 0x080842d8 in ap_run_process_connection ()\n#16 0x08084707 in ap_process_connection ()\n#17 0x080c2ecb in process_socket ()\n#18 0x080c36d4 in worker_thread ()\n#19 0xb7e8b868 in dummy_worker () from /home/apache22/lib/libapr-1.so.0\n#20 0x00bfb341 in start_thread () from /lib/tls/libpthread.so.0\n#21 0x00b136fe in clone () from /lib/tls/libc.so.6\n(gdb) thread 2\n[Switching to thread 2 (Thread -2052428880 (LWP 8785))]#0  0x0807b039 in\nap_core_input_filter ()\n(gdb) bt\n#0  0x0807b039 in ap_core_input_filter ()\n#1  0x08087957 in ap_get_brigade ()\n#2  0x0806e3c8 in ap_rgetline_core ()\n#3  0x0806e7ff in ap_getline ()\n#4  0x0809b4f9 in ap_proxy_read_headers ()\n#5  0x0809ba00 in ap_proxy_http_process_response ()\n#6  0x0809c878 in proxy_http_handler ()\n#7  0x08095249 in proxy_run_scheme_handler ()\n#8  0x08092ab1 in proxy_handler ()\n#9  0x0807ce41 in ap_run_handler ()\n#10 0x0807d581 in ap_invoke_handler ()\n#11 0x080b4a66 in ap_process_request ()\n#12 0x080b1f7c in ap_process_http_connection ()\n#13 0x080842d8 in ap_run_process_connection ()\n#14 0x08084707 in ap_process_connection ()\n#15 0x080c2ecb in process_socket ()\n#16 0x080c36d4 in worker_thread ()\n#17 0xb7e8b868 in dummy_worker () from /home/apache22/lib/libapr-1.so.0\n#18 0x00bfb341 in start_thread () from /lib/tls/libpthread.so.0\n#19 0x00b136fe in clone () from /lib/tls/libc.so.6\n(gdb) thread 1\n[Switching to thread 1 (Thread -1209833792 (LWP 8702))]#0  0xb7f89402 in ?? ()\n(gdb) bt\n#0  0xb7f89402 in ?? ()\n#1  0x00bfc06d in pthread_join () from /lib/tls/libpthread.so.0\n#2  0xb7e8ba3b in apr_thread_join () from /home/apache22/lib/libapr-1.so.0\n#3  0x080c3c89 in join_workers ()\n#4  0x080c4090 in child_main ()\n#5  0x080c419d in make_child ()\n#6  0x080c46c3 in perform_idle_server_maintenance ()\n#7  0x080c48e4 in server_main_loop ()\n#8  0x080c4bb8 in ap_mpm_run ()\n#9  0x080683aa in main ()\n\n\nI reattached the process a few times, the threads seem to do not leave their\nfunctions.\n\nstepi circles around this:\n\n0xb7e7f277 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f27a in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f27c in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f27f in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f282 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f285 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f288 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f28c in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2a6 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2aa in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2ac in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2af in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2b2 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2b5 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2b9 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2bb in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2bd in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2cd in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2d0 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2d3 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2d6 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2da in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2dd in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2e0 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f2e2 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f300 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f303 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f306 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f308 in allocator_free () from /home/apache22/lib/libapr-1.so.0\n0xb7e7f30a in allocator_free () from /home/apache22/lib/libapr-1.so.0", "tags": [], "bug_id": 38403, "attachment_id": null, "count": 5, "id": 85095, "time": "2006-01-27T12:36:49Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-01-27T12:36:49Z", "is_private": false}, {"count": 6, "tags": [], "text": "This was the proposed patch, but i guess you're right and this is not used in my\nsetup.\n\n\n*** mod_proxy_connect.c.myversion 2006-01-26 18:32:12.000000000 +0000\n--- mod_proxy_connect.c.orig    2006-01-27 11:32:08.000000000 +0000\n*************** static int proxy_connect_handler(request\n*** 325,334 ****\n                      else\n                          break;\n                  }\n!                 else if ((pollevent & APR_POLLERR) || (pollevent & APR_POLLHUP)) {\n!                   rv = APR_EOF;\n                      break;\n-               }\n              }\n              else if (cur->desc.s == client_socket) {\n                  pollevent = cur->rtnevents;\n--- 325,332 ----\n                      else\n                          break;\n                  }\n!                 else if ((pollevent & APR_POLLERR) || (pollevent & APR_POLLHUP))\n                      break;\n              }\n              else if (cur->desc.s == client_socket) {\n                  pollevent = cur->rtnevents;\n\n\n\n\nAnother interesting point:  \nI configured the server to use a pool for to the backend server:\n\n\nit limits the maximal connections to the backend server. this works normal, but\nif i have this amok threads it stops working. could be related to the behaviour\nof a graceful restart ..\n", "is_private": false, "bug_id": 38403, "id": 85097, "time": "2006-01-27T13:33:08Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-01-27T13:33:08Z", "attachment_id": null}, {"count": 7, "tags": [], "creator": "mk-asf@gigacodes.de", "attachment_id": null, "id": 85098, "time": "2006-01-27T13:37:04Z", "bug_id": 38403, "creation_time": "2006-01-27T13:37:04Z", "is_private": false, "text": "> it limits the maximal connections to the backend server. this works normal, but\n> if i have this amok threads it stops working. could be related to the behaviour\n> of a graceful restart ..\n> \n\nto clarify this is the pool:\nProxyPass           / http://127.0.0.1:8081/ min=1 smax=10 max=14 ttl=120\nacquire=10000 timeout=5 retry=2\n\nwenn starting multiple clients against a sleep.php there are no more than 14\nconenctions on the backend. with a amok-thread + possibly after a graceful\nrestart. there's no limit on the connections to the backend anymore. \n\nthis could be intended behaviour of graceful restart or it is related to the\nloop above which prevents apache from counting. \n\nI know that the limit is per process, so i double checked that there's only one\nprocess with 250 threads running in apache2.\n"}, {"text": "Please do not change the assignments of bugs.", "tags": [], "bug_id": 38403, "attachment_id": null, "count": 8, "id": 85143, "time": "2006-01-28T00:45:35Z", "creator": "rpluem@apache.org", "creation_time": "2006-01-28T00:45:35Z", "is_private": false}, {"text": "(In reply to comment #8)\n> Please do not change the assignments of bugs.\nI'm sorry, this info on the status link encouraged me:\n\"Once you provide this information, please reassign thebug back to the person\nthat placed it in the NEEDINFO status.\"\nhttp://issues.apache.org/bugzilla/page.cgi?id=fields.html#status", "tags": [], "bug_id": 38403, "is_private": false, "count": 9, "id": 85150, "time": "2006-01-28T13:25:17Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-01-28T13:25:17Z", "attachment_id": null}, {"count": 10, "tags": [], "bug_id": 38403, "is_private": false, "text": "To be sure that the thread is really looping in allocator_free, could you please\nrun ltrace against the process to see which library calls it does?", "id": 85171, "time": "2006-01-29T12:44:22Z", "creator": "rpluem@apache.org", "creation_time": "2006-01-29T12:44:22Z", "attachment_id": null}, {"count": 11, "attachment_id": null, "creator": "mk-asf@gigacodes.de", "is_private": false, "id": 85406, "time": "2006-02-04T12:02:43Z", "bug_id": 38403, "creation_time": "2006-02-04T12:02:43Z", "tags": [], "text": "we were unable to ltrace the processes (ltrace returns without error, and\nsometimes kills the amok process). we found out from looking at /proc, that it\nis not the allocator_free thread which consumes the cpu, but the\ncore_input_filter thread (telling from the SleepAVG in /proc/PID/status).\n\nwe recompiled with -ggdb and obtained more infos at the next occurence of the\nphenomenon. \n\n(gdb) info threads\n  3 Thread -2063254608 (LWP 9309)  allocator_free (allocator=0x195737d0,\nnode=0xa52eec0) at memory/unix/apr_pools.c:332\n  2 Thread 1801628592 (LWP 9350)  0x0807af5b in ap_core_input_filter\n(f=0x195740b8, b=0x9feddc8, mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0)\n    at core_filters.c:141\n  1 Thread -1210169664 (LWP 9228)  0xb7f09402 in __kernel_vsyscall ()\n\nI attach a complete backtrace to this bug. Bottom line is, that we are stuck on\nthis line: \n\nap_core_input_filter (f=0x195740b8, b=0x9feddc8, mode=AP_MODE_GETLINE,\n    block=APR_BLOCK_READ, readbytes=0) at core_filters.c:141\n141         BRIGADE_NORMALIZE(ctx->b);\n\n\ncode says this:\n\n    /* ### This is bad. */\n    BRIGADE_NORMALIZE(ctx->b);\n\n\nbut I'm not sure whether \"this is bad\" refers to a possible bug or just a\nperformance issue. \n\nIs there a simple way to inspect the BRIGADE in gdb ? I guess there's a special\nsituation with the brigade or its buckets, that arives from the fact that we run\nthe http-backend on localhost (probably caused by unusual/local tcp behaviour ?)"}, {"count": 12, "tags": [], "text": "Created attachment 17584\nfull gdb backtrace, (copied from shell)", "is_private": false, "bug_id": 38403, "id": 85407, "time": "2006-02-04T12:04:29Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-02-04T12:04:29Z", "attachment_id": 17584}, {"count": 13, "attachment_id": null, "creator": "rpluem@apache.org", "text": "(In reply to comment #11)\nThanks for the update.\n\n> \n> ap_core_input_filter (f=0x195740b8, b=0x9feddc8, mode=AP_MODE_GETLINE,\n>     block=APR_BLOCK_READ, readbytes=0) at core_filters.c:141\n> 141         BRIGADE_NORMALIZE(ctx->b);\n> \n> \n> code says this:\n> \n>     /* ### This is bad. */\n>     BRIGADE_NORMALIZE(ctx->b);\n> \n> \n> but I'm not sure whether \"this is bad\" refers to a possible bug or just a\n> performance issue. \n> \n> Is there a simple way to inspect the BRIGADE in gdb ? I guess there's a special\n\nYes, there is a helpful macro to inspect a brigade in gdb (dump_brigade). Please\nhave a look at the end of the section of\nhttp://httpd.apache.org/dev/debugging.html#gdb\n\nBRIGADE_NORMALIZE is a macro that does a loop. May it would be helpful to replace\n\nBRIGADE_NORMALIZE(ctx->b);\n\nwith the expanded macro to see if the loop is never left.\n\ndo { \n    apr_bucket *e = APR_BRIGADE_FIRST(ctx->b); \n    do {  \n        if (e->length == 0 && !APR_BUCKET_IS_METADATA(e)) { \n            apr_bucket *d; \n            d = APR_BUCKET_NEXT(e); \n            apr_bucket_delete(e); \n            e = d; \n        } \n        else { \n            e = APR_BUCKET_NEXT(e); \n        } \n    } while (!APR_BRIGADE_EMPTY(ctx->b) && (e != APR_BRIGADE_SENTINEL(ctx->b))); \n} while (0)\n\nFurthermore it would be better to use step / next instead of stepi to check\nwhere the code circles. stepi only executes one assembler instruction, whereas\nstep executes one line of C code.\n\nIn your initial comment to this bug you mentioned that a thread is looping with\nclose(-1). Would it be possible to get a backtrace of this thread?\n\nSo I would recommend the following next steps:\n\n1. Expand the BRIGADE_NORMALIZE macro and recompile.\n2. If the error occurs again do a dump_brigade ctx->b and attach the output\n3. Check with if the BRIGADE_NORMALIZE loop is left. For this set a breakpoint\nafter the loop and cont. If you reach it the loop is left if not we circle in\nthe loop.\n4. Try to get a backtrace of the thread that loops with close(-1)", "id": 85408, "time": "2006-02-04T12:44:16Z", "bug_id": 38403, "creation_time": "2006-02-04T12:44:16Z", "tags": [], "is_private": false}, {"count": 14, "tags": [], "creator": "mk-asf@gigacodes.de", "attachment_id": null, "is_private": false, "id": 85439, "time": "2006-02-05T19:00:36Z", "bug_id": 38403, "creation_time": "2006-02-05T19:00:36Z", "text": "short update: I'm still trying to hunt it down. here's a short summary:\n\nCurrently not reproducable at will, but steadily recurring about once a day. \n\nAs long as I did close looks at it, there are always 2 threads with similar\nstack trace ( ap_proxy_http_process_response, one ap_proxy_read_headers, the\nother just ap_getline ). While one is cleaning up, the other is probably stuck\nin BRIGADE_NORMALIZE (which also does some cleanup). Sounds like a race\ncondition where 2 threads are cleaning up the same brigade. (I will try to get\nsome info on the brigade that the other thread tries to cleanup, next time). \n\nstep/next did not return the last time, so the MACRO line was never left. \n\nthe strace close(-1) threads does only show up 1 in 10 times. Either its a\ndifferent bug or it is about the repeated cleanup/normalization of a\nsocket-bucket. Most of the times the threads produce no system/library calls, so\nI guess they are caught in a very small loop."}, {"count": 15, "tags": [], "bug_id": 38403, "is_private": false, "text": "We probably found the root of evil:\n\na graceful restart does destroy but not reinitialize the resource list that\nkeeps the pooled connections. this is due to a early return in\nap_proxy_initialize_worker (proxy_util.c) when called again. Our patch is not\nnice, but worked for us. (restart instead of graceful did it also as workaround)\n\n\n--- proxy_util.c        2006-02-06 13:59:35.000000000 +0000\n+++ proxy_util.c.orig   2006-02-06 17:48:39.000000000 +0000\n@@ -1629,7 +1630,7 @@\n     int mpm_threads;\n #endif\n\n-    if (worker->s->status & PROXY_WORKER_INITIALIZED && worker->cp->res) {\n+    if (worker->s->status & PROXY_WORKER_INITIALIZED) {\n         /* The worker is already initialized */\n         return APR_SUCCESS;\n     }\n\n\nWe noticed this when we tried to force the 99% cpu threads by stressing the\nwebserver with ab ( -k -c 20 ). after a graceful restart the server was likely\nto coredump at \n\nproxy_util.c:1758 (*conn)->worker = worker;\n\nbecause conn was null. it seems that conn is deleted be another thread.\nconditional breakpoints above proxy_util.c:1745 (rv = APR_SUCCESS;) never\nspotted a conn = 0.\n\nbefore the graceful we run through this condition (no problems) but after\ngracefull res is null.\n\n    if (worker->hmax && worker->cp->res) {\n        rv = apr_reslist_acquire(worker->cp->res, (void **)conn);\n    }\n\nthis change in flow introduces the concurrency problems. is the else branch\nwithout the pool threadsafe or only intended for prefork ? \n\nSome facts that would speek for the graceful theory:\na) 99% threads only appeared on servers in the farm that used graceful and not\nrestart\nb) they only appeared after a graceful, never directly after startup\nc) after a graceful, there was no limit on the number of backend connections\n\n\non the setup: We use Dual Xeons with Hyperthreading (so there might be higher\nconcurrency) and we compiled with --enable-nonportable-atomics", "id": 85478, "time": "2006-02-06T19:19:22Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-02-06T19:19:22Z", "attachment_id": null}, {"count": 16, "tags": [], "bug_id": 38403, "is_private": false, "text": "Thanks for the update and the in depth analysis. From my first brief into this I\nagree that the resource list needs to be initialized in these cases.\nBut it seems also that your patch as you assumed isn't complete. From a first\nbrief view I think more fixes are required.\nAs I am still wondering whether the problem that is fixed by your patch is\nreally the root cause of the problem you reported I would like you to keep me\nupdated if the problem is gone with your patched httpd's.\nAnyway: Good work.", "id": 85564, "time": "2006-02-07T21:16:28Z", "creator": "rpluem@apache.org", "creation_time": "2006-02-07T21:16:28Z", "attachment_id": null}, {"text": "Feedback: we are running almost a week now with the patched version. So far this\nserver remains free of problems, except for 2 seg faults in 5 days. (so there's\nmore to do as you said)\n\nOn the other servers in the farm I got one 99% percent process in the last 5\ndays and a whole bunch of segfaults. \n\nThey are all running non-patched non-debug httpds and are running on same\nhardware, kernel, config etc. except that they do a restart instead of graceful,\nwhich has helped so far (i thought). So if \"restart\" always runs the if-branch\nthat creates the pool, than you're right and it was not the root of OUR evil).\n\nI also have to modify my statement that 99er only appear on graceful restarting\napaches, they are just more likely to be seen there and to survive long enough\nto be analyzed.\n\nWe will do more testing with patched versions next week and see if we can get\nmore insight. I'll keep you updated.", "tags": [], "bug_id": 38403, "attachment_id": null, "count": 17, "id": 85705, "time": "2006-02-11T18:21:58Z", "creator": "mk-asf@gigacodes.de", "creation_time": "2006-02-11T18:21:58Z", "is_private": false}, {"count": 18, "tags": [], "creator": "rpluem@apache.org", "text": "Meanwhile patches have been created on trunk that are proposed for backport:\n\nhttp://svn.apache.org/viewcvs?rev=377738&view=rev\nhttp://svn.apache.org/viewcvs?rev=377780&view=rev\n", "id": 87491, "time": "2006-04-01T09:06:26Z", "bug_id": 38403, "creation_time": "2006-04-01T09:06:26Z", "is_private": false, "attachment_id": null}, {"count": 19, "tags": [], "bug_id": 38403, "attachment_id": null, "id": 87506, "time": "2006-04-01T22:15:58Z", "creator": "rpluem@apache.org", "creation_time": "2006-04-01T22:15:58Z", "is_private": false, "text": "Backported to 2.2.1."}]