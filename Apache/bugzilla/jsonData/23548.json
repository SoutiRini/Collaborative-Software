[{"count": 0, "tags": [], "bug_id": 23548, "text": "We are running apache 2.0.47 in prefork model on solaris 2.6 and solaris 2.8. We are using the \nldap authenticator which is still in experimental mode using the NETSCAPE C libraries (without \nSSL).\n\nAuthentication to ldap using .htaccess files occurs flawlessly.  Each apache child holds open a \nconnection to ldap on port 389 for the specified length of time.  However, when that time expires \nor the port 389 connection is closed, the apache child does not reap the connection and it remains \nstuck in close_wait.\n\nEventually Apache reaches the file descriptor limit (set to 64) and stop accepting new connections.  \nWe don't want to raise the FD limit we would like to get to the root cause.  Any ideas?\n\nWe do not see this on solaris 2.8!\n\nAPPENDIX (error log , truss, lsof output from parent and children).\n========\nERROR LOG INDICATES THE SYMPTOM:\n\nTue Sep 30 18:40:48 2003] [error] [client 10.0.0.2] (24)Too many open files\n\nTRUSS of parent seems normal:\n\n15598:  poll(0xEFFFDA90, 0, 1000)                       = 0\n15598:  waitid(7, 0, 0xEFFFF9A0, 0107)                  = 0\n15598:        siginfo: SIG#0\n15598:  poll(0xEFFFDA90, 0, 1000)                       = 0\n15598:  waitid(7, 0, 0xEFFFF9A0, 0107)                  = 0\n15598:        siginfo: SIG#0\n15598:  poll(0xEFFFDA90, 0, 1000)                       = 0\n15598:  waitid(7, 0, 0xEFFFF9A0, 0107)                  = 0\n15598:        siginfo: SIG#0\n(SO ON AND SO FORTH)\n\nLSOF OF A CHILD SHOWS THE STUCK CLOSE_WAITS:\nCOMMAND  PID     USER   FD   TYPE     DEVICE SIZE/OFF      NODE NAME\napache2047   5435 www    9u  inet 0x90c4c9d8     0t37       TCP solars2.6:80->eam-\nsj2.cisco.com:37648 (CLOSE_WAIT)\napache2047   5435 www   10u  inet 0x9278db10    0t427       TCP solars2.6:50616->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   11u  inet 0xad5a3958    0t386       TCP solars2.6:50627->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   12u  inet 0x83b41ec8   0t1612       TCP solars2.6:50630->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   13u  inet 0x7e1cf160    0t744       TCP solars2.6:50666->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   14u  inet 0x7231f7c8   0t3507       TCP solars2.6:50670->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   15u  inet 0xad5a2f58    0t361       TCP solars2.6:51412->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   16u  inet 0x76985e38  0t12054       TCP solars2.6:35189->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   17u  inet 0x9278aa88    0t333       TCP solars2.6:37367->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   18u  inet 0x7d6771c0   0t5455       TCP solars2.6:40510->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   19u  inet 0x81a710f8   0t3548       TCP solars2.6:41520->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   20u  inet 0x7e1cfc60   0t6510       TCP solars2.6:41536->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   21u  inet 0x90c74a78    0t732       TCP solars2.6:43119->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   22u  inet 0x71cee450   0t6255       TCP solars2.6:52432->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   23u  inet 0x759de7d0    0t366       TCP solars2.6:53967->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   24u  inet 0x839526b8   0t3942       TCP solars2.6:56627->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   25u  inet 0x83b435d0    0t366       TCP solars2.6:57230->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   26u  inet 0x9c7aaeb8    0t422       TCP solars2.6:57249->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   27u  inet 0x72b4bd58    0t366       TCP solars2.6:57250->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   28u  inet 0x83b40548    0t821       TCP solars2.6:57267->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   29u  inet 0x9278cd90    0t366       TCP solars2.6:57348->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   30u  inet 0x83861140   0t8091       TCP solars2.6:57513->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   31u  inet 0x9c79aeb0    0t380       TCP solars2.6:58749->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   32u  inet 0x83950730   0t4662       TCP solars2.6:58776->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   33u  inet 0x81a70ff8    0t380       TCP solars2.6:59546->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   34u  inet 0x7072d540    0t810       TCP solars2.6:59555->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   35u  inet 0x9278a388    0t366       TCP solars2.6:59559->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   36u  inet 0x83860440    0t403       TCP solars2.6:59576->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   37u  inet 0xad6ab4e8    0t380       TCP solars2.6:59640->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   38u  inet 0xad6aaf68   0t3142       TCP solars2.6:59643->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   39u  inet 0x7dca54f8   0t1084       TCP solars2.6:60061->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   40u  inet 0x96072850    0t817       TCP solars2.6:60070->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   41u  inet 0x7e1cf560    0t380       TCP solars2.6:60141->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   42u  inet 0x83b42fd0   0t1589       TCP solars2.6:60159->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   43u  inet 0x7dca8a88    0t380       TCP solars2.6:60375->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   44u  inet 0x9c799310   0t3202       TCP solars2.6:60385->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   45u  inet 0xad67ba60    0t732       TCP solars2.6:61203->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   46u  inet 0x83950f30   0t2381       TCP solars2.6:61243->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   47u  inet 0xad6b2af0    0t746       TCP solars2.6:61359->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   48u  inet 0x7d6775c0   0t4131       TCP solars2.6:61380->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   49u  inet 0x90c4d9d8    0t386       TCP solars2.6:61741->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   50u  inet 0x90c74278  0t14254       TCP solars2.6:61756->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   51u  inet 0x9278c410   0t2164       TCP solars2.6:63803->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   52u  inet 0x7072c0c0    0t420       TCP solars2.6:63816->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   53u  inet 0x9c797188   0t1096       TCP solars2.6:63822->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   54u  inet 0x769844b8    0t420       TCP solars2.6:63843->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   55u  inet 0x7e1d5de8    0t384       TCP solars2.6:63865->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   56u  inet 0x7e1d54e8   0t3558       TCP solars2.6:64149->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   57u  inet 0xad6b2bf0    0t646       TCP solars2.6:64641->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   58u  inet 0x83950230   0t4721       TCP solars2.6:65486->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   59u  inet 0x7231e748    0t366       TCP solars2.6:32822->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   60u  inet 0x90c75578    0t427       TCP solars2.6:35352->netscape-\nldap:389 (CLOSE_WAIT)\napache2047   5435 www   61r  VREG  138,99002        0     16128 /fs/apache2047 (/dev/vx/dsk/\nccodg/apache2047vol)\napache2047   5435 www   62u  FIFO 0x76dff700      0t0   5476979 PIPE->0x76dff784", "id": 44960, "attachment_id": null, "creator": "redwoodtree@mac.com", "creation_time": "2003-10-01T16:46:53Z", "time": "2003-10-01T16:46:53Z", "is_private": false}, {"count": 1, "tags": [], "bug_id": 23548, "text": "See comments/patches on bug 27134\n\nWe're seeing a similar problem under Red Hat 9.0, though I'm mainly testing\nversions patched for a different problem.\n\n", "id": 56416, "time": "2004-04-26T16:04:02Z", "creator": "atlunde@panix.com", "creation_time": "2004-04-26T16:04:02Z", "is_private": false, "attachment_id": null}, {"count": 2, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "Are you using a shared LDAP connection pool?\n\nThe mod_ldap connection pool will keep the connections open until the process is\nshut down, when it will gracefully close them. This should happen when your\nprocess reaches it's maximum number of allowed requests, and will shut down to\nbe replaced by a new one.\n\nIf you're using a shared pool though, the connections will only be shut down\nwhen httpd receives a graceful restart, which I can see could be a problem if\nhttpd is long lived.\n", "id": 57822, "time": "2004-05-21T15:49:47Z", "creator": "minfrin@sharp.fm", "creation_time": "2004-05-21T15:49:47Z", "is_private": false}, {"count": 3, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "As I understand it, the Shared Connection Pool is on by default.  You bring up a\ngood point though. Maybe it's a matter of adjusting this shared pool inside the\nsource code.  I don't see anyway to do that from a config line.  The\ndocumentation simply says it's on by default.  It's still curious that we only\nsee this under solaris 2.6 (and perhaps another person sees it under redhat). \nAlthough I can see how this could simply be a mask of the problem perhaps (we\nsee it earlier on 2.6 and our 2.8 environments doesn't get similar traffic\npatterns?).   \n\nAnyway... THANKS! I will look into the code from this angle, sometimes it just\ntakes another pair of eyes.\n\nLDAP Connection Pool\n\nLDAP connections are pooled from request to request. This allows the LDAP server\nto remain connected and bound ready for the next request, without the need to\nunbind/connect/rebind. The performance advantages are similar to the effect of\nHTTP keepalives.\n\nOn a busy server it is possible that many requests will try and access the same\nLDAP server connection simultaneously. Where an LDAP connection is in use,\nApache will create a new connection alongside the original one. This ensures\nthat the connection pool does not become a bottleneck.\n\nThere is no need to manually enable connection pooling in the Apache\nconfiguration. Any module using this module for access to LDAP services will\nshare the connection pool.\n", "id": 57833, "time": "2004-05-21T16:16:46Z", "creator": "redwoodtree@mac.com", "creation_time": "2004-05-21T16:16:46Z", "is_private": false}, {"count": 4, "tags": [], "bug_id": 23548, "text": "Hmmm... in that case the fix would be to include a timeout on the LDAP\nconnections in the cache, so that those unused after a vertain age will be\nclosed down and removed from the pool.\n\nCurrently this is done via an apr cleanup call when the memory pool is cleaned\nup, but this only happens when a graceful restart happens. Will chase the apr\npeople to see if there is an elegant way of timing out these connections.\n", "id": 57835, "attachment_id": null, "creator": "minfrin@sharp.fm", "creation_time": "2004-05-21T16:26:23Z", "time": "2004-05-21T16:26:23Z", "is_private": false}, {"count": 5, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "One further thing: Can you try the v2.1.0-dev CVS version of mod_ldap and\nmod_auth_ldap and tell me if it makes a difference (you can just drop the *ldap*\nfiles into c2.0.49 and it should work, or at least it does for me).\n\nThe newer code is a lot more robust on handling errors (such as \"server has\ntimed out\") - connections are gracefully closed and brought to a sane state\nafter an error condition instead of being abandoned as before.\n\nThis might solve your problem.\n", "id": 57836, "time": "2004-05-21T16:34:46Z", "creator": "minfrin@sharp.fm", "creation_time": "2004-05-21T16:34:46Z", "is_private": false}, {"count": 6, "tags": [], "creator": "redwoodtree@mac.com", "attachment_id": null, "id": 57840, "time": "2004-05-21T16:50:32Z", "bug_id": 23548, "creation_time": "2004-05-21T16:50:32Z", "is_private": false, "text": "I love bug cleanup days.  Thanks for the heads up on the new version of\nmod_ldap.  I will try it and update the case and update this case. It may take\nseveral days to play out an accurate test. thanks once again!"}, {"count": 7, "tags": [], "bug_id": 23548, "text": "All the LDAP fixes just hit v2.0.50-dev - if you could give this version a try\nand tell me whether it fixes this problem, I can close this bug :)\n", "id": 57886, "attachment_id": null, "creator": "minfrin@sharp.fm", "creation_time": "2004-05-21T23:58:18Z", "time": "2004-05-21T23:58:18Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "the \"right\" way to fix this is to use apr-util's reslist functions, which handle\nthings like resource timeouts for you.\n", "id": 58072, "time": "2004-05-25T18:02:21Z", "creator": "minfrin@sharp.fm", "creation_time": "2004-05-25T18:02:21Z", "is_private": false}, {"count": 9, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "Having the apache ldap connection cache hang onto connections too long is a bad\nthing for the ldap server, even if connections are left in a sane state. An\nexplict, configurable timeout (or other resource-use controls) would be a good\nthing.\n", "id": 60264, "time": "2004-07-07T21:05:57Z", "creator": "atlunde@panix.com", "creation_time": "2004-07-07T21:05:57Z", "is_private": false}, {"count": 10, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "Can someone confirm whether this bug is still present in either HEAD or v2.0.52?", "id": 64526, "time": "2004-10-03T16:15:20Z", "creator": "minfrin@apache.org", "creation_time": "2004-10-03T16:15:20Z", "is_private": false}, {"count": 11, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "I have been testing this extensively on RedHat AS 3.0 (Taroon Update 1) with\nkernel 2.6.7 and I still see it happening using Apache 2.0.52.  I can duplicate\nit easily by setting the cache TTLs to 30 seconds and my OpenLDAP server\nidletimeout to 60 seconds.  Simply login to the server, refresh the page once or\ntwice, then wait 60 seconds.  For some reason I have to do this 5 times for it\nto \"break\".  If I do a netstat -anp on the web server machine I see 5 LDAP\nsessions in CLOSE_WAIT state attached to httpd children.  I should also mention\nthat this only happens with LDAPS connections.  If I switch my URL to LDAP the\nCLOSE_WAIT session is either reaped or reused.  It does go into CLOSE_WAIT, but\nit gets \"fixed\".  I hope this information was helpful.  Please contact\ntodd@mtu.edu with further questions.\n\nBy the way, shutting off the cache causes the server's children to seg fault\ncontinuously.\n\nTodd Piket (todd@mtu.edu)\nAnalyst/Programmer\nDistributed Computing Services\nMichigan Technological University", "id": 65160, "time": "2004-10-14T21:11:15Z", "creator": "todd@mtu.edu", "creation_time": "2004-10-14T21:11:15Z", "is_private": false}, {"count": 12, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "Could you run a test of both HEAD (httpd v2.1) and the most recent CVS of\nAPACHE_2_0_BRANCH and tell me if there is any difference - another whole lot of\npatches just went in.\n\nA fix went in to stop the segfault when the cache was switched off, you should\nno longer see the segfaults in the above latest versions.\n", "id": 65247, "time": "2004-10-17T18:32:36Z", "creator": "minfrin@apache.org", "creation_time": "2004-10-17T18:32:36Z", "is_private": false}, {"count": 13, "tags": [], "bug_id": 23548, "text": "I've checked out 2.1-HEAD, I think.  That was the first time I've ever checked\nApache out of CVS.  Not the first time for CVS though by any means so I think I\ngot it right.  If I did get it right then the problem appears to be fixed, in a\nmanner of speaking, for LDAPS.  The CLOSE_WAITs still appear and still 5 times\nbefore anything happens, but the sixth one causes one of the other 5 to get\nreaped.  I'm not sure how, but I know it does because the port number of the\nspeaking client changes.  This is similar to the behavior for an unencrypted\nLDAP session except LDAP only gets one CLOSE_WAIT socket.  I've included a\nnetstat -anp dialog for LDAPS below as well as my, mostly original, .htaccess\nfile for generating the errors.  The syntax changed quite a bit with the new\nAuthBasicProvider directive and the \"ldap-\" prefix for the requires.  It took me\na while to figure that out, but the gist of the .htaccess file is the same.\n\nI haven't checked APACHE_2_0_BRANCH yet, but will try to by the end of my day\ntomorrow because I'll be out of town for the rest of the week.  Please let me\nknow if there is anything more I can do.  I may try to check them both on\nSolaris as well, but my main platform is RedHat AS3.\n\nOh, the seg fault issue appears to be resolved as well in httpd-2.1-HEAD.\n\n\nMy .htaccess file (names changed to protect the innocent):\n----------------------------------------------------------\nAuthType Basic\nAuthBasicProvider ldap\nAuthName \"AuthLDAP WSS\"\nAuthLDAPURL\nldaps://ldapmaster.mtu.edu:636/dc=mtu,dc=edu?uid?sub?(objectclass=posixac\ncount)\nAuthLDAPBindDN \"uid=somedude,ou=somewhere,dc=mtu,dc=edu\"\nAuthLDAPBindPassword secret\nAuthLDAPGroupAttribute uniquemember\nrequire ldap-group cn=My Group,ou=Groups,dc=mtu,dc=edu\nrequire ldap-user user1\nrequire ldap-user user2\n\n\nThe netstat -anp dialog\nWatch the Local Address\nport numbers.\n-----------------------\nSome CLOSE_WAIT, Some ESTABLISHED\n=================================\ntcp        0      0 192.168.104.4:35889     141.219.70.115:636      ESTABLISHED \n13228/httpd         \ntcp        0      0 192.168.104.4:35888     141.219.70.115:636      ESTABLISHED \n13230/httpd         \ntcp        1      0 192.168.104.4:35876     141.219.70.115:636      CLOSE_WAIT  \n13227/httpd         \ntcp        0      0 192.168.104.4:35885     141.219.70.115:636      ESTABLISHED \n13235/httpd         \ntcp        1      0 192.168.104.4:35884     141.219.70.115:636      CLOSE_WAIT  \n13229/httpd         \n\nALL CLOSE_WAIT\n==============\ntcp        1      0 192.168.104.4:35889     141.219.70.115:636      CLOSE_WAIT  \n13228/httpd         \ntcp        1      0 192.168.104.4:35888     141.219.70.115:636      CLOSE_WAIT  \n13230/httpd         \ntcp        1      0 192.168.104.4:35876     141.219.70.115:636      CLOSE_WAIT  \n13227/httpd         \ntcp        1      0 192.168.104.4:35885     141.219.70.115:636      CLOSE_WAIT  \n13235/httpd         \ntcp        1      0 192.168.104.4:35884     141.219.70.115:636      CLOSE_WAIT  \n13229/httpd         \n\n\nNew/Reaped Conn after all CLOSE_WAIT\n====================================\ntcp        1      0 192.168.104.4:35889     141.219.70.115:636      CLOSE_WAIT  \n13228/httpd         \ntcp        1      0 192.168.104.4:35888     141.219.70.115:636      CLOSE_WAIT  \n13230/httpd         \n*tcp        0      0 192.168.104.4:35891     141.219.70.115:636      ESTABLISHED \n13229/httpd         \ntcp        1      0 192.168.104.4:35876     141.219.70.115:636      CLOSE_WAIT  \n13227/httpd         \ntcp        1      0 192.168.104.4:35885     141.219.70.115:636      CLOSE_WAIT  \n13235/httpd         \n\n* This one reaped 35884.\n\n\nHope that helps,\n\nTodd Piket (todd@mtu.edu)\nAnalyst/Programmer\nDistributed Computing Services\nMichigan Technological University", "id": 65321, "time": "2004-10-19T15:17:33Z", "creator": "todd@mtu.edu", "creation_time": "2004-10-19T15:17:33Z", "is_private": false, "attachment_id": null}, {"count": 14, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "Ok this is starting to make sense. In the original LDAP code, the module didn't\nclean up after itself properly when an LDAP error occurred (most common error\nbeing a timeout). Instead of trying to shut down broken LDAP connections still\nin the pool, it would ignore them, resulting in the increasing list of CLOSE_WAITs.\n\nNow, if the module tries to reuse a connection in the pool, but this connection\nhas timed out, it cleanly tries to close the old connection and reopens a new\none - this is probably why you're seeing the CLOSE_WAITs from each connection in\nthe pool until they are reaped. This isn't ideal, however it is no longer\nunbounded like before.\n\nThe correct solution to all of this is to implement the LDAP connection pool\nusing apr_reslist_*, which handles things like timing out connections and\ncleanly removing them from the pool for us. The catch is that apr_reslist_* is\napparently only available in APR v1.0, which limits this to being properly\nimplemented in httpd v2.1/v2.2 (which uses APR v1.0), and not httpd v2.0 (which\nuses APR v0.9).\n\nI'm keen to see if the CLOSE_WAITs remain within the limits of the size of the\npool in the httpd v2.0 branch, which hasn't been overhauled to the same extent\nas the HEAD branch has. If so, then the bug can probably be marked as fixed\n(mostly) until apr_reslist_* is properly implemented.\n", "id": 65325, "time": "2004-10-19T16:34:42Z", "creator": "minfrin@apache.org", "creation_time": "2004-10-19T16:34:42Z", "is_private": false}, {"count": 15, "tags": [], "bug_id": 23548, "text": "FYI, under my RedHat AS3 kernel 2.6.7 test machine the latest CVS extract,\nhttpd-2.0.53-dev, is still \"broken\".  The CLOSE_WAITs will pile up until there\nare no more fds.  Sorry to bring the bad news, but that's what I'm seeing. \nHopefully the 2.1/2.2 build will be released soon.", "id": 65364, "attachment_id": null, "creator": "todd@mtu.edu", "creation_time": "2004-10-20T15:27:39Z", "time": "2004-10-20T15:27:39Z", "is_private": false}, {"count": 16, "tags": [], "bug_id": 23548, "text": "Is it possible to test this on httpd v2.1 and see if this problem is still there?\n\nDevelopment of httpd v2.0's LDAP has been abandoned as APR v0.9 needed an\noverhaul that was better done in APR v1.1 and httpd v2.1.\n", "id": 69946, "attachment_id": null, "creator": "minfrin@apache.org", "creation_time": "2005-01-21T22:42:43Z", "time": "2005-01-21T22:42:43Z", "is_private": false}, {"count": 17, "tags": [], "bug_id": 23548, "attachment_id": null, "text": "It looks like this patch might go some way towards solving the problem.\n\nThe authn and authz phases have been separated entirely from each other in httpd\nv2.1, so using the user's bind details to search for the group is non trivial\nexercise. But at least the patch allows authn to work binding as the user directly.\n\n\n*** This bug has been marked as a duplicate of 31352 ***", "id": 69959, "time": "2005-01-21T23:22:57Z", "creator": "minfrin@apache.org", "creation_time": "2005-01-21T23:22:57Z", "is_private": false}]