[{"count": 0, "tags": [], "creator": "carnold@apache.org", "text": "On Oct 30, 2007, at 2:16 PM on log4j-user, Jessica Lin wrote:\n\nI want to use Chainsaw to view the log file contains Chinese character. The log file was recorded by \nusing FileAppender which I defined the endoding as \u201cUTF-8\u201d. Here is part of my log4j.properties file.\n\n\n# xml format file appender\nlog4j.appender.xml=org.apache.log4j.FileAppender\nlog4j.appender.xml.file=xml.log\nlog4j.appender.xml.encoding=UTF-8\nlog4j.appender.xml.append=false\nlog4j.appender.xml.layout=org.apache.log4j.xml.XMLLayout\n\nThen  I use Chainsaw to load \u201cxml.log\u201d file. The Chinese characters are shown as \u201c \u00e5\u0160 \u00e8\u00bf\u2122\u00e4\u00b8\u00aa\u00e5\u0160\u0178\u00e8\u0192\u00bd\u201d. \nThe Original characters are \u201c?????\u201d. \n\nI double checked the \u201cxml.log\u201d which did save as UTF-8 encoding. The XMLDecoder file Which \nChainsaw uses to load XML file also use UTF-8 encoding.\n\nCan you help me?\n\nThanks,\n\nJessica\n\n\n---------\n\nThe problem appears to be in o.a.l.xml.XMLDecoder in the receivers companion where at line 186 and \n188, InputStreamReaders are allocated without explicitly specifying an encoding.  That will cause the \nInputStreamReader to use the default platform encoding which appears not be to UTF-8 in this \ninstance.\n\nThe approach is broken and needs to be rewritten to handle any arbitrary encoding.  The XML parser \nshould be presented with a minimal document like:\n\n<!DOCTYPE log4j:eventSet [\n<!ENTITY content SYSTEM \"...\">\n]>\n<log4j:eventSet version=\"1.2\" xmlns:log4j=\"...\">\n    &content;\n</log4:eventSet>\n\nand an entity resolver should then load the URL as a byte stream in response to the resolveEntity call. \n\nFor a work around, anything that sets the default charset for the JVM to UTF-8 should avoid the \nproblem until it can be fixed.  There is not a clearly documented way to do that and it is platform \ndependent.  On a Nix machine, you could try\n\nexport LC_CTYPE=UTF-8\n\non Windows you could try:\n\njava -Dfile.encoding=UTF-8 org.apache.log4j.chainsaw...", "id": 109969, "time": "2007-10-30T13:18:38Z", "bug_id": 43736, "creation_time": "2007-10-30T13:18:38Z", "is_private": false, "attachment_id": null}, {"count": 1, "text": "Created attachment 21059\nThis is the file contains Chinese characters.\n\nHere is the file for you testing. \n\nThanks,\nJessica", "bug_id": 43736, "attachment_id": 21059, "id": 109983, "time": "2007-10-30T15:02:02Z", "creator": "jlin@akonix.com", "creation_time": "2007-10-30T15:02:02Z", "tags": [], "is_private": false}, {"count": 2, "tags": [], "bug_id": 43736, "attachment_id": null, "is_private": false, "id": 109989, "time": "2007-10-30T17:12:26Z", "creator": "carnold@apache.org", "creation_time": "2007-10-30T17:12:26Z", "text": "Minimal fix would be something like:\n\n===============================================================\n====\n--- src/main/java/org/apache/log4j/xml/XMLDecoder.java  (revision 570571)\n+++ src/main/java/org/apache/log4j/xml/XMLDecoder.java  (working copy)\n@@ -183,9 +183,9 @@\n     if (owner != null) {\n       reader = new LineNumberReader(new InputStreamReader(\n               new ProgressMonitorInputStream(owner,\n-                      \"Loading \" + url , url.openStream())));\n+                      \"Loading \" + url , url.openStream(), \"UTF-8\")));\n     } else {\n-      reader = new LineNumberReader(new InputStreamReader(url.openStream()));\n+      reader = new LineNumberReader(new InputStreamReader(url.openStream(), \"UTF-8\"));\n     }\n \n     Vector v = new Vector();\n\n\nWith comparable changes likely for UtilLogXMLDecoder.  That isn't sufficient to handle UTF-16 encoded \nlog files, but would be a good interim fix.  Proper fix probably means rewriting the whole thing (likely \nusing a SAX instead of a DOM parser to reduce memory use).  Would be good to get unit tests around it \nfirst before a rewrite."}, {"count": 3, "tags": [], "bug_id": 43736, "text": "I have applied Curt's patch to XMLDecoder with a very minor tweak.  The DOCTYPE\npre-amble was always assuming that the file was UTF-8 anyway, so it seems safe\nto do this, but any arbitrary  encoding can not be used without quite a bit of\nfiddly UI work at this stage.\n\nI have elected not to modify the UtilLoggingXMLDecoder at this point as it does\nnot use a pre-amble and is therefore assuming that the file is in whatever the\ndefault platform encoding is anyway.  \n\n", "id": 110202, "time": "2007-11-02T22:19:14Z", "creator": "psmith@apache.org", "creation_time": "2007-11-02T22:19:14Z", "is_private": false, "attachment_id": null}, {"count": 4, "text": "The presense of an encoding in the document type declaration in the string representation of the \ndocument is ignored by the parser.  It is a relic of the document once being encoded in as a byte \nstream, by the time that the parser is seeing it the byte stream has been decoded as a string by the \nInputStreamReader which is oblivious to any encoding declaration in the byte stream.\n\nAn external parsed entity (such as our and JUL's XML log files) that is not in UTF-8 or UTF-16 requires \nan explicit text declaration (http://www.w3.org/TR/2006/REC-xml-20060816/#charencoding).  \nWithout a text declaration, a parser will snif the file and determine if it is UTF-16BE or UTF-16LE by the \npresense of alternative 0 bytes and if not will assume that is it is UTF-8.  More detail at http://\nwww.w3.org/TR/2006/REC-xml-20060816/#sec-guessing.  It will never consult the platform default \nencoding.\n\nHowever, if it were written in the platform encoding with the proper text declaration, like:\n\n<?xml encoding=\"ISO-8859-1\"?>\n<log4j:event../>\n<log4j:event../>\n\nthe current XML decoders would fail to read the file since the would just append that to the string and \nthen the text declaration would be in the wrong place leading to a parsing error.\n\nThe only way to do it right is to rewrite, which I'm willing to do after getting log4cxx out the door.  But \nthere is never any case where using the platform encoding will get you the right content and using \n\"UTF-8\" would get you the wrong content.", "creator": "carnold@apache.org", "attachment_id": null, "id": 110211, "time": "2007-11-03T10:44:50Z", "bug_id": 43736, "creation_time": "2007-11-03T10:44:50Z", "tags": [], "is_private": false}, {"count": 5, "tags": [], "creator": "carnold@apache.org", "attachment_id": null, "is_private": false, "id": 110212, "time": "2007-11-03T10:54:36Z", "bug_id": 43736, "creation_time": "2007-11-03T10:54:36Z", "text": "On fiddly UI changes.  There should be no changes to the user-interface necessary since the UI appears to \nbe about to display the right characters if the log file is parsed correctly and all the information to parse \nthe file correctly is in the document itself."}, {"count": 6, "tags": [], "bug_id": 43736, "attachment_id": null, "text": "I suggest looking at what the Rome developers have done with their XMLReader class [1].  It goes through an elaborate process to figure out the proper charset for the document.  It's explained here [2] and here [3].  It's copyrighted by Sun Microsystems, but it's under an Apache license.  I adapted it for the XMLC project [4] and modified it to use gnu-regexp instead of JDK1.4 regexp, since my project depends on JDK1.3, not 1.4.  I also slightly modified a couple constructors to make it easier to provide a per/instance defaultEncoding if, for some reason, none can be detected.  I use it like this....\n\ntry {\n    InputSource inputSource = new ClosingInputSource(url);\n    try {\n        XmlReader reader = new XmlReader(InputSourceOps.openSystemId(url), false, defaultEncoding);\n        inputSource.setCharacterStream(reader);\n        inputSource.setEncoding(reader.getEncoding());\n    } catch (XmlReaderException xre) {\n        //This is somewhat unlikely to happen, but doesn't hurt to have\n        //extra fallback, which XmlReader conveniently allows for by\n        //providing access to the original unconsumed inputstream via\n        //the XmlReaderException\n        inputSource.setByteStream(xre.getInputStream());\n        inputSource.setEncoding(defaultEncoding);\n    }\n    return inputSource;\n} catch (IOException ioe) {\n    throw new XMLCError(\"Couldn't load file \"+url, ioe);\n}\n\nA lot of thought was put into this by the Rome team.  Seems like it would make sense to reuse it in Log4j rather than reinvent the wheel with something, likely, not nearly as robust.\n\nJake\n\n[1] https://rome.dev.java.net/source/browse/rome/src/java/com/sun/syndication/io/XmlReader.java\n[2] http://wiki.java.net/bin/view/Javawsxml/Rome05CharsetEncoding\n[3] http://blogs.sun.com/tucu/entry/detecting_xml_charset_encoding_again\n[4] http://cvs.xmlc.forge.objectweb.org/cgi-bin/viewcvs.cgi/xmlc/xmlc/xmlc/modules/xmlc/src/org/enhydra/xml/io/XmlReader.java\n\n", "id": 110217, "time": "2007-11-03T13:37:56Z", "creator": "hoju@visi.com", "creation_time": "2007-11-03T13:37:56Z", "is_private": false}, {"count": 7, "tags": [], "bug_id": 43736, "text": "Any XML parser will have the proper handling for encodings, you just need to allow it to work with the \nbyte streams instead of trying to help it by passing it character strings which will have already made \nassumptions about encoding that can't be undone.  I'm not familiar with Rome, but it seems like \neverything is well within the capabilities of JAXP if called properly and there should be no need to add \nanother dependency.  \n\nThe current approach is also wasteful since it first populates a DOM document and then extracts the \ninfo into an event stream when using an event based parser could eliminate the intermediate DOM \ndocument and the associated memory use.  I could see that being a pretty substantial performance \nissue with large log files.\n\nI've got a lot of experience in this area and it would not take much time for me to rewrite the code and \nwill try to get to it quickly.", "id": 110220, "time": "2007-11-03T22:41:53Z", "creator": "carnold@apache.org", "creation_time": "2007-11-03T22:41:53Z", "is_private": false, "attachment_id": null}, {"count": 8, "tags": [], "creator": "jlin@akonix.com", "attachment_id": null, "is_private": false, "id": 110275, "time": "2007-11-06T11:01:11Z", "bug_id": 43736, "creation_time": "2007-11-06T11:01:11Z", "text": "Since the workaround in the description doesn't work for me, do you think I \ncan get the fix within one week? Can you send the chainsaw bundle in zip or \nthe affected jars? \n\n\nThanks,\nJessica"}, {"count": 9, "tags": [], "creator": "psmith@apache.org", "text": "You could try this link:\n\nhttp://people.apache.org/builds/logging/repo/log4j/apache-chainsaw/1.99.0-SNAPSHOT/apache-chainsaw-1.99.0-20071103.061102-3-standalone.zip\n\nI'm currently experimenting with maven packaging options.  If you unpack the zip\nand run the .sh or .bat file in the bin/ subdirectory it should launch Chainsaw\nwith the encoding change.", "id": 110278, "time": "2007-11-06T12:57:20Z", "bug_id": 43736, "creation_time": "2007-11-06T12:57:20Z", "is_private": false, "attachment_id": null}, {"count": 10, "tags": [], "creator": "jlin@akonix.com", "text": "Created attachment 21092\nthe problme seems still exist", "id": 110282, "time": "2007-11-06T15:48:38Z", "bug_id": 43736, "creation_time": "2007-11-06T15:48:38Z", "is_private": false, "attachment_id": 21092}, {"count": 11, "tags": [], "creator": "thorbjoern@gmail.com", "text": "(In reply to comment #4)\n\n> The only way to do it right is to rewrite, which I'm willing to do after\n> getting log4cxx out the door.  But \n> there is never any case where using the platform encoding will get you the\n> right content and using \n> \"UTF-8\" would get you the wrong content.\n\nI would suggest looking at the XML snippet generating code instead, and change it so that all non-ASCII characters as well as <, > and & are encoded as &#...; (using the unicode value).  This will - I guess - parse correctly and completely circumvent the encoding problem.\n\nThe CDATA wrapper will not be necessary then, since all problematic characters are properly encoded.\n\nAdditionally this will be backware compatible.", "id": 118234, "time": "2008-07-03T04:02:52Z", "bug_id": 43736, "creation_time": "2008-07-03T04:02:52Z", "is_private": false, "attachment_id": null}, {"count": 12, "tags": [], "bug_id": 43736, "attachment_id": null, "text": "My experience with XML parsers have indicated that in order to get character encoding right, they should have the File object to work with instead of a stream or a reader?\n\nIs that an option with this issue in the code?", "id": 119254, "time": "2008-08-02T09:55:43Z", "creator": "thorbjoern@gmail.com", "creation_time": "2008-08-02T09:55:43Z", "is_private": false}, {"text": "Streams are okay, they don't have the base information to get relative URL's, but that isn't the case here.  XML parsers need to work with the raw undecoded byte stream, so InputStream, InputSource, File, URL all work fine.  However, String and Reader both decode upstream of the parser based on the default encoding (which XML purposefully ignores) and by the time the parser gets the content, everything may be corrupt.\n\nThis might be a blocker for a receivers release, but isn't part of log4j so would not be considered a blocker for log4j 1.2.16.", "tags": [], "bug_id": 43736, "attachment_id": null, "count": 13, "id": 119457, "time": "2008-08-05T17:25:57Z", "creator": "carnold@apache.org", "creation_time": "2008-08-05T17:25:57Z", "is_private": false}]