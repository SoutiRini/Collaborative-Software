[{"count": 0, "tags": [], "text": "I got n web servers (www1 ... wwwn) mounting the same resources from a nfs \nserver (nfs-server).\nAll of these are different physical machine.\nThe same resources are shared through web servers and ftp servers.\nAny updates of the web contents is done through ftp.\nSome time (very frequently) we see some of the httpd processes swallowing all \nthe memory resources\ntill the kernel get almost stuck. The overall throughput of the web servers is \n150-250 Mbit.\nWe tried to reproduce the event (with good luck) this way:\non ftp server I put this simple script\nwhile : do; cat b.html > a.hmtl; cat c.html > a.hmtl; done\na.html and b.html are different files of different size\nFrom a client I wget a.html in a loop.\nI targeted only one web server but it happens on all.\nFor some misterious reasons some gets not all, force the httpd process in \ncharge \nof the request, to grow huge in memory.\nI tested this behavour with different nfs server (NetApp and Solaris 9) and \ndifferent \nversion of Apache-httpd: this blocking event happens on apache version 2.0.[44-\n54] but \nnot on 1.3.33. The conf files are those Apache is shipped with, with only few \nnecessary\nline about virtual host (just one virtual) uncommented.\nThe loadmodule directives have not been changed.\nWeb servers and ftp server are version of RedHat linux ES 3.0 (update 3).\nThnks for your work and the effort spent in the past, present and future.", "is_private": false, "id": 74320, "creator": "mariop69@libero.it", "time": "2005-05-02T15:40:58Z", "bug_id": 34708, "creation_time": "2005-05-02T15:40:58Z", "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 34708, "attachment_id": null, "id": 74328, "time": "2005-05-02T19:07:26Z", "creator": "slive@apache.org", "creation_time": "2005-05-02T19:07:26Z", "is_private": false, "text": "Do you have:\nEnableSendfile Off\nin httpd.conf?"}, {"count": 2, "tags": [], "bug_id": 34708, "attachment_id": null, "id": 74329, "time": "2005-05-02T19:14:28Z", "creator": "chip@force-elite.com", "creation_time": "2005-05-02T19:14:28Z", "is_private": false, "text": "This is a known and documented problem when using NFS with Apache.\n\nPlease Add the following to your configuration file:\nEnableMMAP Off\nEnableSendfile Off\n\nSee the documentation for details on why this fixes your problem:\n\nhttp://httpd.apache.org/docs-2.0/mod/core.html#enablesendfile\nhttp://httpd.apache.org/docs-2.0/mod/core.html#enablemmap"}, {"count": 3, "tags": [], "bug_id": 34708, "attachment_id": null, "is_private": false, "id": 74331, "time": "2005-05-02T21:09:56Z", "creator": "mariop69@libero.it", "creation_time": "2005-05-02T21:09:56Z", "text": "I forgot \nthe only directives I changed were\nEnableMMAP off\nEnableSendfile off\nand that's how they appear in httpd.conf\nregards"}, {"count": 4, "tags": [], "text": "Created attachment 15171\nsnapshot of SysRq+M meminfo slabinfo\n\nBrief descrption of the event\nThe snapshots of /proc/meminfo , /proc/slabinfo and  have been taken Each\nseparately, after restarting httpd, but each snapshot has been taken Before,\nwhile \"growing in memory\", and after the httpd process with few hits (no more\nthan five) was requested.\nTests have been conducted under no production environment, the event has been\nreproduced with few hits and with only two httpd processes running.\nEach time a request hit the web server the child process grew in memory, Till\nafter few seconds from the last request, freed memory.\nThe httpd.conf has been taken (or better reduced) as clean as we could:\n\nServerRoot \"/etc/httpd\"\nPidFile run/httpd.pid\n\nListen 80\n\nUser apache\nGroup apache\n\nEnableMMAP off\nEnableSendfile off\nMaxClients\t1\n\nServerName www.host1.it\n\n<VirtualHost *>\n     DocumentRoot /nsf/mounted/dir\n     ServerName www.host2.it\n</VirtualHost>", "is_private": false, "id": 75383, "creator": "mariop69@libero.it", "time": "2005-05-25T22:44:52Z", "bug_id": 34708, "creation_time": "2005-05-25T22:44:52Z", "attachment_id": 15171}, {"count": 5, "tags": [], "bug_id": 34708, "attachment_id": null, "id": 75545, "time": "2005-05-27T22:12:02Z", "creator": "trawick@apache.org", "creation_time": "2005-05-27T22:12:02Z", "is_private": false, "text": "True or false: The problem only occurs when Apache is trying to serve a file which is being truncated at \nabout the same time."}, {"count": 6, "tags": [], "bug_id": 34708, "attachment_id": null, "is_private": false, "id": 75573, "time": "2005-05-29T15:52:44Z", "creator": "mariop69@libero.it", "creation_time": "2005-05-29T15:52:44Z", "text": "(In reply to comment #5)\n> True or false: The problem only occurs when Apache is trying to serve a file \nwhich is being truncated at \n> about the same time.\n\nI'd say true but I'll be investigating a little more for a final answer.\n"}, {"count": 7, "tags": [], "text": "Created attachment 15201\nsnapshot with gdb\n\nSome information hoping usefull. \nI run httpd from gdb and took a snapshot of what I thought usefull.", "is_private": false, "id": 75574, "creator": "mariop69@libero.it", "time": "2005-05-29T16:02:00Z", "bug_id": 34708, "creation_time": "2005-05-29T16:02:00Z", "attachment_id": 15201}, {"count": 8, "tags": [], "text": "A couple of other pieces of interest:\n\na) in gdb after it crashes, run \"backtrace full\" to get backtrace\nb) strace of the process up until the time everything goes to hell\n(hopefully you're using prefork MPM; no clue here how to get syscall trace of\nthreaded process on Linux)\n", "is_private": false, "id": 75575, "creator": "trawick@apache.org", "time": "2005-05-29T16:38:40Z", "bug_id": 34708, "creation_time": "2005-05-29T16:38:40Z", "attachment_id": null}, {"attachment_id": 15205, "tags": [], "bug_id": 34708, "is_private": false, "count": 9, "id": 75579, "time": "2005-05-30T03:43:55Z", "creator": "mariop69@libero.it", "creation_time": "2005-05-30T03:43:55Z", "text": "Created attachment 15205\nmore gdbing"}, {"count": 10, "tags": [], "bug_id": 34708, "attachment_id": 15206, "id": 75580, "time": "2005-05-30T04:17:49Z", "creator": "mariop69@libero.it", "creation_time": "2005-05-30T04:17:49Z", "is_private": false, "text": "Created attachment 15206\nstrace of httpd child\n\nUnfortunately it's hard to see it crashing due to the fact that at some point\nserver gets hugly slow, so the strace only refers to the time httpd grows huge\nin memory."}, {"attachment_id": null, "tags": [], "bug_id": 34708, "is_private": false, "count": 11, "id": 75589, "time": "2005-05-30T13:57:36Z", "creator": "trawick@apache.org", "creation_time": "2005-05-30T13:57:36Z", "text": "file was originally 110 bytes but it has been truncated after the time that we\nretrieved the size...  so we keep trying to 110 bytes over and over, allocating\nmemory each time\n\nopen(\"/usr/develop/CorriereOnline/baseroot/webroot/aa.html\", O_RDONLY) = 8\ngettimeofday({1117417939, 561092}, NULL) = 0\nlseek(8, 0, SEEK_SET)                   = 0\nread(8, \"\", 110)                        = 0\nlseek(8, 0, SEEK_SET)                   = 0\nread(8, \"\", 110)                        = 0\n... lseek and read repeated forever\n\nfile-bucket-read needs to realize what has happened (hit EOF on file where we\nexpected to read more bytes), and keep from adding another bucket in this case...\n\ncurrent:\n\n    /* If we have more to read from the file, then create another bucket */\n    if (filelength > 0) {\n\nmaybe?\n\n    if (filelength > 0 && *len != 0) {\n\ncore_output_filter is going to have to recognize this condition and decide what\nto do; \n\nif data has already been sent (presumably including HTTP response header which\nindicates how many bytes are to be sent), the best possibility may be to log an\nerror and drop the connection abruptly; \n\nFWIW, Apache 1.3 will usually SIGBUS when this happens (file truncated after\nApache find the file size), though that can be mitigated by sending truncatable\nfiles through mod_include since 1.3 mod_include uses the simplest file I/O\nmodel.    On a normal filesystem, as long as files are moved into the document\nroot without truncating in place, everything works fine.   When files in\ndocument root are truncated, Apache can't send a meaningful response to the\nclient (irrespective of whether it dies with SIGBUS or loops).\n\nSo yes Apache needs to handle this situation more gracefully, but there are\nlimits to that gracefulness which are only resolved by avoiding the trucate\nmethod of replacing file contents.\n\nSome methods that truncate:\nshell redirection\nfopen() without append flag\n\nA method that doesn't truncate:\nuse mv command to update file in documentroot from complete new file"}, {"count": 12, "tags": [], "bug_id": 34708, "attachment_id": null, "id": 75722, "time": "2005-06-01T13:26:19Z", "creator": "jorton@redhat.com", "creation_time": "2005-06-01T13:26:19Z", "is_private": false, "text": "That change to file_bucket_read looks correct, using rv != APR_EOF instead might\nit clear, are you going to commit?   I've seen this lseek/read loop from a user\nonce before but not been able to track it down.  (nor was the memory use\nexplosion reported, oddly, since it seems unavoidable)\n\nThe core_output_filter fix: it looks like this can trigger only in the case\nwhere it falls through to read the bucket for non-FILE || length <\nAP_MIN_SENDFILE_BYTES, right?  Just aborting in that case seems sensible,\nthere's similar logic further down when saving to the deferred_write brigade. \nGot a patch ready or shall I?"}, {"count": 13, "tags": [], "bug_id": 34708, "attachment_id": null, "id": 75735, "time": "2005-06-01T16:15:01Z", "creator": "trawick@apache.org", "creation_time": "2005-06-01T16:15:01Z", "is_private": false, "text": "I don't have any patches and/or tested code.  I was hoping you'd chime in ;)\n\nI concur on checking for rv != APR_EOF.\n\nFor core_output_filter, looks like there are two paths, based on what we\noriginally found out for the file length (before truncation):\n\na) big enough for sendfile\n\nnot improved by file-bucket-read fix; I suspect sendfile caller will loop since\nwe won't make any progress when calling sendfile (0 bytes written)\nincidentally, I've seen a few loopers with sendfile enabled on HP-UX; supposedly\nthat is NOT due to truncated file but instead something happening in network\nlayer; catching this no-progress-made scenario would exit the loop on the HP-UX\nscenario as well\n\nb) not big enough for sendfile\n\nWith the file-bucket-read fix we won't be misled by e->length since we do the\nbucket read in the path you mentioned to find the length.  rv will be APR_EOF\n(not abnormal when reading a file) and length will be 0 (not abnormal when\nreading a file).  How do we know that something has gone wrong?  Compare the\ndeclared length of the file (e->length) with the length returned by\nfile-bucket-read?\n"}, {"count": 14, "tags": [], "text": "Marking fixed with the apr-util change which was committed.\n\nhttp://svn.apache.org/viewcvs?rev=190575&view=rev\n\nre. (a) this is specific to some of the apr_socket_sendfile() implementations;\nall should return APR_EOF if no bytes were sent\n\nre. (b) it *is* abnormal for the core output filter to read EOF from the file -\nthat should never happen, because it knows exactly how many bytes to read from\nthe file and should never read beyond that to get the EOF; so APR_EOF should be\ntreated like any other error, the code looks OK in that respect.   (modulo\nplaces where error checking is absent completely)", "is_private": false, "id": 78313, "creator": "jorton@redhat.com", "time": "2005-08-09T22:55:00Z", "bug_id": 34708, "creation_time": "2005-08-09T22:55:00Z", "attachment_id": null}]