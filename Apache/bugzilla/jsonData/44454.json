[{"count": 0, "tags": [], "creator": "anoonan@indeed.com", "text": "We have some servers running Apache 2.2.3-11 on Centos 5 with mod_jk 1.2.25\nconnecting to tomcats running 5.5.20.  Over the course of a couple of days, the\nbusy count on the various servers (as reported by the jkstatus page) creeps\nupward, causing the balancing to get out of whack.  A graceful restart of apache\ngets things back to normal, as does a restart of the tomcat, but the process\nstarts again.  I'm not entirely sure what information would be helpful in\ntroubleshooting this, please advise.  Earlier load testing did not show\nanything, so I'm guessing that specific circumstances may cause the count to\nbecome wrong.\n\nThanks,\nAndrew", "id": 113896, "time": "2008-02-19T13:03:33Z", "bug_id": 44454, "creation_time": "2008-02-19T13:03:33Z", "is_private": false, "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 44454, "attachment_id": null, "is_private": false, "id": 113897, "time": "2008-02-19T13:31:23Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2008-02-19T13:31:23Z", "text": "Are we sure, that the growing busy numbers are wrong?\n\nAsked differently: can you do Thread Dumps of your backends to confirm, that\nthey are *not* having that many requests busy? In theory some requests might got\nstuck in the backend. If you don't have a reaply_timeout, mod_jk will wait\nindefinitely for a response.\n\nBy using \"kill -QUIT PID\" (PID is the process id of your backend=romcat process)\nyou will get a stack for each thread in Tomcat in your catalina.out. This\nprocedure does not influence your running Tomcat negatively. It will pause for a\nfew milliseconds and then resume the usual work.\n\nIn the dump, the threads responsible for requests coming in via AJP13 (from\nmod_jk) can be identified as TP-ProcessorXXX. Some of the will only have a very\nshort stack, which indicates, that they are idle in the thread pool. Some have a\nslightly longer stack (about 10 lines) with a top level method being a socket\nread, which tells us, that mod_jk is connected to them, but they are not actualy\nprocessing a request, and few will have a very long stack, usually containing\nwebapp methods, a service method etc. Those ar the ones that process a request\nand should be equivalnt to \"busy\" (at least of Tomcat only gets requests from\none httpd. Otherwise you need to add up all the busy for this Tomcat in the\nvarious httpd).\n\nAltough in principal there's a chance, that busy could go wrong, I never\ndetected it in practice and the feature is out in the wild for quite some time\nnow, without problem reports. That's why I first want you to check, taht the\nnumbers are really wrong.\n\nFurthermore: are there errors in the JK error log?\n\nLast: if you want to track how and when busy increases, you can add\n\"%{JK_LB_LAST_BUSY}n\" to your LogFormat in httpd. Then your access log will\ncontain the busy number after each request in the request log line.\n\nRegards,\n\nRainer\n"}, {"count": 2, "tags": [], "creator": "anoonan@indeed.com", "attachment_id": 21564, "text": "Created attachment 21564\nResults from Thread Dump - Idle Server\n\nThe JK Threads from an idle tomcat instance during a thread dump", "id": 113898, "time": "2008-02-19T14:00:02Z", "bug_id": 44454, "creation_time": "2008-02-19T14:00:02Z", "is_private": false}, {"count": 3, "tags": [], "creator": "anoonan@indeed.com", "attachment_id": null, "text": "I took one of our systems out of rotation and let it sit for a while.  Its busy\nwas hanging around 16-20 while in rotation.  Once out, it almost immediately\nwent to 12 and remained at 12 the entire time.  I did the thread dump and\nattached the TP-ProcessorXXX entries.  It looks like the vast majority of\nconnections show in socket read, with a few idle in the pool.  There were no\nactive threads (which is good, since it was out of rotation).  The mod_jk.log\nshows no errors, though there are many warnings for things like: \n\n[Tue Feb 19 15:56:18.849 2008] [13119:2611919760] [warn]\nmap_uri_to_worker::jk_uri_worker_map.c (550): Uri echo|20YYY;echo| is invalid.\nUri must start with /\n\nGood ole' internet.  Thanks for the variable, I can see that being handy, though\nI can't add anything in right now.\n\nLet me know if there's anything more.", "id": 113899, "time": "2008-02-19T14:08:47Z", "bug_id": 44454, "creation_time": "2008-02-19T14:08:47Z", "is_private": false}, {"count": 4, "tags": [], "creator": "rainer.jung@kippdata.de", "is_private": false, "id": 113904, "creation_time": "2008-02-19T14:38:02Z", "time": "2008-02-19T14:38:02Z", "bug_id": 44454, "text": "Thanks for the check, so we know for sure, that the busy count doesn't\ncorrespond to the backend status.\n\nAlthough there is still a theoretical possibility, that mod_jk *thinks* the\nbackend is that busy, I checked, if we miss some error conditions when\ndecreasing the busy count. There is not condition in the code, how the busy\nincrement could not be followed by a decrement. The counter is hold inside a\nshared memory segment and it is marked as volatile. In case you hae a *very*\nbusy web site, it's possible, that we run into a non-atomic increment/decrement\nproblem. Although that's possible, I'm still thinking about other possibilities.\n\nThe status worker shows a busy counter for the lb and for each member of an lb.\nDo both types of busy counters suffer from the problem?\n\nTo get an idea: how fast does the problem happen (how much busy increase per day\nor so, and also how many requests approximately are handled by the corresponding\nworker during that time)?\n\nYou mentioned, that this influences negatively the balancing behaviour. I deduct\nfrom that, that you are using the Busyness method. Do you have a reason not to\nuse the Requests method? It would not suffer from the \"wrong busy\" problem.\n\nFinally: although we know no change in the code that would suggest that the\nproblem will go sway in 1.2.26, is it possible to switch to that version? Be\ncareful: in 1.2.26 your JkMounts are not automatically inhertited between httpd\nVirtualHosts. You either need to include them in each VirtualHost or use\nJkMountCopy (see docs). Apart from that, the version update should be reliable.\n\nI'm still wondering, if mod_jk threads might hang between start of request and\nend of response. You could use gstack, to do thread dumps on the httpd side.\nSince the busy number is summed up over all httpd child processes and we don't\nknow if the problem is located in only one of those or visible in all of them,\nyou would need to do gstack on all httpd child processes.\n\nThe ones busy in mod_jk will be easily detectable, since mod_jk uses \"jk\" as\npart of most function names.\n", "attachment_id": null}, {"count": 5, "tags": [], "creator": "anoonan@indeed.com", "text": "     Well, we're a busy site, but not stunningly so... everyone's idea on what\nmakes a busy site may differ.  The httpd systems are never that terribly loaded\nCPU/Load wise (less then 40%).  The global counter and the individual counters\nadd up to each over, so I believe that indicates that they both are having the\nissue.  It's hard to tell at what level the inflation occurs, as without calming\nservers at around the same time each day it's hard to tell what are real\nrequests and which are inflation.  We have a few servers in rotation that are\nbulkier and run a 2x higher LBFactor, and they seem currently to have about 2x\nthe false busys.  I'm guessing about 5 a day for the lighter servers, and 10 a\nday for the bigger servers.  Because the load balancing is unequal, it varies,\nbut since our graceful restart 2.5 days ago, the ACC on the servers is anywhere\nfrom 2-4 million, depending on the server.\n     As for the balancing method, the busyness method is definitely more even\nwhen it works since our requests can be very different weights.  As for running\n1.2.26, I think we could compile/use it no problem (we're running 2.2.3, the\nprecompile indicates 2.2.6 as a prereq), but I know management would be a little\ngrumbly to make a change without a better idea as to what's going on.  I don't\nthink the config changes are a problem for us as we use preference files and\nJKMounts only at VHost level... no globals.\n     I'm gathering the gstack data now and will post in a few minutes.\n\nIn case it comes up, as typical worker is configured like:\nworker.<WORKER>.port=8009\nworker.<WORKER>.host=<IP>\nworker.<WORKER>.type=ajp13\nworker.<WORKER>.socket_timeout=30\nworker.<WORKER>.connection_pool_timeout=60\nworker.<WORKER>.lbfactor=20\n\nThe LB involved is:\nworker.<LB>.lock=P\nworker.<LB>.method=B\n\nand we have a global setting of:\nworker.maintain=15\n\nThanks,\nAndrew", "id": 113907, "time": "2008-02-19T15:50:35Z", "bug_id": 44454, "creation_time": "2008-02-19T15:50:35Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 44454, "is_private": false, "id": 113908, "creation_time": "2008-02-19T15:54:03Z", "time": "2008-02-19T15:54:03Z", "creator": "anoonan@indeed.com", "text": "Created attachment 21566\nThread Dump from Apache Children", "attachment_id": 21566}, {"count": 7, "tags": [], "bug_id": 44454, "text": "How many threads per process do you use in httpd (MPM configuration)? 60?\n\nAnyways, I adopted my JVM Stack analysis script to gstack. The results (sums\nover all 19 processes=1178 Threads) are following. In total I can see 68 threads\nthat would count as busy in the lb (and 2 threads that seem to do direct ajp\nwork without lb). So how do these numbers compare to your observed busyness at\nthe time of dumps?\n\nConcerning your configuration:\n\nPessimistic locking for me is more a relic from the early days of the code. I\ndon't know about any cases, ahere it is actually needed. In case you had opened\nthis case with optimistic locking (default) when running out of ideas, I woulkd\nhave asked you to try pessimistic locking, but your case proves once more, that\nit's not really useful. I would stick to the default (optimistic) in order to\nrun the code path that#s used most often.\n\nYou don't have a reply_timeout. The stack seem to indicate, that hanging replies\nmight well be your problem. Unfortunately in 1.2.25 max_reply_timeouts doesn't\nwork, so any reply_timeout that fires will put your worker into error state\nuntil recovery, so other users will loose sessions when being switched over to\nfailover workers.\n\nIn 1.2.26 we fixed max_reply_timeouts, so that you can configure a reply_timeout\n(don't wait longer for an answer than X seconds) and the number of\nreply_timeouts that you expect before you think the backend itself is in serious\ntrouble and should be put out of service. So maybe here's the argument for a\nswitch to 1.2.26, although I hadn't expected that situation last night, when I\nwrote about 1.2.26.\n\nIn 1.2.27 (march) we will be able to configure a reply_timeout not only per\nworker, but instead per mapping, so that you can give individual JkMounts\nindividual reply_timeouts. So if some use cases are known to take a long time,\nyou can have an appropriately short general timeout, with some longer timeouts\nfor the URLs known to take notoriously long. It already works in 1.2.27-dev, but\nthere's no release out there.\n\nHave a look at the following data yourself. I think the most reasonable approach\nis to use a reply_timeout with max_reply-timeouts, which in turn will force you\nto update to 1.2.26.\n\nThe details of the dumps:\n\n548 Threads idle waiting (I would say in Keepalive,\n    so hanging on a connection from the client,\n    but waiting for the next request):\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in poll () from /lib/libc.so.6\n#2  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#3  0xHEXADDR in apr_socket_recv () from /usr/lib/libapr-1.so.0\n#4  0xHEXADDR in apr_bucket_socket_create () from /usr/lib/libaprutil-1.so.0\n#5  0xHEXADDR in apr_brigade_split_line () from /usr/lib/libaprutil-1.so.0\n#6  0xHEXADDR in ap_core_input_filter () from /proc/PID/exe\n#7  0xHEXADDR in ap_get_brigade () from /proc/PID/exe\n#8  0xHEXADDR in ap_rgetline_core () from /proc/PID/exe\n#9  0xHEXADDR in ap_read_request () from /proc/PID/exe\n#10 0xHEXADDR in ap_register_input_filter () from /proc/PID/exe\n#11 0xHEXADDR in ap_run_process_connection () from /proc/PID/exe\n#12 0xHEXADDR in ap_process_connection () from /proc/PID/exe\n#13 0xHEXADDR in ap_graceful_stop_signalled () from /proc/PID/exe\n#14 0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#15 0xHEXADDR in start_thread () from /lib/libpthread.so.0\n#16 0xHEXADDR in clone () from /lib/libc.so.6\n\n272 Threads idle\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0\n#2  0xHEXADDR in apr_thread_cond_wait () from /usr/lib/libapr-1.so.0\n#3  0xHEXADDR in ap_queue_pop () from /proc/PID/exe\n#4  0xHEXADDR in ap_graceful_stop_signalled () from /proc/PID/exe\n#5  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#6  0xHEXADDR in start_thread () from /lib/libpthread.so.0\n#7  0xHEXADDR in clone () from /lib/libc.so.6\n\n235 Threads lingering close (client connection shutdown)\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in poll () from /lib/libc.so.6\n#2  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#3  0xHEXADDR in apr_socket_recv () from /usr/lib/libapr-1.so.0\n#4  0xHEXADDR in ap_lingering_close () from /proc/PID/exe\n#5  0xHEXADDR in ap_graceful_stop_signalled () from /proc/PID/exe\n#6  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#7  0xHEXADDR in start_thread () from /lib/libpthread.so.0\n#8  0xHEXADDR in clone () from /lib/libc.so.6\n\n59 Threads in mod_jk, waiting for reply from the backend (with lb?)\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in read () from /lib/libpthread.so.0\n#2  0xHEXADDR in jk_tcp_socket_recvfull () from /etc/httpd/modules/mod_jk.so\n#3  0xHEXADDR in ajp_connection_tcp_get_message ()\n#4  0xHEXADDR in ajp_get_reply () from /etc/httpd/modules/mod_jk.so\n#5  0xHEXADDR in ajp_service () from /etc/httpd/modules/mod_jk.so\n#6  0xHEXADDR in service () from /etc/httpd/modules/mod_jk.so\n#7  0xHEXADDR in jk_handler () from /etc/httpd/modules/mod_jk.so\n#8  0xHEXADDR in ap_run_handler () from /proc/PID/exe\n#9  0xHEXADDR in ap_invoke_handler () from /proc/PID/exe\n#10 0xHEXADDR in ap_process_request () from /proc/PID/exe\n#11 0xHEXADDR in ap_register_input_filter () from /proc/PID/exe\n#12 0xHEXADDR in ap_run_process_connection () from /proc/PID/exe\n#13 0xHEXADDR in ap_process_connection () from /proc/PID/exe\n#14 0xHEXADDR in ap_graceful_stop_signalled () from /proc/PID/exe\n#15 0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#16 0xHEXADDR in start_thread () from /lib/libpthread.so.0\n#17 0xHEXADDR in clone () from /lib/libc.so.6\n\n36 Threads internal tasks (non JK, non-worker threads)\n\n11 Threads sending non-JK content back to browser\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in poll () from /lib/libc.so.6\n#2  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#3  0xHEXADDR in apr_socket_sendv () from /usr/lib/libapr-1.so.0\n#4  0xHEXADDR in ap_bucket_eoc_create () from /proc/PID/exe\n#5  0xHEXADDR in ap_core_output_filter () from /proc/PID/exe\n#6  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#7  0xHEXADDR in ap_http_chunk_filter () from /proc/PID/exe\n#8  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#9  0xHEXADDR in ap_http_outerror_filter () from /proc/PID/exe\n#10 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#11 0xHEXADDR in ap_content_length_filter () from /proc/PID/exe\n#12 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#13 0xHEXADDR in ?? () from /etc/httpd/modules/mod_deflate.so\n#14 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#15 0xHEXADDR in ap_old_write_filter () from /proc/PID/exe\n#16 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#17 0xHEXADDR in ap_note_auth_failure () from /proc/PID/exe\n#18 0xHEXADDR in ap_process_request () from /proc/PID/exe\n#19 0xHEXADDR in ap_register_input_filter () from /proc/PID/exe\n\n6 Threads sending back jk-replies to browser\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in poll () from /lib/libc.so.6\n#2  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#3  0xHEXADDR in apr_socket_sendv () from /usr/lib/libapr-1.so.0\n#4  0xHEXADDR in ap_bucket_eoc_create () from /proc/PID/exe\n#5  0xHEXADDR in ap_core_output_filter () from /proc/PID/exe\n#6  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#7  0xHEXADDR in ap_http_chunk_filter () from /proc/PID/exe\n#8  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#9  0xHEXADDR in ap_http_outerror_filter () from /proc/PID/exe\n#10 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#11 0xHEXADDR in ap_content_length_filter () from /proc/PID/exe\n#12 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#13 0xHEXADDR in ap_filter_flush () from /proc/PID/exe\n#14 0xHEXADDR in apr_brigade_write () from /usr/lib/libaprutil-1.so.0\n#15 0xHEXADDR in ap_old_write_filter () from /proc/PID/exe\n#16 0xHEXADDR in ap_rwrite () from /proc/PID/exe\n#17 0xHEXADDR in ws_write () from /etc/httpd/modules/mod_jk.so\n#18 0xHEXADDR in ajp_get_reply () from /etc/httpd/modules/mod_jk.so\n#19 0xHEXADDR in ajp_service () from /etc/httpd/modules/mod_jk.so\n\n2 Threads waiting in mod_jk for response from backend (without lb?)\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in read () from /lib/libpthread.so.0\n#2  0xHEXADDR in jk_tcp_socket_recvfull () from /etc/httpd/modules/mod_jk.so\n#3  0xHEXADDR in ajp_connection_tcp_get_message ()\n#4  0xHEXADDR in ajp_get_reply () from /etc/httpd/modules/mod_jk.so\n#5  0xHEXADDR in ajp_service () from /etc/httpd/modules/mod_jk.so\n#6  0xHEXADDR in jk_handler () from /etc/httpd/modules/mod_jk.so\n#7  0xHEXADDR in ap_run_handler () from /proc/PID/exe\n#8  0xHEXADDR in ap_invoke_handler () from /proc/PID/exe\n#9  0xHEXADDR in ap_process_request () from /proc/PID/exe\n#10 0xHEXADDR in ap_register_input_filter () from /proc/PID/exe\n#11 0xHEXADDR in ap_run_process_connection () from /proc/PID/exe\n#12 0xHEXADDR in ap_process_connection () from /proc/PID/exe\n#13 0xHEXADDR in ap_graceful_stop_signalled () from /proc/PID/exe\n#14 0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#15 0xHEXADDR in start_thread () from /lib/libpthread.so.0\n#16 0xHEXADDR in clone () from /lib/libc.so.6\n\n2 Threads writing back jk-replies to client (with chunking)\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in poll () from /lib/libc.so.6\n#2  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#3  0xHEXADDR in apr_socket_sendv () from /usr/lib/libapr-1.so.0\n#4  0xHEXADDR in ap_bucket_eoc_create () from /proc/PID/exe\n#5  0xHEXADDR in ap_core_output_filter () from /proc/PID/exe\n#6  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#7  0xHEXADDR in ap_http_outerror_filter () from /proc/PID/exe\n#8  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#9  0xHEXADDR in ap_content_length_filter () from /proc/PID/exe\n#10 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#11 0xHEXADDR in ap_filter_flush () from /proc/PID/exe\n#12 0xHEXADDR in apr_brigade_write () from /usr/lib/libaprutil-1.so.0\n#13 0xHEXADDR in ap_old_write_filter () from /proc/PID/exe\n#14 0xHEXADDR in ap_rwrite () from /proc/PID/exe\n#15 0xHEXADDR in ws_write () from /etc/httpd/modules/mod_jk.so\n#16 0xHEXADDR in ajp_get_reply () from /etc/httpd/modules/mod_jk.so\n#17 0xHEXADDR in ajp_service () from /etc/httpd/modules/mod_jk.so\n#18 0xHEXADDR in service () from /etc/httpd/modules/mod_jk.so\n#19 0xHEXADDR in jk_handler () from /etc/httpd/modules/mod_jk.so\n\n1 Thread flushing to client in jk reply\n#0  0xHEXADDR in __kernel_vsyscall ()\n#1  0xHEXADDR in poll () from /lib/libc.so.6\n#2  0xHEXADDR in apr_wait_for_io_or_timeout () from /usr/lib/libapr-1.so.0\n#3  0xHEXADDR in apr_socket_sendv () from /usr/lib/libapr-1.so.0\n#4  0xHEXADDR in ap_bucket_eoc_create () from /proc/PID/exe\n#5  0xHEXADDR in ap_core_output_filter () from /proc/PID/exe\n#6  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#7  0xHEXADDR in ap_http_outerror_filter () from /proc/PID/exe\n#8  0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#9  0xHEXADDR in ap_content_length_filter () from /proc/PID/exe\n#10 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#11 0xHEXADDR in ap_old_write_filter () from /proc/PID/exe\n#12 0xHEXADDR in ap_pass_brigade () from /proc/PID/exe\n#13 0xHEXADDR in ap_rflush () from /proc/PID/exe\n#14 0xHEXADDR in ws_flush () from /etc/httpd/modules/mod_jk.so\n#15 0xHEXADDR in ajp_get_reply () from /etc/httpd/modules/mod_jk.so\n#16 0xHEXADDR in ajp_service () from /etc/httpd/modules/mod_jk.so\n#17 0xHEXADDR in service () from /etc/httpd/modules/mod_jk.so\n#18 0xHEXADDR in jk_handler () from /etc/httpd/modules/mod_jk.so\n#19 0xHEXADDR in ap_run_handler () from /proc/PID/exe\n\n5 Threads other states, non-jk related\n", "id": 113918, "time": "2008-02-20T06:18:25Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2008-02-20T06:18:25Z", "is_private": false, "attachment_id": null}, {"count": 8, "attachment_id": null, "bug_id": 44454, "is_private": false, "id": 113919, "time": "2008-02-20T09:10:53Z", "creator": "anoonan@indeed.com", "creation_time": "2008-02-20T09:10:53Z", "tags": [], "text": "     The busyness as reported at the time of the dump was around 300, though\ngiven the amount of inflation or so from previous testing, 68 sounds reasonable\nfor actual connections at the time.  We are running 60 threads per child.  I\nthink it's reasonable for us to run optimistic locking, and I'll see if we can\nchange to that this evening.  As for 1.2.26, we're doing some compiling and\ntesting on that right now... any thoughts on the use of --enable-flock on the\ncompile?  Going to 1.2.26 will take a few days, though, as we will need to move\nslowly and test things out."}, {"count": 9, "tags": [], "creator": "rainer.jung@kippdata.de", "text": "So unfoirtunately we now *know*, that the number is wrong, i.e. it doesn't\nreflect the internal state of mod_jk. I had the hope, that 68 was already the\ninflated value.\n\nThat means: all my comments on reply_timeout are true, but will most likely\n*not* help about the wrong busy values.\n\nSo I'm a little clueless on how to proceed further ...\n\nWith the busy count in the access log, we could see, if the unwanted increase in\nbusy mostly happens during busy hours (assuming that your load is not totally\nequally distributed over the day). If we really have a problem with non-atomic\nincrease/decrease, we should expect that to happen mostly during the peak hours.\n\nPossible workarounds:\n\n- don't use lbmethod busyness (yes I know, you've got good reasons to use it,\nbut unfortunately also one reason to not use it)\n- (gracefully) restarting the web server. If you can live with the problem for a\nday, you could restart the web server once every night. Caution: this will reset\nall settings changed in mod_jk via the status worker during the day back to the\nvalues configured in the config files, e.g. activations status (disable, stop).\n- I could find out, if oine can easily change the busy value from the outside in\nthe shm file. Of course we don't know the correct busy value, so we could only\nreset to 0. If we decrease the value in mod_jk and it would get below 0, we keep\nit at zero. So there is some chance, that after resetting it to zero, it will\nget close to the correct number after some time (we would only need a short\nmoment were statistically load is low).\n\nNo really nice option available at the moment ...\n\nHow many threads per process do you have configured for the MPM? 60?\n\nNo advise on enable-flock.\n\nDo you think you could reproduce with a simple test scenario? This would open up\nthe opportunity to test patches. As I said, we can't reproduce and I don't know\nof any other such case.", "id": 113920, "time": "2008-02-20T09:46:45Z", "bug_id": 44454, "creation_time": "2008-02-20T09:46:45Z", "is_private": false, "attachment_id": null}, {"count": 10, "tags": [], "text": "Yeah, I did a bunch of load testing against these systems when we moved to 2.2\nand 1.2.25 (we were using 2.0 and an internally hacked version of 1.2.<something\nold, perhaps 12>) where I tossed a ton of requests at them to the tune of 5000\nreq/sec through JK with a couple of servers on the backend that soaked up the\ndummy requests with a random delay.  These tests would run for over a day and\nthe result would have the busy count going back down to 0.  I suspect that this\nis event driven, and that the load relationship exists simply because higher\nloaded systems are more likely to experience the event.  Is there a case that\nyou can think of where the busy counter is incremented, but the thread is\naborted?  Is there JK logging that could be turned on for a while that would log\nthe increment and decrement of the counter with requests?  This way they could\nbe paired up and the specific requests not being decremented would be identified\nand could be examined for issues.  It seems like the access log results would\nnot let us know this info because it could show the overall state at the\ncompletion of the request, and not if the thread actually incremented and\ndecremented the counters.  I don't really know what I'm talking about, here, so\nlet me know if something like that is possible or not very practical.\n\nThanks,\nAndrew", "is_private": false, "bug_id": 44454, "id": 113923, "time": "2008-02-20T10:28:28Z", "creator": "anoonan@indeed.com", "creation_time": "2008-02-20T10:28:28Z", "attachment_id": null}, {"count": 11, "tags": [], "creator": "rainer.jung@kippdata.de", "attachment_id": null, "text": "Before I start to think deeper about your suggestion: could it be related \nto \"apachectl restart\" or \"apachectl graceful\"? Or would you say that that \ndoesn't fit what you see?\n\nSecond possibility: are we sure, that you don't experience any process crashes \n(segfaults et.al.)? Normaly those should get logged in the usual error log of \nhttpd.", "id": 113936, "time": "2008-02-20T15:47:46Z", "bug_id": 44454, "creation_time": "2008-02-20T15:47:46Z", "is_private": false}, {"count": 12, "tags": [], "bug_id": 44454, "text": "Well, I'm not 100% sure what you mean by related to the apache graceful/restart.\n When we do a graceful (don't really do restarts because of the live nature),\nthe counts do seem to reset, so I don't *think* there's a problem there.  I\ndon't see any segfaults in the log, just junk like \"request failed: error\nreading the headers\" from crappy bots and script kiddies.  Also, I just started\nlooking through the code and it seems like the difference between the optimistic\nlocking and pessimistic locking is just that the pessimistic locking just does\nthe same locking/unlocking sequence in more places, is that right?  I'm not a C\nprogrammer by any stretch of the imagination.", "id": 113937, "time": "2008-02-20T16:01:33Z", "creator": "anoonan@indeed.com", "creation_time": "2008-02-20T16:01:33Z", "is_private": false, "attachment_id": null}, {"count": 13, "tags": [], "bug_id": 44454, "is_private": false, "text": "optimistic/pessimistic: that's right. As I said, switching to ptimistic will\nmost likely not help with this issue, but I would expect less risk for other issues.\n\ngraceful: I was thinking about graceful restarts, which let threads end their\nrequest, but thought they might not be able to decrease the busy counter in the\nshared memory any more. As you said, that does not seem to be the reason for\nyour problem.\n\nAnother comment: the busy count is not really what's used by the Busyness\nmethod. We still use what's called \"V\" in the status output. It should be the\nsame as busyness, in case all load factors are 1, and otherwise some bigger\nnumbers indicating weighted busyness. As you observe problems in the balancing,\nI expect the \"V\" values to get inflated as well. Right?\n\nThe factor by which V differs from busy is shown as multiplicity \"M\" in the\nstatus output. Thos are integers which have the same ratios for the different\nworkers as 1/(load factor).\n", "id": 113938, "time": "2008-02-20T16:36:24Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2008-02-20T16:36:24Z", "attachment_id": null}, {"count": 14, "tags": [], "bug_id": 44454, "is_private": false, "id": 113964, "creation_time": "2008-02-21T08:45:23Z", "time": "2008-02-21T08:45:23Z", "creator": "anoonan@indeed.com", "text": "Yeah, the relationship between M, V, and the Busy is correct (which is to say\nthat V is inflated).", "attachment_id": null}, {"count": 15, "tags": [], "bug_id": 44454, "attachment_id": null, "text": "We've noticed this problem in our production environment as well...  At present it is a low-load point in the day (when we are processing ~20 requests), yet the busy count is reported as being 400+ (which is about MAX - 20 in jk-status).  \n\nNothing short of restarts seem to correct the count.\n\nI've literally tried all the timeout values I could find (on both ends - mod_jk and jboss) - keeping of course in mind their intended purposes as described in the docs - but none seemed to have any effect on the busy count not decrementing.\n\nI did a jboss thread dump and found 146 threads with the word \"8009\" in it, but according to Rainer's analysis only a handful (11 to be exact - I grepped RUNNABLE threads only) of them are actually processing requests.\n\nCuriously enough, we did not seem to have this issue in our test environment (with nearly identical configurations), even after intensive load testing.\n\nI'm not sure if this is something specific to our environment.  For the record:\n\nSunOS mega 5.10 Generic_127111-06 sun4v sparc SUNW,Sun-Fire-T200\n\nServer Version: Apache/2.2.6 (Unix) mod_jk/1.2.25\nJK Version:     mod_jk/1.2.25\n\nServer version: Apache/2.2.6 (Unix)\nServer built:   Jan 19 2008 22:12:59\nServer's Module Magic Number: 20051115:5\nServer loaded:  APR 1.2.11, APR-Util 1.2.10\nCompiled using: APR 1.2.11, APR-Util 1.2.10\nArchitecture:   32-bit\nServer MPM:     Worker\n  threaded:     yes (fixed thread count)\n    forked:     yes (variable process count)\nServer compiled with....\n -D APACHE_MPM_DIR=\"server/mpm/worker\"\n -D APR_HAS_SENDFILE\n -D APR_HAS_MMAP\n -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled)\n -D APR_USE_FCNTL_SERIALIZE\n -D APR_USE_PTHREAD_SERIALIZE\n -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT\n -D APR_HAS_OTHER_CHILD\n -D AP_HAVE_RELIABLE_PIPED_LOGS\n -D DYNAMIC_MODULE_LIMIT=128\n -D HTTPD_ROOT=\"/usr/local/httpd-cls01\"\n -D SUEXEC_BIN=\"/usr/local/httpd-cls01/bin/suexec\"\n -D DEFAULT_SCOREBOARD=\"logs/apache_runtime_status\"\n -D DEFAULT_ERRORLOG=\"logs/error_log\"\n -D AP_TYPES_CONFIG_FILE=\"conf/mime.types\"\n -D SERVER_CONFIG_FILE=\"conf/httpd.conf\"\n\n\nWe are currently running in degraded mode due to a session replication issue, so for the time being only one node is responding to requests.  Perhaps this is part of the equation?\n", "id": 115587, "time": "2008-04-14T06:25:52Z", "creator": "toutatisdev@gmail.com", "creation_time": "2008-04-14T06:25:52Z", "is_private": false}, {"count": 16, "tags": [], "creator": "toutatisdev@gmail.com", "attachment_id": null, "text": "I forgot to mention the rate of the leak... we last did a restart about 48 hours ago.  We seem to be incrementing the busy count by about 200 per day.\n\n(In reply to comment #15)\n> We've noticed this problem in our production environment as well...  At present\n> it is a low-load point in the day (when we are processing ~20 requests), yet\n> the busy count is reported as being 400+ (which is about MAX - 20 in\n> jk-status).  \n> \n> Nothing short of restarts seem to correct the count.\n> \n> I've literally tried all the timeout values I could find (on both ends - mod_jk\n> and jboss) - keeping of course in mind their intended purposes as described in\n> the docs - but none seemed to have any effect on the busy count not\n> decrementing.\n> \n> I did a jboss thread dump and found 146 threads with the word \"8009\" in it, but\n> according to Rainer's analysis only a handful (11 to be exact - I grepped\n> RUNNABLE threads only) of them are actually processing requests.\n> \n> Curiously enough, we did not seem to have this issue in our test environment\n> (with nearly identical configurations), even after intensive load testing.\n> \n> I'm not sure if this is something specific to our environment.  For the record:\n> \n> SunOS mega 5.10 Generic_127111-06 sun4v sparc SUNW,Sun-Fire-T200\n> \n> Server Version: Apache/2.2.6 (Unix) mod_jk/1.2.25\n> JK Version:     mod_jk/1.2.25\n> \n> Server version: Apache/2.2.6 (Unix)\n> Server built:   Jan 19 2008 22:12:59\n> Server's Module Magic Number: 20051115:5\n> Server loaded:  APR 1.2.11, APR-Util 1.2.10\n> Compiled using: APR 1.2.11, APR-Util 1.2.10\n> Architecture:   32-bit\n> Server MPM:     Worker\n>   threaded:     yes (fixed thread count)\n>     forked:     yes (variable process count)\n> Server compiled with....\n>  -D APACHE_MPM_DIR=\"server/mpm/worker\"\n>  -D APR_HAS_SENDFILE\n>  -D APR_HAS_MMAP\n>  -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled)\n>  -D APR_USE_FCNTL_SERIALIZE\n>  -D APR_USE_PTHREAD_SERIALIZE\n>  -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT\n>  -D APR_HAS_OTHER_CHILD\n>  -D AP_HAVE_RELIABLE_PIPED_LOGS\n>  -D DYNAMIC_MODULE_LIMIT=128\n>  -D HTTPD_ROOT=\"/usr/local/httpd-cls01\"\n>  -D SUEXEC_BIN=\"/usr/local/httpd-cls01/bin/suexec\"\n>  -D DEFAULT_SCOREBOARD=\"logs/apache_runtime_status\"\n>  -D DEFAULT_ERRORLOG=\"logs/error_log\"\n>  -D AP_TYPES_CONFIG_FILE=\"conf/mime.types\"\n>  -D SERVER_CONFIG_FILE=\"conf/httpd.conf\"\n> \n> \n> We are currently running in degraded mode due to a session replication issue,\n> so for the time being only one node is responding to requests.  Perhaps this is\n> part of the equation?\n> \n\n", "id": 115588, "time": "2008-04-14T06:29:06Z", "bug_id": 44454, "creation_time": "2008-04-14T06:29:06Z", "is_private": false}, {"count": 17, "tags": [], "text": "Hi,\n\nI wanted to know if there was any intent on fixing this issue as I have experienced EXACTLY the same symptoms on my production environment and unable to reproduce them in a test environment by simply throwing a load tester at it.\n\nI'm using:\nServer Version:\tApache/2.0.52 (CentOS)\nJK Version:\tmod_jk/1.2.26\nJBoss Version: 4.2.2 GA\n\nI have also set about every timeout I could find in the workers.properties documentation as well as the timeout on the AJP connector inside tomcat. \n\nI was incredibly happy to see this bug filed as every time I see a forum post with the same issue, someone is quick to respond with telling the poster to set all these timeout settings.\n\nThanks.", "is_private": false, "bug_id": 44454, "id": 121094, "time": "2008-09-30T08:19:21Z", "creator": "greenemj@gmail.com", "creation_time": "2008-09-30T08:19:21Z", "attachment_id": null}, {"count": 18, "tags": [], "bug_id": 44454, "text": "I too have been having this exact same issue in my PROD environment. I am using version 1.2.27 of the mod_jk and running IHS on Solaris. The symptoms are identical to the ones described in this bug. We have spent a good part of the last 3 weeks trying to address the issue of the inflated Busy count. \n\nIs there any plans to resolve this issue once and for all?\n\nThank you", "id": 128259, "time": "2009-06-24T21:12:53Z", "creator": "amit.x.yadav@jpmorgan.com", "creation_time": "2009-06-24T21:12:53Z", "is_private": false, "attachment_id": null}, {"count": 19, "tags": [], "bug_id": 44454, "is_private": false, "id": 137792, "creation_time": "2010-06-21T22:44:52Z", "time": "2010-06-21T22:44:52Z", "creator": "wrowe@apache.org", "text": "Studying this at length; the locking mechanism is not the issue.  fcntl() \nis likely the most reliable (not fastest) and flock(), which is not used by\ndefault, would be far less reliable as it isn't supported by nas solutions.\n\nThe faulty assumption that 'volatile' magically enables atomic math operations\non smb environments is the issue.  'int64' is not atomic on any platform, and\nit would appear that 'int' increments are acting appropriately for affected users,\nwhile their decrements are failing.\n\nAll operations on the scoreboard must be mutexed.", "attachment_id": null}, {"count": 20, "tags": [], "creator": "rainer.jung@kippdata.de", "attachment_id": null, "text": "Thanks for looking into this Bill!\n\nShould 32Bit operations be atomic? The busy counter could well be 32 Bits.\n\nFor most of the other pure statistics counters, like bytes transferred, total request number etc. I'm not to concerned about correctness of the numbers, but the busy count influences the work of the balancer.\n\nRegards,\n\nRainer", "id": 137794, "time": "2010-06-22T03:26:19Z", "bug_id": 44454, "creation_time": "2010-06-22T03:26:19Z", "is_private": false}, {"count": 21, "tags": [], "creator": "wrowe@apache.org", "attachment_id": null, "text": "Well, we are already misusing the phrase optlock vs real locking, so perhaps an\nadditional pedantic mode that always locks?\n\n32 bit math is not smp-safe on most architectures.  There was an apr_atomic\ninterface that illustrated that, if you would like to refer to the apr project\nsources.", "id": 137795, "time": "2010-06-22T03:44:22Z", "bug_id": 44454, "creation_time": "2010-06-22T03:44:22Z", "is_private": false}, {"count": 22, "tags": [], "bug_id": 44454, "attachment_id": null, "is_private": false, "id": 137810, "time": "2010-06-22T07:26:05Z", "creator": "sebb@apache.org", "creation_time": "2010-06-22T07:26:05Z", "text": "At the risk of stating the obvious:\n\n32 bit reads and writes are guaranteed atomic by the JVM.\n\nHowever, an arithmetic operation (e.g. increment) requires two operations:  read and then write, and another thread can write the field between the two atomic operations.\n\n64 bit reads and writes are not even guaranteed atomic. This was to allow for systems that did not have 64 bit operations, which therefore had to perform 2 off 32-bit operations. (Note: they probably are atomic on most modern systems, depending on the alignment).\n\nAdding volatile makes 64 bit reads and writes atomic, but does not protect multiple operations such as increment."}, {"count": 23, "tags": [], "bug_id": 44454, "attachment_id": null, "is_private": false, "id": 137811, "time": "2010-06-22T07:29:39Z", "creator": "markt@apache.org", "creation_time": "2010-06-22T07:29:39Z", "text": "(In reply to comment #22)\n> At the risk of stating the obvious:\n\nInteresting but irrelevant. This is httpd module code, not Java code."}, {"count": 24, "tags": [], "bug_id": 44454, "text": "Oops, sorry! Though a JVM is not involved here, increment still requires two separate memory operations.\n\nBTW, shared memory can behave very unexpectedly - e.g. writes by one CPU may appear out of order to another CPU unless appropriate instructions (memory barriers) are used. This is in addition to the problems caused by interleaved reads/writes, which can affect arithmetic operations potentially even on a single CPU system.", "id": 137815, "time": "2010-06-22T08:00:29Z", "creator": "sebb@apache.org", "creation_time": "2010-06-22T08:00:29Z", "is_private": false, "attachment_id": null}, {"count": 25, "tags": [], "creator": "joekislos@gmail.com", "is_private": false, "id": 141009, "creation_time": "2010-10-25T13:00:50Z", "time": "2010-10-25T13:00:50Z", "bug_id": 44454, "text": "(In reply to comment #21)\n> Well, we are already misusing the phrase optlock vs real locking, so perhaps an\n> additional pedantic mode that always locks?\n> \n> 32 bit math is not smp-safe on most architectures.  There was an apr_atomic\n> interface that illustrated that, if you would like to refer to the apr project\n> sources.\n\nWe are experiencing this problem with ModJK 1.2.28 and Tomcat 6.0.26 on Ubuntu Lucid 64bit.  I can confirm this is still very much an issue.  \n\nWhen we graceful apache, the counter seems to reset back to 0... however an apache graceful sets the counter back to 0 even if there *ARE* active connections (this is probably expected).  When those active connections finish up, the busy counter does not go negative, it stays at zero (which is correct).\n\nAre there plans on fixing this issue?  We have built into our tomcat init.d wrapper the ability to gracefully bounce a JVM without interrupting or loosing any pending apache requests, or requests that come in while the JVM is restarting.  Unfortunately this bug makes this impossible because we think there are active pending requests, when there are not.", "attachment_id": null}, {"count": 26, "tags": [], "creator": "zxf5420@126.com", "is_private": false, "id": 159191, "creation_time": "2012-05-21T06:57:45Z", "time": "2012-05-21T06:57:45Z", "bug_id": 44454, "text": "Created attachment 28807\nMod_jk scheduling errors", "attachment_id": 28807}, {"count": 27, "tags": [], "bug_id": 44454, "is_private": false, "text": "For anyone having this issue, you should know that mod_proxy can essentially be used as a functional replacement to mod_jk.\n\nWhile mod_proxy is theoretically less efficient than mod_jk, we found that the overhead added was minimal and of essentially no consequence as the apache-tomcat bridge was far from being the bottleneck.\n\nI had worked around this issue by moving to mod_proxy - if this avenue is available to you, it is worth exploring.", "id": 159200, "time": "2012-05-21T15:02:23Z", "creator": "toutatisdev@gmail.com", "creation_time": "2012-05-21T15:02:23Z", "attachment_id": null}, {"count": 28, "tags": [], "bug_id": 44454, "is_private": false, "text": "I added a warning to the docs in r1649182.\n\nThe only real solution would be the introduction of atomics to \"sync\" increments and decrements of the busy counter. We could use APR atomics for mod_jk and would have to look for Windows atomics for IIS. That would leave NSAPI open. Since it seems to be used only very infrequently, we could leave the problem as-is for NSAPI.", "id": 180136, "time": "2015-01-03T09:54:25Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2015-01-03T09:54:25Z", "attachment_id": null}, {"count": 29, "tags": [], "bug_id": 44454, "is_private": false, "text": "I have now changed the implementation of the busy counter to use atomics (see r1649306).\n\nThis is based on the builtin atomics on Windows, gcc atomics if gcc is used or APR atomics if APR is available. In other cases, still no atomics are used.\n\nSome smaller problems might still exist, e.g. when doing a web server restart it could be that the busy number might not be correct. I wasn't able to reproduce such behavior, but it might happen. Most of the cases for skewed numbers should be fixed by the atomics, so I'm closing this ticket now.", "id": 180139, "time": "2015-01-04T09:59:18Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2015-01-04T09:59:18Z", "attachment_id": null}, {"count": 30, "attachment_id": null, "bug_id": 44454, "is_private": false, "id": 180193, "time": "2015-01-06T19:40:07Z", "creator": "chris@christopherschultz.net", "creation_time": "2015-01-06T19:40:07Z", "tags": [], "text": "Should we add a WARN-level log message if the user requests \"busyness\" and the binary does not support atomic busyness-tracking?\n\nI think that's a reasonable warning to issue. We could just emit that log on startup... no need to spew logs repeatedly."}, {"count": 31, "tags": [], "creator": "rainer.jung@kippdata.de", "attachment_id": null, "text": "Warning added in r1650043:\n\n[Wed Jan 07 12:53:10.882 2015] [6976:1] [warn] init::jk_lb_worker.c (1908): Missing support for atomics: LB method 'busyness' not recommended", "id": 180204, "time": "2015-01-07T11:56:04Z", "bug_id": 44454, "creation_time": "2015-01-07T11:56:04Z", "is_private": false}]