[{"count": 0, "tags": [], "text": "Hello,\n\nMy system is weblogic 6.1 and solaris 5.8. The JDK is 1.3.1\n\nI use HSSF to generate an excel file that sizes 550kb (so a big file)\n\nIt works 2 times, the third there is an error \u2018java.lang.OutOfMemoryError: \nunable to create new native thread\u2019\n\nI've tried with the option -verbosegc to see the works of the garbage collector:\n\nThere is a need of lot of memory for HSSF work, but when the exception is \nraised, there is plenty of memory available. (In fact, I see that the used \nmemory goes from 9M to 25M for the generation of one file, go back to 9M after \nthe file generation)\n\n\nSo I suppose there is a memory leak (but in a \u2018native call\u2019?) and it may be \nlinked to the OS/JDK\n\nRegard from a Frenchman living in Belgium", "is_private": false, "bug_id": 21876, "id": 41485, "time": "2003-07-25T08:04:47Z", "creator": "fbernard167@altran-europe.be", "creation_time": "2003-07-25T08:04:47Z", "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 21876, "attachment_id": null, "is_private": false, "id": 41486, "time": "2003-07-25T08:43:56Z", "creator": "fbernard167@altran-europe.be", "creation_time": "2003-07-25T08:43:56Z", "text": "Changing the parameters -ms128m -mx128m has no effect."}, {"count": 2, "tags": [], "bug_id": 21876, "text": "now that is weird.  98% chance this has nothing to do with POI per se.  Who's JDK?", "id": 41499, "time": "2003-07-25T12:20:33Z", "creator": "poi-support@buni.org", "creation_time": "2003-07-25T12:20:33Z", "is_private": false, "attachment_id": null}, {"count": 3, "attachment_id": null, "creator": "fbernard167@altran-europe.be", "text": "The JDK is java version \"1.3.1_01\"\n\nI found a kind of turnaround with the option -Xss128k ==> To limit the thread \nstack size\n\nNow I can generate more (up to 10) but the pb remains", "id": 41501, "time": "2003-07-25T12:32:18Z", "bug_id": 21876, "creation_time": "2003-07-25T12:32:18Z", "tags": [], "is_private": false}, {"count": 4, "tags": [], "bug_id": 21876, "attachment_id": null, "is_private": false, "id": 41503, "time": "2003-07-25T12:41:15Z", "creator": "poi-support@buni.org", "creation_time": "2003-07-25T12:41:15Z", "text": "POI has some places where its recursive but it runs all in 1 thread.  My suspicion is that Weblogic \nhas sprung a leak and that you're just catching this in POI because its a big long running process.  \nYou'd eventually see it elsewhere.  In \"top\" does it show you're running close to your max memory?  \nIt could also be that its when its expanding the heap."}, {"count": 5, "tags": [], "bug_id": 21876, "is_private": false, "text": "Sorry I don't have top installed on my system, so I will use the solaris tools\n\nSo based on http://developers.sun.com/solaris/articles/prstat.html#fpcpu\n\nI have search for memory/threads leak:\n\nHere is the result:\n\n<----------------------------------------------------------->\nbash-2.03$ prstat -p 12936 15 >server.out\nbash-2.03$ tail -f server.out \n\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    144M   64M sleep   58    0   0:00.10 0.1% java/11\nTotal: 1 processes, 11 lwps, load averages: 1.16, 3.58, 2.27\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    144M   64M sleep   58    0   0:00.10 0.0% java/11\nTotal: 1 processes, 11 lwps, load averages: 0.90, 3.41, 2.24\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    144M   64M sleep   58    0   0:00.10 0.0% java/11\nTotal: 1 processes, 11 lwps, load averages: 0.70, 3.24, 2.20\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    144M   64M sleep   58    0   0:00.10 0.0% java/11\nTotal: 1 processes, 11 lwps, load averages: 0.55, 3.09, 2.16\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    145M   65M sleep   58    0   0:00.10 1.9% java/15\nTotal: 1 processes, 15 lwps, load averages: 0.47, 2.95, 2.13\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    146M   66M sleep   58    0   0:00.10 1.4% java/16\nTotal: 1 processes, 16 lwps, load averages: 0.45, 2.82, 2.10\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    147M   68M sleep   58    0   0:00.10 3.3% java/50\nTotal: 1 processes, 50 lwps, load averages: 0.39, 2.69, 2.07\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    151M   72M sleep   58    0   0:00.10 5.5% java/200\nTotal: 1 processes, 200 lwps, load averages: 0.41, 2.58, 2.04\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    153M   76M cpu0    38    0   0:00.00 6.5% java/291\nTotal: 1 processes, 291 lwps, load averages: 0.43, 2.48, 2.02\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    156M   80M sleep   58    0   0:00.10 6.7% java/410\nTotal: 1 processes, 410 lwps, load averages: 0.35, 2.36, 1.98\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    158M   84M sleep   58    0   0:00.10 7.3% java/516\nTotal: 1 processes, 516 lwps, load averages: 0.32, 2.26, 1.96\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    161M   88M sleep   58    0   0:00.10 6.9% java/625\nTotal: 1 processes, 625 lwps, load averages: 0.36, 2.17, 1.93\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    164M   93M cpu1    38    0   0:00.00 7.9% java/735\nTotal: 1 processes, 735 lwps, load averages: 0.36, 2.09, 1.91\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    167M   96M sleep   58    0   0:00.10 7.4% java/857\nTotal: 1 processes, 857 lwps, load averages: 0.43, 2.02, 1.89\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    171M  102M cpu2    32    0   0:00.00 8.0% java/993\nTotal: 1 processes, 993 lwps, load averages: 0.47, 1.95, 1.87\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    174M  106M cpu3    31    0   0:00.00 8.0% java/1110\nTotal: 1 processes, 1110 lwps, load averages: 0.40, 1.86, 1.84\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    176M  110M sleep   58    0   0:00.00 7.2% java/1198\nTotal: 1 processes, 1198 lwps, load averages: 0.44, 1.80, 1.82\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    179M  114M cpu2    38    0   0:00.00 7.8% java/1299\nTotal: 1 processes, 1299 lwps, load averages: 0.46, 1.74, 1.80\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    180M  116M sleep   58    0   0:00.10 6.0% java/1341\nTotal: 1 processes, 1341 lwps, load averages: 0.40, 1.66, 1.77\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    180M  116M sleep   58    0   0:00.10 2.8% java/1342\nTotal: 1 processes, 1342 lwps, load averages: 0.31, 1.58, 1.74\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  117M sleep   58    0   0:00.10 2.1% java/1345\nTotal: 1 processes, 1345 lwps, load averages: 0.26, 1.51, 1.71\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  117M cpu1    11    0   0:00.01 7.0% java/1345\nTotal: 1 processes, 1345 lwps, load averages: 0.30, 1.46, 1.69\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  118M sleep   58    0   0:00.10 8.5% java/1345\nTotal: 1 processes, 1345 lwps, load averages: 0.37, 1.41, 1.68\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  118M sleep   58    0   0:00.10 7.1% java/1345\nTotal: 1 processes, 1345 lwps, load averages: 0.38, 1.37, 1.65\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  118M sleep   58    0   0:00.10 3.6% java/1346\nTotal: 1 processes, 1346 lwps, load averages: 0.30, 1.30, 1.62\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  119M sleep   58    0   0:00.00 5.5% java/1346\nTotal: 1 processes, 1346 lwps, load averages: 0.34, 1.26, 1.61\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  119M sleep   58    0   0:00.10 6.6% java/1346\nTotal: 1 processes, 1346 lwps, load averages: 0.36, 1.22, 1.59\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    181M  119M sleep   58    0   0:00.10 3.4% java/1346\nTotal: 1 processes, 1346 lwps, load averages: 0.27, 1.16, 1.56\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    182M  119M sleep   58    0   0:00.10 1.8% java/1346\nTotal: 1 processes, 1346 lwps, load averages: 0.23, 1.10, 1.54\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    182M  119M sleep   58    0   0:00.10 0.9% java/1346\nTotal: 1 processes, 1346 lwps, load averages: 0.26, 1.07, 1.52\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    182M  119M sleep   58    0   0:00.10 5.0% java/1350\nTotal: 1 processes, 1350 lwps, load averages: 0.33, 1.05, 1.50\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    182M  120M sleep   58    0   0:00.10 7.6% java/1350\nTotal: 1 processes, 1350 lwps, load averages: 0.35, 1.02, 1.48\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    182M  120M sleep   58    0   0:00.10 8.5% java/1350\nTotal: 1 processes, 1350 lwps, load averages: 0.36, 0.98, 1.46\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    182M  120M sleep   58    0   0:00.10 5.7% java/1350\nTotal: 1 processes, 1350 lwps, load averages: 0.31, 0.94, 1.44\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    182M  121M run     51    0   0:00.15 7.1% java/1350\nTotal: 1 processes, 1350 lwps, load averages: 0.39, 0.93, 1.43\n<----------------------------------------------------------->\n\n(sorry for this borring copy paste)\n\nSo it is possible that the server application is leaking threads...(NLWP)\n\nNote: \n - from NLWP=15 to 1345: one file generation\n - from NLWP=1345 to 1350: big seach on database\n\neven if I don't understand why weblogic use so much threads for POI, I'm agree \nto say that my bug should be assigned to bea\n\nThank's for being so reactive\n", "id": 41505, "time": "2003-07-25T13:32:54Z", "creator": "fbernard167@altran-europe.be", "creation_time": "2003-07-25T13:32:54Z", "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 21876, "text": "hey it looks like it just hit the 128M, increase to 256M  BTW How much memory is on this system?  \nIs it dedicated?", "id": 41508, "time": "2003-07-25T13:57:37Z", "creator": "poi-support@buni.org", "creation_time": "2003-07-25T13:57:37Z", "is_private": false, "attachment_id": null}, {"count": 7, "attachment_id": null, "creator": "fbernard167@altran-europe.be", "text": "This server is our development platform and is not dedicated to my application.\n(Memory is increased when needed, actually 4G ram)\n\nThe log has been done for one file. The problem is that memory is not freed \nafter. e.g. Now I have:\n\n   PID USERNAME  SIZE   RSS STATE  PRI NICE      TIME  CPU PROCESS/NLWP       \n 12936 dcstar    285M  247M sleep   58    0   0:00.11 0.5% java/5324\n\n(and only 5 reports generated...)\n\nnote: the POI class is wrapped into a bean that has a 'request' scope\n", "id": 41511, "time": "2003-07-25T14:26:18Z", "bug_id": 21876, "creation_time": "2003-07-25T14:26:18Z", "tags": [], "is_private": false}, {"count": 8, "tags": [], "bug_id": 21876, "is_private": false, "text": "I think you're holding on to objects that you shouldn't.  I can't say for sure without seeing your \ncode, but something is leaking.  Can you attach the output from the garbage collector?  I want to \nmake sure its not POI.  (It shouldn't be if you're newing another up every time)", "id": 41514, "time": "2003-07-25T14:37:14Z", "creator": "poi-support@buni.org", "creation_time": "2003-07-25T14:37:14Z", "attachment_id": null}, {"count": 9, "attachment_id": null, "creator": "fbernard167@altran-europe.be", "is_private": false, "id": 41521, "time": "2003-07-25T15:13:04Z", "bug_id": 21876, "creation_time": "2003-07-25T15:13:04Z", "tags": [], "text": "the code for the exel generation is simple\n=> a collection is created, using data from different DAO class (connected to \noracle and corba)\n\nonce the collection is ready, i do the excel ==> no special layout, just 10 \nsheet, about 1000 record per sheet, 30 column\n\n    public HSSFWorkbook doTheExcel(JspWriter out) throws IOException {\n        HSSFSheet sheet;\n        HSSFWorkbook wb = new HSSFWorkbook();\n        int myint = 0;\n        Collection allDan =  new ReportBean().getDanList();\n        Dan oneDan;\n        Iterator iter = allDan.iterator();\n        out.flush();\n        while (iter.hasNext()){\n            myint++;\n            oneDan = (Dan) iter.next();\n            sheet = wb.createSheet(\"PortPassport_\"+ oneDan.getDan_name()\n+\"_\"+myint);\n            out.println(\"<li><b>DAN </b> \"+oneDan.getDan_name());\n            out.flush();\n            doTitle(sheet);\n            doContent(sheet,oneDan.getDan_id());\n            out.println(\"<font class=DarkGreen>  ok</font></li>\");\n        }\n        out.print(\"</MENU>\");\n        return wb;\n    }\n\n\nwith the -verbosegc option I have (part of the log)\n\n<----------------------------------------------------------->\n[GC 58111K->44763K(130560K), 0.0836426 secs]\n[GC 58267K->44651K(130560K), 0.0813831 secs]\n[GC 58155K->44658K(130560K), 0.0774749 secs]\n[GC 58162K->44685K(130560K), 0.0781372 secs]\n[GC 58189K->44830K(130560K), 0.0856084 secs]\n[GC 58334K->45019K(130560K), 0.0884809 secs]\n[GC 58521K->45088K(130560K), 0.0909737 secs]\n[GC 58592K->45019K(130560K), 0.0832764 secs]\n[GC 58521K->45141K(130560K), 0.0815629 secs]\n[GC 58645K->45066K(130560K), 0.0825176 secs]\n[GC 58568K->45186K(130560K), 0.0820842 secs]\n[GC 58690K->45389K(130560K), 0.0905469 secs]\n[GC 58893K->45406K(130560K), 0.0989173 secs]\n[GC 58910K->46923K(130560K), 0.1802498 secs]\n[GC 60427K->47006K(130560K), 0.1081365 secs]\n[GC 60510K->47044K(130560K), 0.0802481 secs]\n[GC 60548K->47101K(130560K), 0.0815010 secs]\n[GC 60605K->47348K(130560K), 0.0879132 secs]\n[GC 60852K->47362K(130560K), 0.0946540 secs]\n[GC 60866K->47326K(130560K), 0.0864550 secs]\n[GC 60830K->47536K(130560K), 0.0881128 secs]\n[GC 61040K->47588K(130560K), 0.0939117 secs]\n[GC 61092K->47468K(130560K), 0.0857027 secs]\n[GC 60971K->47679K(130560K), 0.0871233 secs]\n[GC 61183K->47576K(130560K), 0.0844342 secs]\n[GC 61079K->47752K(130560K), 0.0895072 secs]\n[GC 61256K->48867K(130560K), 0.1623272 secs]\n[GC 62371K->49246K(130560K), 0.1264462 secs]\n[GC 62750K->49342K(130560K), 0.1036315 secs]\n[GC 62845K->49507K(130560K), 0.0903454 secs]\n[GC 63011K->49474K(130560K), 0.0909878 secs]\n[GC 62978K->49757K(130560K), 0.0974931 secs]\n[GC 63261K->49760K(130560K), 0.0991084 secs]\n[GC 63264K->49649K(130560K), 0.0891965 secs]\n[GC 63153K->50081K(130560K), 0.1121554 secs]\n[GC 63585K->51016K(130560K), 0.1641777 secs]\n[GC 64520K->50955K(130560K), 0.1054016 secs]\n<Jul 25, 2003 10:34:01 AM CEST> <Error> <HTTP> <[WebAppServletContext\n(1102187,DCStarWebApp,/DCStarWebApp)] Servlet failed with Exception\njava.lang.OutOfMemoryError: unable to create new native thread\n\tat java.lang.Thread.start(Native Method)\n\tat org.apache.log4j.PropertyConfigurator.configureAndWatch\n(PropertyConfigurator.java:375)\n\tat org.apache.log4j.PropertyConfigurator.configureAndWatch\n(PropertyConfigurator.java:355)\n\tat mobistar.dcstar.tools.ErrorBean.<init>(ErrorBean.java:46)\n\tat jsp_servlet._Jsp.__errorpage_include._jspService\n(__errorpage_include.java:96)\n\tat weblogic.servlet.jsp.JspBase.service(JspBase.java:27)\n\tat weblogic.servlet.internal.ServletStubImpl.invokeServlet\n(ServletStubImpl.java:265)\n\tat weblogic.servlet.internal.ServletStubImpl.invokeServlet\n(ServletStubImpl.java:304)\n\tat weblogic.servlet.internal.ServletStubImpl.invokeServlet\n(ServletStubImpl.java:200)\n\tat weblogic.servlet.internal.RequestDispatcherImpl.include\n(RequestDispatcherImpl.java:482)\n\tat weblogic.servlet.internal.RequestDispatcherImpl.include\n(RequestDispatcherImpl.java:308)\n\tat weblogic.servlet.jsp.PageContextImpl.include\n(PageContextImpl.java:116)\n\tat weblogic.servlet.jsp.PageContextImpl.handlePageException\n(PageContextImpl.java:269)\n\tat jsp_servlet._Jsp.__popup_HSSF._jspService(__popup_HSSF.java:146)\n\tat weblogic.servlet.jsp.JspBase.service(JspBase.java:27)\n\tat weblogic.servlet.internal.ServletStubImpl.invokeServlet\n(ServletStubImpl.java:265)\n\tat weblogic.servlet.internal.ServletStubImpl.invokeServlet\n(ServletStubImpl.java:200)\n\tat weblogic.servlet.internal.WebAppServletContext.invokeServlet\n(WebAppServletContext.java:2546)\n\tat weblogic.servlet.internal.ServletRequestImpl.execute\n(ServletRequestImpl.java:2260)\n\tat weblogic.kernel.ExecuteThread.execute(ExecuteThread.java:139)\n\tat weblogic.kernel.ExecuteThread.run(ExecuteThread.java:120)\n> \n<----------------------------------------------------------->\n\nfor this test i have \"-ms128m -mx128m\" (but \"-ms256m -mx256m\" has the same \neffect, there is still free memory)\n\n\nan other part of the log file with \"-ms64m -mx64m\" to see the action of a full \nGC:\n\n<----------------------------------------------------------->\n[GC 57847K->51140K(65280K), 0.1227375 secs]\n[GC 57860K->51153K(65280K), 0.1218409 secs]\n[GC 57873K->51312K(65280K), 0.1386660 secs]\n[GC 58032K->51879K(65280K), 0.1718578 secs]\n[Full GC 58599K->21266K(65280K), 2.0990213 secs]\n[GC 27986K->21304K(65280K), 0.1174647 secs]\n[GC 28023K->21809K(65280K), 0.1361481 secs]\n[GC 28529K->21737K(65280K), 0.1250673 secs]\n[GC 28457K->22345K(65280K), 0.1266070 secs]\n[GC 29065K->22432K(65280K), 0.1251829 secs]\n[GC 29152K->22434K(65280K), 0.1254387 secs]\n[GC 29154K->22433K(65280K), 0.1226090 secs]\n[GC 29153K->22430K(65280K), 0.1269320 secs]\n[GC 28748K->24307K(65280K), 0.1471761 secs]\n[GC 31027K->25197K(65280K), 0.1567875 secs]\n[GC 31917K->25391K(65280K), 0.2083542 secs]\n[GC 32110K->25447K(65280K), 0.2917099 secs]\n[Full GC 28557K->13166K(65280K), 1.4535354 secs]\n<----------------------------------------------------------->\n\n(first Full GC is automatic, second is forced)"}, {"count": 10, "tags": [], "bug_id": 21876, "attachment_id": null, "is_private": false, "id": 46182, "time": "2003-10-23T21:04:35Z", "creator": "ian_springer@hp.com", "creation_time": "2003-10-23T21:04:35Z", "text": "It's possible you're hitting a max set in the kernel - ie, max threads per \nprocess. I believe BEA provides recommended kernel param settings somewhere on \ntheir website.\n\nIan\n"}, {"count": 11, "tags": [], "text": "then again, maybe not... the following post says that there is no such thing as \na per-process thread limit in Solaris..\nhttp://www.sunmanagers.org/pipermail/summaries/2002-February/002355.html\n\nmaybe you could play with some of the other threading-related JVM options. i \nfound this page that discusses them in detail:\nhttp://java.sun.com/docs/hotspot/threads/threads.html\n", "is_private": false, "id": 46185, "creator": "ian_springer@hp.com", "time": "2003-10-23T21:24:14Z", "bug_id": 21876, "creation_time": "2003-10-23T21:24:14Z", "attachment_id": null}, {"count": 12, "tags": [], "bug_id": 21876, "attachment_id": null, "is_private": false, "id": 75110, "time": "2005-05-20T11:47:59Z", "creator": "avik@apache.org", "creation_time": "2005-05-20T11:47:59Z", "text": "Doesnt look like a POI issue. POI does have memory issues, but this is not one\nof them, I think. "}]