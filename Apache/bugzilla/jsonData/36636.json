[{"count": 0, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "After load on our production mod_dav server increased, we saw sporadic\nnon-functioning of DAV requests (PROPFIND especially), while GET requests\nwould still work. Each DAV request would hereafter block a child. This would\nsometimes lead to effective DoS due to the\ncontinous stream of new incoming requests blocking more and more children.\nAfter long investigation of the problem, I am able to reproduce the bug, but\nonly using low-level\nflowreplay (application layer tcpdump replay tool, part of tcpreplay).\nI think the bug is caused by a blocking poll()-call (similar to bug 22030) after\na writev() call due to (missing TCP ACKs|some other network problem)\nThe PROPFIND causing it requests a large amount of data, so poll() blocks\nseveral times [Timeout] sec.\nBefore blocking, this child has allocated a filesystem lock\n\n10:53:01.096934 [ffffe410] fcntl64(9, F_SETLKW, {type=F_WRLCK, whence=SEEK_SET,\nstart=0, len=0}) = 0 <0.000010>\n\non /tmp/DavLock.dir, which is the file the other children (new DAV requests)\nrequire a lock for, too, hence blocking in the lock call:\n\n10:53:01.370153 [ffffe410] fcntl64(9, F_SETLKW, {type=F_WRLCK, whence=SEEK_SET,\nstart=0, len=0}) = 0 <716.870032>\n\nThe GET requests still work (when enough non-blocked children are still\navailable) since they do not require the /tmp/DavLock.dir fs-lock.\n\nThe part where the offending child is blocking poll() is straced here:\n\n[...]\n10:53:01.216048 [ffffe410] open(\"/srv/www/webdav/..........\", O_RDONLY) = -1\nENOENT (No such file or directory) <0.000014>\n10:53:01.216220 [ffffe410] writev(8, [{\"1f5e\\r\\n\", 6}, {\"<insert large amount of\ndata here>\"}, {\"\\r\\n\", 2}], 4) = 2487 <0.000011>\n10:53:01.216489 [ffffe410] poll([{fd=8, events=POLLOUT}], 1, 300000) = 0\n<299.956071>\n10:58:01.172688 [ffffe410] open(\"/srv/www/webdav/..........\", O_RDONLY) = -1\nENOENT (No such file or directory) <0.000023>\n[...]\n10:58:01.174614 [ffffe410] writev(8, [{\"1f60\\r\\n\", 6}, {\"<insert large amount of\ndata here>\"}, {\"\\r\\n\", 2}], 4) = -1 EAGAIN <0.000013>\n10:58:01.174957 [ffffe410] poll([{fd=8, events=POLLOUT}], 1, 300000) = 0\n<299.844216>\n11:03:01.019320 [ffffe410] open(\"/src/www/webdav/..........\", O_RDONLY) = -1\nENOENT (No such file or directory) <0.000015>\n[...]\n11:03:01.021422 [ffffe410] writev(8, [{\"1f67\\r\\n\", 6}, {\"<insert large amount of\ndata here>\"}, {\"\\r\\n\", 2}], 4) = -1 EAGAIN <0.000012>\n11:03:01.021711 [ffffe410] poll([{fd=8, events=POLLOUT,\nrevents=POLLERR|POLLHUP}], 1, 300000) = 1 <113.248743>\n11:04:54.270509 [ffffe410] writev(8, [{\"1f67\\r\\n\", 6}, {\"<insert large amount of\ndata here>\"}, {\"\\r\\n\", 2}], 4) = -1 EPIPE (Broken pipe) <0.000016>\n11:04:54.270812 [ffffe410] --- SIGPIPE (Broken pipe) @ 0 (0) ---\n[...]\n\nAfter this last blocking poll(), all writev()s are repeated similar to the last\none (\"1f67\"...) resulting in EPIPE and the server\nstarts working again.\n\nA backtrace on this process during a blocking poll() gives:\n\n#0  0xffffe410 in ?? ()\n#1  0xbfffc7d8 in ?? ()\n#2  0x000493e0 in ?? ()\n#3  0x00000001 in ?? ()\n#4  0x404207bd in poll () from /lib/tls/libc.so.6\n#5  0x402e9aad in apr_poll (aprset=0xbfffc860, num=1, nsds=0xbfffc85c,\ntimeout=Variable \"timeout\" is not available.\n    ) at poll.c:130\n#6  0x402e9e9f in apr_wait_for_io_or_timeout (f=Variable \"f\" is not available.\n    ) at waitio.c:54\n#7  0x402dfbee in apr_socket_sendv (sock=0x81bdb80, vec=0xbfffea58, nvec=3,\nlen=0xbfffc908) at sendrecv.c:208\n#8  0x402dfcf6 in apr_sendv (sock=0x81bdb80, vec=0xbfffea58, nvec=3,\nlen=0xbfffc908) at sendrecv.c:991\n#9  0x0807850a in writev_it_all (s=0x81bdb80, vec=0x493e1, nvec=1, len=8038,\nnbytes=0xbfffc988) at core.c:2914\n#10 0x08078fed in core_output_filter (f=0x81bdff8, b=0x81c7c58) at core.c:4247\n#11 0x0807236b in ap_pass_brigade (next=0x493e1, bb=0x1) at util_filter.c:512\n#12 0x08060ca8 in chunk_filter (f=0x81d9a90, b=0x81c7c58) at http_core.c:218\n#13 0x0807236b in ap_pass_brigade (next=0x493e1, bb=0x1) at util_filter.c:512\n#14 0x08075653 in ap_content_length_filter (f=0x81cdc60, b=0x81c7c58) at\nprotocol.c:1232\n#15 0x0807236b in ap_pass_brigade (next=0x493e1, bb=0x1) at util_filter.c:512\n#16 0x4002f7bb in apr_brigade_write (b=0x81c7c58, flush=0x80723a0\n<ap_filter_flush>, ctx=0x81cdc60,\n    str=0x82580a8 \"<lp1:getetag>\\\"2229f-1e5e-3f81de40\\\"</lp1:getetag>\\n\",\nnbyte=49) at apr_brigade.c:393\n#17 0x4002fa28 in apr_brigade_puts (bb=0x81c7c58, flush=0x80723a0\n<ap_filter_flush>, ctx=0x81cdc60,\n    str=0x82580a8 \"<lp1:getetag>\\\"2229f-1e5e-3f81de40\\\"</lp1:getetag>\\n\") at\napr_brigade.c:569\n#18 0x404c2be4 in dav_send_one_response (response=Variable \"response\" is not\navailable.\n    ) at mod_dav.c:464\n#19 0x404c2d73 in dav_stream_response (wres=Variable \"wres\" is not available.\n    ) at mod_dav.c:1100\n#20 0x404c439a in dav_propfind_walker (wres=0xbfffeeb4, calltype=1) at\nmod_dav.c:1917\n#21 0x404d7cb8 in dav_fs_walker (fsctx=Variable \"fsctx\" is not available.\n    ) at repos.c:1485\n#22 0x404d826e in dav_fs_internal_walk (params=Variable \"params\" is not available.\n    ) at repos.c:1729\n#23 0x404c7dd2 in dav_handler (r=0x81ccff0) at mod_dav.c:2045\n#24 0x080674eb in ap_run_handler (r=0x81ccff0) at config.c:152\n#25 0x08069f45 in ap_invoke_handler (r=0x81ccff0) at config.c:364\n#26 0x080656ef in ap_process_request (r=0x81ccff0) at http_request.c:249\n#27 0x08060e79 in ap_process_http_connection (c=0x81bdc58) at http_core.c:251\n#28 0x080702fb in ap_run_process_connection (c=0x81bdc58) at connection.c:43\n#29 0x080665f8 in child_main (child_num_arg=Variable \"child_num_arg\" is not\navailable.\n    ) at prefork.c:610\n#30 0x08066781 in make_child (s=Variable \"s\" is not available.\n    ) at prefork.c:704\n#31 0x08066811 in startup_children (number_to_start=3) at prefork.c:722\n#32 0x08066edd in ap_mpm_run (_pconf=0x809c0a8, plog=0x80c6150, s=0x809de58) at\nprefork.c:941\n#33 0x0806bffa in main (argc=3, argv=0xbffff504) at main.c:618\n\nPlease let me know if you need more information. I am sorry not being able to\nprovide an easy way to reproduce the bug, since it seems\nspecific to network effects. I can provide the .pcap dumps and part of the data\nused in the DAV, or, alternatively, test the\nbug with patched httpd versions.\n\nThank you for your efforts!\n\nRegards,\nArne Voigtlaender\n\nSystem-Info:\n------------\nTested on:Linux 2.6.7, 2.6.13 (Distro: SuSE SLES 9)\n          httpd 2.0.49, 2.0.53, 2.0.54\nHardware: 2x Xeon 3.4GHz, 5GB RAM, 8GB HDD,\n          2x Broadcom Corporation NetXtreme BCM5704S Gigabit Ethernet\n          (IBM Blade)\n\nReferences:\n-----------\nhttp://archives.free.net.ph/message/20050909.014740.64bfea69.en.html\nhttp://issues.apache.org/bugzilla/show_bug.cgi?id=22030", "id": 79862, "time": "2005-09-13T14:11:04Z", "creator": "arne@arne.net", "creation_time": "2005-09-13T14:11:04Z", "is_private": false}, {"count": 1, "tags": [], "bug_id": 36636, "attachment_id": null, "id": 79864, "time": "2005-09-13T14:34:31Z", "creator": "arne@arne.net", "creation_time": "2005-09-13T14:34:31Z", "is_private": false, "text": "The bug is similar/identical to bug 34221.\n\n"}, {"count": 2, "tags": [], "creator": "jorton@redhat.com", "attachment_id": 16394, "id": 79882, "time": "2005-09-13T16:09:21Z", "bug_id": 36636, "creation_time": "2005-09-13T16:09:21Z", "is_private": false, "text": "Created attachment 16394\nstop sending for aborted connections\n\nThe problem is that mod_dav never checks whether the connection has been\naborted and continues blindly trying to send.  Can you try this patch?"}, {"count": 3, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "Thank you for your quick response!\n\nThe patch does not fix the problem completely. As it seems, it does change the \nbehaviour of the problem, reduces the timeout a bit by trying less writev()s, \nbut still hangs around 600 seconds (Timeout 300) when trying to write data to \nthe socket:\n\n[...]\n19:02:19.105921 [ffffe410] writev(8, [{\"1f6a\\r\\n\", 6}, {\"<insert data>\"}, 76}, \n{\"\\r\\n\", 2}], 4) = 1777 <0.000012>\n19:02:19.106186 [ffffe410] poll([{fd=8, events=POLLOUT, revents=POLLOUT}], 1, \n300000) = 1 <0.010639>\n19:02:19.116867 [ffffe410] writev(8, [{\"<insert data>\"}, , 76}, {\"\\r\\n\", 2}], \n3) = 6273 <0.000013>\n19:02:19.119128 [ffffe410] writev(8, [{\"1f5e\\r\\n\", 6}, {\"<insert data>\"}, 49}, \n{\"\\r\\n\", 2}], 4) = 2487 <0.000013>\n19:02:19.119565 [ffffe410] poll([{fd=8, events=POLLOUT}], 1, 300000) = 0 \n<299.954921>\n[...]\n19:07:22.418883 [ffffe410] writev(8, [{\"1a7\\r\\n\", 5}, {\"<insert some data>\"},\n{\"\\r\\n\", 2}, {\"0\\r\\n\\r\\n\", 5}], 4) = -1 EAGAIN <0.0000\n19:07:22.419221 [ffffe410] poll([{fd=8, events=POLLOUT}], 1, 300000) = 0 \n<299.955070>\n19:12:22.374387 [ffffe410] write(7, \"172.17.225.123 - - [13/Sep/2005:19:02:18 \n+0200] \\\"PROPFIND /.......\", 160) = 160 <0.000021>\n19:12:22.374512 [ffffe410] close(8)     = 0 <0.000016>\n[...] \n\n(the stack backstrace remains as previously stated)\n\nI observed a correlation between number of CLOSE_WAIT connections and the \nbehaviour described earlier. That is, once the poll() call locks, the \nCLOSE_WAIT connections accumulate up to a number of 131 (when MaxClients is \nset to 3). In the time period before this happens, the server can deal well \nwith my test load (which is constant). But once it appears, CLOSE_WAIT \nconnections add up.\nIt looks like this is a deeper problem in apr_poll() or deeper, do you agree?\n\nThank you again!\n\nRegards,\nArne Voigtlaender", "id": 79896, "time": "2005-09-13T21:11:23Z", "creator": "arne@arne.net", "creation_time": "2005-09-13T21:11:23Z", "is_private": false}, {"count": 4, "tags": [], "bug_id": 36636, "attachment_id": null, "id": 79898, "time": "2005-09-13T21:36:03Z", "creator": "jorton@redhat.com", "creation_time": "2005-09-13T21:36:03Z", "is_private": false, "text": "Actually it is weird that you see mod_dav taking an exclusive lock on the lockdb\nfor a PROPFIND, if that's really what this is.  (\"p r->the_request\" in gdb\nsomewhere up the stack to confirm?)  That should be a read lock AFAIK.\n\n10:53:01.096934 [ffffe410] fcntl64(9, F_SETLKW, {type=F_WRLCK, whence=SEEK_SET,\nstart=0, len=0}) = 0 <0.000010>"}, {"count": 5, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "It is a write-lock, but setting it \"read-only\" does not fix the problem, I\nalready tried that.\n\nmodules/dav/main/mod_dav.c:1300:\n    /* open lock database, to report on supported lock properties */\n    /* ### should open read-only */\n    if ((err = dav_open_lockdb(r, 0, &lockdb)) != NULL) {\nmodules/dav/main/mod_dav.c:2005:\n    /* ### should open read-only */\n    if ((err = dav_open_lockdb(r, 0, &ctx.w.lockdb)) != NULL) {\n", "id": 79922, "time": "2005-09-14T09:49:41Z", "creator": "arne@arne.net", "creation_time": "2005-09-14T09:49:41Z", "is_private": false}, {"count": 6, "tags": [], "creator": "arne@arne.net", "attachment_id": null, "id": 79928, "time": "2005-09-14T10:59:02Z", "bug_id": 36636, "creation_time": "2005-09-14T10:59:02Z", "is_private": false, "text": "I can confirm that it is in fact the PROPFIND:\n\n#14 0x08075653 in ap_content_length_filter (f=0x11107ef0, b=0x105884f0) at\nprotocol.c:1232\n1232        return ap_pass_brigade(f->next, b);\n(gdb) p r->the_request\n$1 = 0x11107f48 \"PROPFIND /webdav/prod-umgebung/data/datenaustausch/beratung/\nHTTP/1.1\"\n\nThe connection, which is being served by this process is:\n\nProto Recv-Q Send-Q Local Address           Foreign Address         State      \ntcp        1  20440 172.17.223.146:80       172.17.225.123:43681    CLOSE_WAIT  \n\n"}, {"count": 7, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "Well, if mod_dav is changed to only use a read lock on the database, then it\nshould not block other children from trying to obtain read access and they\nshould be able to process PROPFIND requests.  What behaviour do you seen in that\ncase?  What system call do the other children block in?  Can you attach the\npatch you used, for reference?\n", "id": 79934, "time": "2005-09-14T11:59:57Z", "creator": "jorton@redhat.com", "creation_time": "2005-09-14T11:59:57Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 36636, "attachment_id": 16402, "text": "Created attachment 16402\nabort connection handling and readonly db-locking for mod_dav\n\nI tried this patch and the just the \"16394: stop sending for aborted\nconnections\" patch by itself, both cases still excibit the described behaviour.", "id": 79937, "time": "2005-09-14T12:32:34Z", "creator": "arne@arne.net", "creation_time": "2005-09-14T12:32:34Z", "is_private": false}, {"count": 9, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "Regarding the other behaviour, I am still investigating that. There are more\nreadlocks now, but it does acquire a writelock for the DavLockDB somewhere in\nthe code, I'm trying to find the place right now.\n\nYou are right, it decreases chances that the behaviour will reoccur as\ndescribed, but for one, there are additional writelocks on the DB (which might\nnot be necessary either) and also, shouldn't writev_it_all _not block_ no matter\nhow much data you push in?\n\nAs I see it, fixing mod_dav will reduce the chances that the problem will come\nabout (ideally only blocking one child instead of all of them if no write locks\nare required), but do you think it is possible to stop this one child from blocking?", "id": 79938, "time": "2005-09-14T12:43:08Z", "creator": "arne@arne.net", "creation_time": "2005-09-14T12:43:08Z", "is_private": false}, {"count": 10, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "I just went through the code in mod_dav.c and checked the cases in which the\nlockdb would be opened read/write. The cases which I found for\n(MOVE|COPY|PUT|MKCOL) seem reasonable, so I checked my packet dump and it seems\nthe problem is caused by:\n\none PROPFIND (depth:1 causes a 13MB response), this response is written into a\nbroken socket (since ACKs do not come in), => poll() locks up\n\nthe rest of the children is then locked up in the write-lock request (as straced\nabove) when doing MKCOL/PUTs. \n", "id": 79939, "time": "2005-09-14T12:50:52Z", "creator": "arne@arne.net", "creation_time": "2005-09-14T12:50:52Z", "is_private": false}, {"count": 11, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "OK, well that sounds like the expected behaviour.  To mitigate this you can\nreduce the Timeout setting to e.g. 10 or 20 seconds.  It would also be useful to\nadd checking in more places (e.g. core_output_filter) for c->aborted to prevent\nrepeated write timeouts.\n\nAlso it's possible that using a different database library for the lockdb might\nhelp - you could expirement with using apr_dbm_open_ex instead of apr_dbm_open\nin dav/fs/lock.c and specifying the database type.\n\nBut essentially this is a design limitation of the mod_dav_fs backend;\neverything has to serialize through a single lock database.  Other mod_dav\nbackends might not have this limitation, e.g. catacomb. \n(http://www.webdav.org/catacomb/)\n\n", "id": 79940, "time": "2005-09-14T13:01:50Z", "creator": "jorton@redhat.com", "creation_time": "2005-09-14T13:01:50Z", "is_private": false}, {"count": 12, "tags": [], "bug_id": 36636, "is_private": false, "text": "Thank you so much for your help! I put a check for f->c->aborted in\ncore.c:core_output_filter and thereby reduced the number of timed out writes to one.\nBut the core issue still remains. I am sending this prepared PROPFIND which\ncauses a large answer, and each request will just stop sending ACKs in the\nmiddle of the transaction. In about 1 in 10 cases (or so), the behaviour changes\nand the poll() locks, it is always with the same data to write, with the exact\nsame packets being sent by the client.\n\nA \"normal\" situation looks like this:\n20:52:09.407647 [ffffe410] writev(8, [...], 3) = 3686 <0.000013>\n20:52:09.409004 [ffffe410] writev(8, [{\"1f5e\\r\\n\", 6}, ...], 4) = 5074 <0.000014>\n20:52:09.409095 [ffffe410] poll([{fd=8, events=POLLOUT,\nrevents=POLLERR|POLLHUP}], 1, 300000) = 1 <0.246947>\n20:52:09.656085 [ffffe410] writev(8, [...], 3) = -1 EPIPE (Broken pipe) <0.000013>\n20:52:09.656163 [ffffe410] --- SIGPIPE (Broken pipe) @ 0 (0) ---\n\nThis I can strace all the time, but occasionally, it is this:\n\n20:57:26.463493 [ffffe410] writev(8, [...], 4) = 1800 <0.000014>\n20:57:26.463608 [ffffe410] poll([{fd=8, events=POLLOUT, revents=POLLOUT}], 1,\n300000) = 1 <0.011306>\n20:57:26.474956 [ffffe410] writev(8, [...], 3) = 6250 <0.000022>\n20:57:26.476328 [ffffe410] writev(8, [{\"1f5e\\r\\n\", 6},...], 4) = 2510 <0.000012>\n20:57:26.476424 [ffffe410] poll([{fd=8, events=POLLOUT}], 1, 300000) = 0\n<299.851111>\n\nIn the latter case, poll does not return the POLLERR. The connection is then in\nCLOSED_WAIT, after a while, it goes to LAST_ACK (while the send-Q is 20k).\nI read something about the behaviour on \nhttp://www.ussg.iu.edu/hypermail/linux/kernel/0407.3/0495.html\nbut I'm not sure if this is intended in this case. \n\nIs it the kernel or apache that is loosing track of the FD? Why is the kernel\nnot signalling the SIGPIPE / POLLERR?", "id": 79966, "time": "2005-09-14T21:32:37Z", "creator": "arne@arne.net", "creation_time": "2005-09-14T21:32:37Z", "attachment_id": null}, {"count": 13, "tags": [], "bug_id": 36636, "attachment_id": 16409, "id": 79967, "time": "2005-09-14T21:36:01Z", "creator": "arne@arne.net", "creation_time": "2005-09-14T21:36:01Z", "is_private": false, "text": "Created attachment 16409\ncore_output_filter patch that destroys bucket_brigades when connection is aborted\n\nhere is the mentioned patch for core_output_filter"}, {"count": 14, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "SIGPIPE (and I guess POLLERR) only happens if the client sends an RST and the\nconnection is forcibly reset.\n\nI'm not sure what *should* happen at LAST_ACK or CLOSE_WAIT in a poll for\nPOLLOUT.  Maybe nothing until that last ACK is received.  Maybe Jeff is\nlistening, he's the one with the TCP state diagram above his desk ;)\n\nw.r.t to patch, there is some funky (and somewhat annoying) logic at the end of\nthe core output filter which deliberately hides the write error from the caller\nso I'm not sure if this should be duplicated here and just return APR_SUCCESS\ninstead.", "id": 79968, "time": "2005-09-14T21:49:41Z", "creator": "jorton@redhat.com", "creation_time": "2005-09-14T21:49:41Z", "is_private": false}, {"count": 15, "tags": [], "bug_id": 36636, "is_private": false, "text": "Thanks for the remark on SIGPIPE (I know, I should have rtfm :)\nAnyway, when I block outgoing RST packets from my test machine so I can send my\npacket dump replay without interference from the host TCP/IP stack (since it\nuses a libndis userspace stack for replaying), the host stack does not reply\nwith a RST to attempts from the server that is trying to push the data to the\nclient [of the aborted replayed PROPFIND request]. \nThat lead to the ability to reproduce the problem in all cases.\nSince your patch to mod_dav (check for aborted) and my analogous patch for\ncore_output_filter reduce the number of writes to one, while the readonly\nDavLockDB patch further reduces the \"problem\" to serialized write-access, the\nremaining poll() call most likely conforms to the protocol.\nWe are reducing the timeout and looking into the proposed alternative backends\nfor mod_dav.\n\nThank you very much for your help in pinning down the problem!\n\nWhat are the chances these patches (or similar, improved ones) make it into\nmainstream Apache?\n", "id": 79979, "time": "2005-09-15T12:11:29Z", "creator": "arne@arne.net", "creation_time": "2005-09-15T12:11:29Z", "attachment_id": null}, {"text": "The abort handling can be tracked by bug 39605 - the mod_dav database locking\nissue can be tracked here.", "tags": [], "bug_id": 36636, "is_private": false, "count": 16, "id": 89288, "time": "2006-05-19T12:35:04Z", "creator": "jorton@redhat.com", "creation_time": "2006-05-19T12:35:04Z", "attachment_id": null}, {"count": 17, "attachment_id": null, "creator": "herlix@gmail.com", "is_private": false, "id": 110137, "time": "2007-11-01T12:34:35Z", "bug_id": 36636, "creation_time": "2007-11-01T12:34:35Z", "tags": [], "text": "Hello,\n\n    I'am use httpd-2.0.52 anda http2.0.59 with mod_dav, any news the fix bug?\n\nregards\n\nHernani"}, {"count": 18, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "Hello,\n\n    I'am use httpd-2.0.52 anda http2.0.59 with mod_dav, any news the fix bug?\n\nregards\n\nHernani", "id": 110138, "time": "2007-11-01T12:35:14Z", "creator": "herlix@gmail.com", "creation_time": "2007-11-01T12:35:14Z", "is_private": false}, {"count": 19, "attachment_id": 21173, "bug_id": 36636, "text": "Created attachment 21173\nchanges dav_method_propfind to lockdb only for readonly\n\nchanges dav_method_propfind to lockdb only for readonly, instead of exclusive\nlocking (propfind should be readonly)\n\nin our production environment we still find a problem with locking the db\nexclusive when the method propfind is used:\n\n#0  0xffffe410 in __kernel_vsyscall ()\n#1  0xb7cae443 in fcntl () from /lib/libpthread.so.0\n#2  0xb7d3cbe9 in apr_file_lock (thefile=0x816e440, type=1) at flock.c:46\n#3  0xb7d78bf1 in apr_sdbm_lock (db=0x816ef50, type=1) at sdbm_lock.c:50\n#4  0xb7d789e4 in apr_sdbm_open (db=0xbfdc0c98, file=0x80eb380 \"/tmp/DavLock\", \n\n    flags=1, perms=4095, p=0x814fb90) at sdbm.c:138\n#5  0xb7d799d8 in vt_sdbm_open (pdb=0xbfdc0cf8, \n    pathname=0x80eb380 \"/tmp/DavLock\", mode=1, perm=4095, pool=0x814fb90)\n    at apr_dbm_sdbm.c:114\n#6  0xb7d7929d in apr_dbm_open (pdb=0xbfdc0cf8, \n    pathname=0x80eb380 \"/tmp/DavLock\", mode=1, perm=4095, pool=0x814fb90)\n    at apr_dbm.c:86\n#7  0xb7b10f66 in dav_dbm_open_direct (p=0x814fb90, \n    pathname=0x80eb380 \"/tmp/DavLock\", ro=1, pdb=0x816e40c) at dbm.c:131\n#8  0xb7b11540 in dav_fs_really_open_lockdb (lockdb=0x816e3f0) at lock.c:305\n#9  0xb7b30bee in dav_get_resource_state (r=0x814fbc8, resource=0x816e3c8)\n    at util_lock.c:715\n#10 0xb7b2b844 in dav_handler (r=0x814fbc8) at mod_dav.c:904\n#11 0x08068959 in ap_run_handler (r=0x814fbc8) at config.c:152\n#12 0x0806b941 in ap_invoke_handler (r=0x814fbc8) at config.c:364\n#13 0x08066428 in ap_process_request (r=0x814fbc8) at http_request.c:249\n#14 0x08061138 in ap_process_http_connection (c=0x8143500) at http_core.c:253\n#15 0x08072d29 in ap_run_process_connection (c=0x8143500) at connection.c:43\n#16 0x0806731a in child_main (child_num_arg=<value optimized out>)\n    at prefork.c:610\n#17 0x080675a3 in make_child (s=0x80a1e88, slot=299) at prefork.c:704\n#18 0x08067dd0 in ap_mpm_run (_pconf=0x80a00a8, plog=0x80cc158, s=0x80a1e88)\n    at prefork.c:839\n#19 0x0806e37e in main (argc=134865184, argv=0x812d280) at main.c:623\n\n\nmany processes get stuck as \"SendingReply\" because one process had locked\nexclusive the DavLockDB and has not released, and it leads to a DoS behave.\nThis patch is Arne's simplified patch, just taking care about the DavLockDB.\n\nit was tested against apache 2.0.59 and 2.0.61, and it seems to affect 2.2.6 as\nwell.", "id": 110972, "time": "2007-11-21T20:45:34Z", "creator": "wfelipe@gmail.com", "creation_time": "2007-11-21T20:45:34Z", "tags": [], "is_private": false}, {"count": 20, "tags": [], "bug_id": 36636, "attachment_id": null, "text": "I can confirm this problem still exists in the 2.4.x releases and in 2.5/trunk (r1734315).\n\nA workaround is to use mod_buffer to buffer the entire PROPFIND response before sending it to the client:\n\n    <If \"toupper(%{REQUEST_METHOD}) == 'PROPFIND'\" >\n        SetOutputFilter BUFFER\n    </If>\n\nOf course this increases memory use and request latency, but it does prevent a single client on a slow connection from locking up the entire DAV repository.", "id": 189477, "time": "2016-03-14T20:07:19Z", "creator": "wiml@omnigroup.com", "creation_time": "2016-03-14T20:07:19Z", "is_private": false}]