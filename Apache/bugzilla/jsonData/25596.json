[{"count": 0, "tags": [], "creator": "sgabriel@brainfuse.com", "is_private": false, "text": "When reloading a web application in production. Tomcat reports that the \napplication is unavailable for a brief period of time. the reason for \nreloading the application is the jsp pages are precompiled and the application \nreloadable property is set to false. So on every migration we are forced to \nreload the application which leads to the problem described above. \nWhen using reloadable propery in development this doesn't happen even after \nrecompiling all class files. Some of the pages on the server are pretty \ncomplex pages that requires a lot of users input and when the application is \nunavailable all the user's work is simply lost and they have to start over \nagain. \nthere are no error messages in the log files and no complains, the application \nuses struts and the reloading is using the Ant tasks shipped with Tomcat. \n\nPlease advice what is the best configuration, may be reloadable is not so bad \nafter all. \nP.S. the application doesn't take this long to load we are talking about \ncouple of seconds at the most.", "id": 49258, "time": "2003-12-17T17:21:22Z", "bug_id": 25596, "creation_time": "2003-12-17T17:21:22Z", "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 25596, "attachment_id": null, "text": "This is a known bug/regression over TC 4, see this thread for more info:\n\nhttp://marc.theaimsgroup.com/?t=107104070600004&r=1&w=2", "id": 49260, "time": "2003-12-17T17:44:50Z", "creator": "drees76@gmail.com", "creation_time": "2003-12-17T17:44:50Z", "is_private": false}, {"count": 2, "tags": [], "creator": "sgabriel@brainfuse.com", "attachment_id": null, "id": 49264, "time": "2003-12-17T19:19:31Z", "bug_id": 25596, "creation_time": "2003-12-17T19:19:31Z", "is_private": false, "text": "Actually, even on TC 4.1.29 I was getting this problem, and one of the main \nreason to upgrade was to avoid this from happening, thought that it might have \nbeen fixed. I was wrong according to the new implementation TC 5 is even worse \nform that prospective.\nI would appreciate if somebody would point out where the code for this \nbehaviour is located I would like to patch it myself if I can."}, {"count": 3, "tags": [], "creator": "drees76@gmail.com", "is_private": false, "text": "In TC 4 when I reload a context through the manager, any requests to that\ncontext don't come back until the context has been reloaded.  I wonder why it's\ndifferent for you?\n\nAnyway, let me post what Remy said about fixing this issue:\n\n\"To solve the recurrent bugs and problems caused by reload and simplify \nthe code, reload was replaced with the stop/start sequence. So \nunfortunately the side effect is that this cannot work anymore, and we \nneed a more generic mechanism to 'wait'.\"\n\nI'm not sure exactly where to look but I would start with the Manager webapp and\nsee where it ties into the container.  From there, you'll have to look at how\nrequests get processed and see if there's a way to get new requests to a context\nto block if a context is reloading.", "id": 49265, "time": "2003-12-17T19:28:50Z", "bug_id": 25596, "creation_time": "2003-12-17T19:28:50Z", "attachment_id": null}, {"count": 4, "tags": [], "creator": "remm@apache.org", "is_private": false, "text": "This works as intended, and this will not be implemented. I belive \"waiting\" is\nactually a bad feature. More robust failover mechanisms should be used instead.", "id": 49280, "time": "2003-12-17T21:46:53Z", "bug_id": 25596, "creation_time": "2003-12-17T21:46:53Z", "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 25596, "attachment_id": null, "text": "This works as intended, and this will not be implemented. I belive \"waiting\" is\nactually a bad feature. More robust failover mechanisms should be used instead.", "id": 49282, "time": "2003-12-17T21:52:19Z", "creator": "remm@apache.org", "creation_time": "2003-12-17T21:52:19Z", "is_private": false}, {"count": 6, "tags": [], "creator": "sgabriel@brainfuse.com", "is_private": false, "text": "If I set the Context to be reloadable, doesn't that involve a Stop and Start \nsequence as well. Just for your info Remmy most of the commercial webserver \nprovides a way for waiting while reload. When a client is making a POST \nrequest like submitting a billing request (with credit card or a lot of data ) \nand gets the application is unavailable, they don't know to click refresh to \ntry again and if they know they will be prompted with the message if they are \nsure to repost the request.\n If TC maintains the session between reloads why can't wait be one of the \nfeatures of TC as well. It might not be as easy to develop as Stop/Start but \nit is definetly worth the effort.\n\nAnyways I would appreciate if you can post the location to patch TC with the \nolder implementation if possible.  ", "id": 49519, "time": "2003-12-22T07:29:52Z", "bug_id": 25596, "creation_time": "2003-12-22T07:29:52Z", "attachment_id": null}, {"count": 7, "tags": [], "bug_id": 25596, "attachment_id": null, "text": "I looked at some of the code and just wanted to ask you guys if there is any \nproblem with replacing the ContainerBase.invoke implementation with something \nalong the lines of.\n    public final void invoke(Request request, Response response)\n        throws IOException, ServletException {\n        while ( getPaused () ) \n        { \n           synchronized(this){ wait(1000);}\n        }\n        pipeline.invoke(request, response);\n\n    }\n\nAccording to the StandardContext implementation the reload method call \nsetPaused(true ) \nsetPaused(false)\nduring the process of reloading.\n\nPlease let me know of the side effects for this solution that you might \nanticipate so that I might be aware of it. \nThanks", "id": 49520, "time": "2003-12-22T07:50:05Z", "creator": "sgabriel@brainfuse.com", "creation_time": "2003-12-22T07:50:05Z", "is_private": false}, {"count": 8, "tags": [], "creator": "sgabriel@brainfuse.com", "attachment_id": null, "id": 49522, "time": "2003-12-22T07:56:49Z", "bug_id": 25596, "creation_time": "2003-12-22T07:56:49Z", "is_private": false, "text": "I apologize for the previous implementation I didn't notice that the paused \nflag is a private variable in the StandardContext, may be the solution for me \nwould be to change the invoke method in the ContainerBase to be not final and \noverride the method with \n\n public final void invoke(Request request, Response response)\n        throws IOException, ServletException {\n        while ( getPaused () ) \n        { \n           synchronized(this){ wait(1000);}\n        }\n        super.invoke(request, response);\n\n    }\nI understand that changing a method not to be final is a direct violation of \nthe Tomcat structure and API but this patch is only for my version and I just \nwanted to know if this is something that you guys would find some other \nproblem that I am not aware of in this solution.\nThanks Again\n"}, {"count": 9, "tags": [], "creator": "remm@apache.org", "is_private": false, "text": "I would be glad if you tested your proposed changes before submitting patches.\nBTW, your proposed change will likely not work.", "id": 49528, "time": "2003-12-22T09:29:32Z", "bug_id": 25596, "creation_time": "2003-12-22T09:29:32Z", "attachment_id": null}, {"count": 10, "tags": [], "creator": "yoavs@computer.org", "is_private": false, "text": "Waiting seems sub-optimal at best: what if the restart takes a long time, e.g. \nthe context does a bunch of caching upon startup?  It's very easy to make a \nwebapp start in more than one minute, by which point the browser is likely to \ngive up on the request regardless of what the server is doing.  A patch would \nbe interesting only if it's optional and not hard-coded into the connector (I \nfor one would not put such waiting behavior in a production system, but then \nagain I also don't have the habit of restarting production webapps while anyone \nis using them).", "id": 49857, "time": "2003-12-30T17:02:29Z", "bug_id": 25596, "creation_time": "2003-12-30T17:02:29Z", "attachment_id": null}, {"count": 11, "tags": [], "creator": "drees76@gmail.com", "attachment_id": null, "id": 50349, "time": "2004-01-10T02:00:15Z", "bug_id": 25596, "creation_time": "2004-01-10T02:00:15Z", "is_private": false, "text": "The deploy function of the manager has the pause parameter which allows the\nadmin to choose whether or not to process requests or not.  What are the chances\nof having a similar parameter added to the reload function?"}, {"count": 12, "tags": [], "bug_id": 25596, "attachment_id": null, "text": "The \"pause\" attribute doesn't exist, as I realized this feature was a terrible\nidea. The documentation has been fixed since. Because this would be difficult to\nimplement, and isn't useful in real production cases, this will not be\naddressed. Instead, I'll tweak things a bit to improve cluster management.", "id": 50359, "time": "2004-01-10T16:01:56Z", "creator": "remm@apache.org", "creation_time": "2004-01-10T16:01:56Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 25596, "text": "Remmy, \nI would greatly appreciate if you can let us know of the downsides of \nimplementing the wait behaviour. You have been mentioning since the start of \nthis bug that it is such a bad theory. \nAgain I can't see why this would be a problem I understand that some apps \nwould take a long time to start like cocoon for example but some other \napplications won't need that sort of thing. Just a few hundred or even a \nthousand classes that can be loaded in a less than 10 seconds on a good \nmultiprocessor machine. And unless you rewrite the session managment to use \nJDBC managment, clustering this way will never work. that ofcourse in addition \nto the extra hardware that needs to maintain the healthness of the cluster and \nso forth. \n", "count": 13, "id": 50360, "time": "2004-01-10T18:10:51Z", "creator": "sgabriel@brainfuse.com", "creation_time": "2004-01-10T18:10:51Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 25596, "text": "The problem is that the \"wait\" is inside the Catalina pipeline. So all\nconnections in a waiting state will eat up a processing thread with all the\nassociated memory. So if you have a medium sized server, this will be a big\nproblem. It would seem evident that any kind of continuous availability would\nrequire at least two servers with clustering. Since the behavior was on by\ndefault, with no way of disabling it, this could have been an issue for large\nserver.\nOf course, it will work great for a toy server.\nAlso, by default, the HTTP connection will timeout after 20s, so for a big\nwebapp, connections will end up being dropped.\n\nThis feature could possibly be reimplemented evetually, but it will definitely\nhave to behave differently, so that it is actually usable even on real\nproduction servers. This is not for the immediate future.", "count": 14, "id": 50446, "time": "2004-01-13T09:39:18Z", "creator": "remm@apache.org", "creation_time": "2004-01-13T09:39:18Z", "is_private": false}, {"count": 15, "tags": [], "bug_id": 25596, "attachment_id": null, "text": "Remy, I would be happy with your last response (use clustering if not dropping\nrequests across application reloads is important), but unfortunately this does\nnot always work with Tomcat 5 as it is now.  Currently the documented method to\ncluster Tomcat is to setup multiple Tomcats behind Apache using mod_jk. \nHowever, mod_jk does not know when a Tomcat context is reloading any better than\nthe HTTP connector so you end up with Tomcat returning \"HTTP/1.1 400 No Host\nmatches server name <servername>\" and dropping requests.  It takes some extra\nwork to reload without dropping requests.\n\nThe work around I have worked out to gracefully reload a node without dropping\nrequests I have found at this point is as follows:\n * Enable session replication / clustering\n * Load balance Tomcat under Apache using JK\n * Modify JK config (workers.properties) and remove the Tomcat node to be reloaded\n * Gracefully restart Apache (requests will stop going to the Tomcat node to be\nreloaded\n * Reload Tomcat node\n * Re-enable Tomcat node in JK config\n * Gracefully restart Apache\n\nIf we are sticking with the WONTFIX on this issue (I did see you post some\nfuture ideas a few days ago about Tomcat 5 refactoring which may cover this\nissue) I could write up a Tomcat/JK clustering HOW-TO which details the method I\noutlined above for addition to the current Tomcat documentation.", "id": 50927, "time": "2004-01-22T01:20:01Z", "creator": "drees76@gmail.com", "creation_time": "2004-01-22T01:20:01Z", "is_private": false}, {"count": 16, "tags": [], "creator": "hans.schmid@einsurance.de", "attachment_id": null, "id": 50939, "time": "2004-01-22T06:55:31Z", "bug_id": 25596, "creation_time": "2004-01-22T06:55:31Z", "is_private": false, "text": "Just a comment:\n\nWe are doing exactly this 'linking in a different worker.properties' with\na different load factor and gracefully restart Apaches.\n\nWorks perfect but:\nMake sure to apply the latest patches (cvs) to mod_jk 1.2.5 which address the\nlogfile file handle leak. Without this patch we get fork errors in Apache 1.3. \n Apache runs out of file handles (Solaris). Each graceful restart ears up 2 file\nhandles per vhost otherwise.\n\nHope this helps\n\nHans"}, {"count": 17, "tags": [], "creator": "drees76@gmail.com", "is_private": false, "text": "Hans, looks like the file descriptor leak only affects Apache 1.3, that explains\nwhy I didn't notice during my testing of web-app reloading today as I am using\nApache 2.0.  Using lsof doesn't reveal any extra file descriptors in use,\neither.  Thanks for the heads up (and apologies for getting a bit off-topic).", "id": 50944, "time": "2004-01-22T08:08:37Z", "bug_id": 25596, "creation_time": "2004-01-22T08:08:37Z", "attachment_id": null}, {"count": 18, "tags": [], "bug_id": 25596, "attachment_id": null, "text": "For anyone following this bug, but not following the dev list, it looks like\nRemy may attempt to fix this issue by implementing pause/resume commands in the\nConnector/ProtocolHandler.\n\nhttp://marc.theaimsgroup.com/?l=tomcat-dev&m=107487246528039&w=2", "id": 51062, "time": "2004-01-23T17:29:40Z", "creator": "drees76@gmail.com", "creation_time": "2004-01-23T17:29:40Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 25596, "text": "No, this is not related (this is global to the Tomcat instance). The change is\nmeant to enable doing updates on a cluster node in a clean way.", "count": 19, "id": 51065, "time": "2004-01-23T17:36:08Z", "creator": "remm@apache.org", "creation_time": "2004-01-23T17:36:08Z", "is_private": false}, {"count": 20, "tags": [], "creator": "drees76@gmail.com", "attachment_id": null, "id": 51066, "time": "2004-01-23T17:41:28Z", "bug_id": 25596, "creation_time": "2004-01-23T17:41:28Z", "is_private": false, "text": "Thanks for the input, Remy."}, {"count": 21, "tags": [], "creator": "sgabriel@brainfuse.com", "attachment_id": null, "id": 51508, "time": "2004-01-30T22:06:05Z", "bug_id": 25596, "creation_time": "2004-01-30T22:06:05Z", "is_private": false, "text": "Hi All, \nI was trying to create this patch that I mentioned to let the context wait \nwhile loading the context. I put a new attribute called \nwaitWhileReload=true/false. The problem is when I finally was able to build \nthe new classes and implemention. Tomcat was still using the old \nimplementation. \n\nIn my changes, the invoke method on the ContainerBase became non final and I \nchanged the one in the StandardContext to be final to still be able to use the \ninlining ability of the hotspot. The problem is when looking at how the new \nversion of tomcat services the request I found out that non of my changes took \neffect. I was able to verify this by creating a dummy jsp page that throws an \nexception and look at the stacktrace that is ofcourse after I gave up adding \ndebugging messages that kept not appearing. Here is the stack trace from my \nnewly compiled TC. \njava.lang.NullPointerException: \n\tat org.apache.jsp.includes.errorpage_jsp._jspService\n(errorpage_jsp.java:41)\n\tat org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:133)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:856)\n\tat org.apache.jasper.servlet.JspServletWrapper.service\n(JspServletWrapper.java:311)\n\tat org.apache.jasper.servlet.JspServlet.serviceJspFile\n(JspServlet.java:301)\n\tat org.apache.jasper.servlet.JspServlet.service(JspServlet.java:248)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:856)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter\n(ApplicationFilterChain.java:284)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter\n(ApplicationFilterChain.java:204)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke\n(StandardWrapperValve.java:256)\n\tat org.apache.catalina.core.StandardValveContext.invokeNext\n(StandardValveContext.java:151)\n\tat org.apache.catalina.core.StandardPipeline.invoke\n(StandardPipeline.java:564)\n\tat org.apache.catalina.core.StandardContextValve.invokeInternal\n(StandardContextValve.java:245)\n\tat org.apache.catalina.core.StandardContextValve.invoke\n(StandardContextValve.java:199)\n\tat org.apache.catalina.core.StandardValveContext.invokeNext\n(StandardValveContext.java:151)\n\tat org.apache.catalina.core.StandardPipeline.invoke\n(StandardPipeline.java:564)\n\tat org.apache.catalina.core.StandardHostValve.invoke\n(StandardHostValve.java:195)\n\tat org.apache.catalina.core.StandardValveContext.invokeNext\n(StandardValveContext.java:151)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke\n(ErrorReportValve.java:164)\n\tat org.apache.catalina.core.StandardValveContext.invokeNext\n(StandardValveContext.java:149)\n\tat org.apache.catalina.core.StandardPipeline.invoke\n(StandardPipeline.java:564)\n\tat org.apache.catalina.core.StandardEngineValve.invoke\n(StandardEngineValve.java:156)\n\tat org.apache.catalina.core.StandardValveContext.invokeNext\n(StandardValveContext.java:151)\n\tat org.apache.catalina.core.StandardPipeline.invoke\n(StandardPipeline.java:564)\n\tat org.apache.catalina.core.ContainerBase.invoke\n(ContainerBase.java:972)\n\tat org.apache.coyote.tomcat5.CoyoteAdapter.service\n(CoyoteAdapter.java:211)\n\tat org.apache.coyote.http11.Http11Processor.process\n(Http11Processor.java:805)\n\tat \norg.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.processConnecti\non(Http11Protocol.java:696)\n\tat org.apache.tomcat.util.net.TcpWorkerThread.runIt\n(PoolTcpEndpoint.java:605)\n\tat org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run\n(ThreadPool.java:677)\n\tat java.lang.Thread.run(Thread.java:534)\n\nNote: Ofcourse I verified the jar files are the right ones, inthe right \nlocation server\\lib that there are no other jar files in the class path, that \nthe class files in the jar files actually has the latest timestamp. I first \ntried to copy catalina.jar to server\\lib when that failed I copied everything. \nWhen that failed I ignored my original installation and used the one that was \nnewly built. \n\nI also tried to explicitly define the Context in the StandardContext. I am not \nsure what else I can do. \nPlease HELP !!!!!!!!\n"}, {"count": 22, "tags": [], "bug_id": 25596, "attachment_id": null, "text": "Any body has anythoughts on patching tomcat problem that I am having. I would \nappreciate any help on that matter.", "id": 51542, "time": "2004-02-01T09:15:29Z", "creator": "sgabriel@brainfuse.com", "creation_time": "2004-02-01T09:15:29Z", "is_private": false}]