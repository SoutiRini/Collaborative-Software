[{"count": 0, "tags": [], "creator": "vicentesalvador@netscape.net", "attachment_id": null, "is_private": false, "id": 24852, "time": "2002-10-18T09:51:36Z", "bug_id": 13759, "creation_time": "2002-10-18T09:51:36Z", "text": "I've just seen this with Linux OS, but this maybe can be at any platform.\n\nAfter some hours working, some threads (1 or 2) of Tomcat begins to spend 100%\nCPU. Tomcat respond to new request but very very slowlly.\n\nHere I attach the thread trace of the threads spending 100% CPU:\n\nCPU1 states:  0,0% user,  7,0% system,  0,0% nice, 92,5% idle\nMem:  1029620K av,  762008K used,  267612K free,       0K shrd,  157844K buff\nSwap: 1060248K av,       0K used, 1060248K free                  219684K cached\n$<5>$<3>$<2>$<2>PID to kill: $<2>ill \n  PID USER     PRI  NI  SIZE  RSS SHARE STAT %CPU %MEM   TIME COMMAND\n20060 root      15   0  297M 297M 67604 R    99,9 29,5 135:44 java\n  764 jas       15   0  1096 1096   812 R     7,0  0,1   0:07 top\n    1 root       8   0   504  504   440 S     0,0  0,0   0:07 init\n    2 root       8   0     0    0     0 SW    0,0  0,0   0:00 keventd\n ...\n \n \n\n\"Thread-13\" daemon prio=1 tid=0x0x8d053b0 nid=0x4e5c runnable\n[bc3ff000..bc3ff8ac]       \n        at\norg.apache.coyote.http11.InternalInputBuffer.fill(InternalInputBuffer.java:777)\n        at\norg.apache.coyote.http11.InternalInputBuffer$InputStreamInputBuffer.doRead(InternalInputBuffer.java:807)\n        at\norg.apache.coyote.http11.filters.IdentityInputFilter.end(IdentityInputFilter.java:203)\n        at\norg.apache.coyote.http11.InternalInputBuffer.endRequest(InternalInputBuffer.java:398)\n        at\norg.apache.coyote.http11.Http11Processor.process(Http11Processor..java:418)\n        at\norg.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.processConnection(Http11Protocol.java:380)\n        at\norg.apache.tomcat.util.net.TcpWorkerThread.runIt(PoolTcpEndpoint..java:508)    \n        at\norg.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:518)\n        at java.lang.Thread.run(Thread.java:536)"}, {"count": 1, "tags": [], "bug_id": 13759, "attachment_id": null, "id": 24859, "time": "2002-10-18T12:53:16Z", "creator": "remm@apache.org", "creation_time": "2002-10-18T12:53:16Z", "is_private": false, "text": "This line is a blocking I/O call, so I don't see how it would hang there, unless\nthe call returns 0 (but I don't think it's supposed to), in which case a loop\ncondition might occur.\nYou'll have to debug this further, as the report doesn't allow to reproduce it\nnor isolate any particular problem."}, {"count": 2, "tags": [], "text": "OK I suppose that the I/O call should not return 0, but, If it returns 0, the\nsystem hangs up.\n\nMaybe is a JDK bug or a Linux bug, but I think we must prevent the Tomcat hangup\nexiting the while ig the call returns 0. Remmy, Don't you think so?\n\nSo, Should I or you reopen the bug?\n", "attachment_id": null, "bug_id": 13759, "id": 25454, "time": "2002-10-31T23:23:47Z", "creator": "vicentesalvador@netscape.net", "creation_time": "2002-10-31T23:23:47Z", "is_private": false}, {"count": 3, "tags": [], "text": "The 0 case did worry me.  It should mean that no imput is available (e.g. a \nslow network connection), but I don't know why it should stay in that state.  \nIt would be nice to be able to find out the underlying cause.\n\nIt should be possible to add a counter to limit the max-tries on recieving a 0 \nresponse, and, possibly add a Thread.sleep(1).  This would at least allow the \nother Tomcat threads to be usable while this one is waiting.", "is_private": false, "bug_id": 13759, "id": 25459, "time": "2002-11-01T02:57:54Z", "creator": "william.barker@wilshire.com", "creation_time": "2002-11-01T02:57:54Z", "attachment_id": null}, {"count": 4, "tags": [], "text": "I don't know if:\n- this is actually causing the bug, and if just disconnecting fixes it (the\nfeedback given is cryptic)\n- having a 0 result is a normal result (if it's not we should disconnect,\notherwise, we add a sleep and a max retry)", "attachment_id": null, "bug_id": 13759, "id": 25660, "time": "2002-11-05T09:09:15Z", "creator": "remm@apache.org", "creation_time": "2002-11-05T09:09:15Z", "is_private": false}, {"count": 5, "tags": [], "text": "On JDK Api Docs, about InputStream.read():\n\nIf len is zero, then no bytes are read and 0 is returned; otherwise, there is an\nattempt to read at least one byte. If no byte is available because the stream is\nat end of file, the value -1 is returned; otherwise, at least one byte is read\nand stored into b. \n\nSo a 0 and a -1 can be returned by read() and it means that there is not more\ndata available, so I think we just should disconnect.\n\nSurelly this is causing the bug, because if 0 or -1 is returned, then lastValid\nvariable is not updated and doRead stands calling fill() forever.\n\nNow I will reopen the bug... If you don't agree, feel free to resolve it again!!!", "attachment_id": null, "bug_id": 13759, "id": 25671, "time": "2002-11-05T11:33:22Z", "creator": "vicentesalvador@netscape.net", "creation_time": "2002-11-05T11:33:22Z", "is_private": false}, {"count": 6, "tags": [], "text": "That's incorrect, as len is never 0. The javadocs explicitely said that the call\nwill block until at least one byte is read.\nAlso, I'm not convinced there's a problem, as the only major problem reported\nwith Coyote HTTP/1.1 has been the sever socket dying (apparently because of bugs\nin the VM network code; workarounds similar to those in 4.0.x were added in\n4.1.14 to fix that).", "is_private": false, "id": 25672, "creator": "remm@apache.org", "time": "2002-11-05T11:45:12Z", "bug_id": 13759, "creation_time": "2002-11-05T11:45:12Z", "attachment_id": null}, {"count": 7, "tags": [], "text": "I cannot reproduce this. If there actually was a bug, the change introduced in\nTomcat 4.1.14 will fix it.", "attachment_id": null, "bug_id": 13759, "id": 26373, "time": "2002-11-15T13:37:21Z", "creator": "remm@apache.org", "creation_time": "2002-11-15T13:37:21Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 13759, "attachment_id": null, "id": 36440, "time": "2003-05-01T13:45:22Z", "creator": "rfelske@kmart.com", "creation_time": "2003-05-01T13:45:22Z", "is_private": false, "text": "I am running Tomcat 4.1.18. When the Tomcat server is up (no activity through \nthe server) for a long period of time the server task will end up utilizing \nall the cpu available. The task needs to be \"killed\" to correct. The server \nstop script doesn't do anything to stop the runaway task. This is running on \nan HP server at level HP-11i. Please contact me to collect whatever trace or \ndebug data that may be required. "}, {"count": 9, "attachment_id": null, "bug_id": 13759, "is_private": false, "id": 36443, "time": "2003-05-01T14:05:06Z", "creator": "funkman@joedog.org", "creation_time": "2003-05-01T14:05:06Z", "tags": [], "text": "Please upgrade your connectors (and/or) tomcat. There was an issue at one time\nwhere an invalid POST with an invalid content length did cause an infinite loop.\nIf that is still indeed the case (for you), it is very easy to reproduce. For\nexample:\n\ntelnet myhost 80\nPOST /foo.jsp HTTP/1.1\nHost: myhost:80\nContent-length: 12\nfoo=bar\n^Kill telnet session before sending all 12 bytes of content. Doing this in the\npast forced a similar error (which was fixed) for me.\n\nIf the previous task reproduces your issue - just upgrade and all will be solved."}, {"count": 10, "tags": [], "creator": "remm@apache.org", "is_private": false, "text": "You'll have to provide a test case, instructions on how to reproduce, or a\npatch, or otherwise this report is invalid (please do not reopen the report\nunless you can provide any of the previous instructions - and no, starting\nTomcat and leaving it running for a while does not do that for me).", "id": 36446, "time": "2003-05-01T14:21:04Z", "bug_id": 13759, "creation_time": "2003-05-01T14:21:04Z", "attachment_id": null}]