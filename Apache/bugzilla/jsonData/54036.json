[{"count": 0, "tags": [], "text": "Hi, since when centos 6 went out I've tryed to switch some of my server from centos 5.x to 6.x I currently own 13 servers with tomcat installed ,\nthse servers have a very high load, and contain only 1 servlet never reloaded/unloaded which print out static content (no cookie/session)  i use the compiled JNI (with apr ) , initially everythin was 1 servlet inside tomcat, now i have directly embedded tomcat into the servlet, \nthe issue is:\nwhen i have tomcat (jdk from 1.6.x  to 1.7.x doesn't change) and the load become a bit intense (about 600 req/sec ) the currentThreadsBusy become very high till maxes out (220 currentThreadsBusy) and stay at that value, seems that threads have some issue in closing ( i've inserted an atomic timer in servlets to monitor if these remains up, and cuncurrent executions calculated by this timer remain between 1 or 2 !!) and that's right because cpu usage is at 0.3%! \n\nsame identical configuration in a server with centos 5.x run fine with about 15 threads busy serving about 8k requests/second with cpu usage of about 35% (even if cuncurrent servlet in execution are usually 1 or 2 here too), seems that tomcat have some issue detaching the thread, i supposed that was my fault of some configuration (even if i'm not so dumb on that ) the ni found that this guy have the same problem :\nhttp://efreedom.com/Question/1-7296648/Tomcat-Centos-CurrentThreadsBusy-Issue\n\nno one seems to have a solution, initially i thought that was because centos 6 was pretty recent so maybe the issue would be fixed in earlyer releases of tomcat, but we are at 7.0.32 and the problem persist..\n\nwhen i have time i want to try to help in developement of tomcat (i'm learning a bit of his internal syntax while working on it..  ) but at this moment i don't even have time to sleep.. nor to provide some easily reproducible test case , also because this need a bit of load from different sources, but if you have suggestions i can try to implements and let you know if works , best i can do now is to attach thread dump of the current bugged execution (with all threads stuck here\n\n\nat org.apache.tomcat.jni.Socket.recvbb(Native Method)\n\tat org.apache.coyote.http11.InternalAprInputBuffer.fill(InternalAprInputBuffer.java:575)\n\tat org.apache.coyote.http11.InternalAprInputBuffer.parseRequestLine(InternalAprInputBuffer.java:134)\n\n\n\nplease let me know.. at this moment i'm again going to reinstall centos 5.x on that server and give up, because i need this server up and working today..\n but anyway this issues must be fixed.", "attachment_id": null, "bug_id": 54036, "id": 162866, "time": "2012-10-22T13:52:15Z", "creator": "vigotab@gmail.com", "creation_time": "2012-10-22T13:52:15Z", "is_private": false}, {"count": 1, "tags": [], "creator": "vigotab@gmail.com", "attachment_id": 29506, "id": 162867, "time": "2012-10-22T13:54:00Z", "bug_id": 54036, "creation_time": "2012-10-22T13:54:00Z", "is_private": false, "text": "Created attachment 29506\nthread dump\n\nthread dump of tomcat when providing the issue described ( incredibly high currentThreadsBusy ) which causes unusable tomcat installation"}, {"count": 2, "attachment_id": null, "creator": "vigotab@gmail.com", "text": "after further investigation seems that the problem is really at this point:\n\nat org.apache.tomcat.jni.Socket.recvbb(Native Method)\n\tat org.apache.coyote.http11.InternalAprInputBuffer.fill(InternalAprInputBuffer.java:575)\n\neach threads hang at this position till timeout and timeout on each connections i just wonder why this doesn't happen in centos 5.x and instead is bugged in centos 6.x probably some EOT/EOF signal didn't pass through APR/JNI ?", "id": 162870, "time": "2012-10-22T14:26:14Z", "bug_id": 54036, "creation_time": "2012-10-22T14:26:14Z", "tags": [], "is_private": false}, {"count": 3, "tags": [], "bug_id": 54036, "attachment_id": null, "text": "Changing component.", "id": 162871, "time": "2012-10-22T14:37:21Z", "creator": "markt@apache.org", "creation_time": "2012-10-22T14:37:21Z", "is_private": false}, {"count": 4, "tags": [], "creator": "vigotab@gmail.com", "text": "Hi Mark do you mean to change connector?\n\ni've tryed changing to BIO but the problem persist..\n\n\ni have 220 threads blocked at this point:\n\n\tat java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:150)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:121)\n\tat org.apache.coyote.http11.InternalInputBuffer.fill(InternalInputBuffer.java:516)\n\tat org.apache.coyote.http11.InternalInputBuffer.fill(InternalInputBuffer.java:501)\n\ni have no other solution than change os for tomcat server, but anyway having tomcat UNCOMPATIBLE (because this bug make the system very susceptible to even small DDoS ) with the new CENTOS >= 6 is a BIG ISSUE i think.\n\n, apache benchmark didn't show the issue in bot APR and BIO , ( keepalive is off (value=1) but the problem show up only when lot of external ip connect) maybe this is due on how the system handle tcp connections closing at lower levels ?", "id": 162873, "time": "2012-10-22T15:09:45Z", "bug_id": 54036, "creation_time": "2012-10-22T15:09:45Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 54036, "is_private": false, "text": "i understood sorry u meant that you have changed location of the report (i'm not very used to send bug reports) no problem :)", "id": 162874, "time": "2012-10-22T15:14:12Z", "creator": "vigotab@gmail.com", "creation_time": "2012-10-22T15:14:12Z", "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 54036, "is_private": false, "id": 162876, "creation_time": "2012-10-22T15:29:04Z", "time": "2012-10-22T15:29:04Z", "creator": "markt@apache.org", "text": "If this occurs across multiple connectors then this is not an APR/native bug. Moving back.\n\nPlease provide a reproducible test case. Without one, this will get closed as INVALID.", "attachment_id": null}, {"count": 7, "tags": [], "bug_id": 54036, "is_private": false, "text": "you are the good tomcat programmer , how i can write a reproducible case if you cannot reproduce the volume of requests i'm getting (these are TCP connections not  udp so i don't know how these can be simulated as coming from multiple different sources (maybe with something like playing with NAT in iptables) ) i am not the only one who has this issue, is someone use tomcat live with centos 6 and have more than 100 visit /seconds should have faced this issue too... the test case doesn't need to be built because happens always on centos >= 6\n(and probably on RHEL but i haven't tested before) , threads didn't close at the end of connections but at the socket timeout, so if you have high connection rate from different sources the thread pool become exhausted soon..\n\nif you know how to reproduce these multiple connections i'm almost sure that also the HelloWorld servlet would works as test case in centos 6", "id": 162877, "time": "2012-10-22T15:46:29Z", "creator": "vigotab@gmail.com", "creation_time": "2012-10-22T15:46:29Z", "attachment_id": null}, {"count": 8, "tags": [], "creator": "bugzilla@pidster.com", "attachment_id": null, "id": 162883, "time": "2012-10-22T18:07:20Z", "bug_id": 54036, "creation_time": "2012-10-22T18:07:20Z", "is_private": false, "text": "Which versions of APR and Java are installed on the Centos 5 and 6 systems?"}, {"attachment_id": null, "tags": [], "bug_id": 54036, "text": "i've used for bot centos 5 and 6, \n\ntomcat-native-1.1.24\nand \napr-1.4.6\n*all tested environments are 64 bit \n\nbut as i've tested yesterday this issue happens also with bio connectors\n\nthe problem anyway exists since some months i have never got a working tomcat in centos 6 (and maybe in fedora/RH lastest releases, i don't have tested there but should be almost the same),\nand because centos  is one of the top used server distro i think that this issue should be treated as critical\ni think that people which try to use tomcat in centos into a server with a quite high load, see the resource usage and think that tomcat is slow\nwhich is not! they don't fill a bug report because they don't think is a but (but instead it is because threads and connections doesn't close when tcp/ip transmission end)\n\nnow i have reinstalled again centos 5.8 on that server (i'm stuck to centos 5 :( ) and the server is replying at about 4k connections / seconds with \n8 cuncurrentthreadsBusy and 19 connectionCount (using APR) everything it's exactly the same as previously tested in centos 6 (which was unable to go over 200 connections/seconds  without beign slow because threads count would always max out )\nsame sysctl.conf, firewall disabled , same tomcat version and same servlet (servlet is compiled locally then rsync'ed ) \n\ni think that someone should take a look at this issue, if i have a bit of time in next months i'll try to setup iptables to try to forward multiple packets to virtual machine\nwith centos 5 and 6, simulating multiple ip connections (if u know a better way to do that let me know ) anyway if someone is running tomcat on centos 6 with a busy site\nand don't have this problem tell there and you can say then that it's me that i'm wrong, \nelseway i'm pretty sure that there is a very important issue there  --> indipendent of servlet served <-- that should need attention :)\n\nthank you,\nFrancesco", "count": 9, "id": 162890, "time": "2012-10-23T08:50:29Z", "creator": "vigotab@gmail.com", "creation_time": "2012-10-23T08:50:29Z", "is_private": false}, {"count": 10, "tags": [], "text": "this error happened with java version since 1.6.xx to 1.7.xx last attempt of yesterday has been done using the lastest  7u9-b05 ,  are months that i always try with newer software available to get tomcat working correctly on centos 6 but nothing at the end i have always to giveup and reinstall centos 5 and with same identical releases of software everything works correctly... the problem here i think is that if you have never tryed with tomcat & centos 5 you think that this is just a problem of performance but i can't believe that in centos 6 threads & connections remain opened this IS a bug, performance serving small static content (to multiple destinations, apache benchmark tests doesn't show this issue) are 100x worse in centos 6 and i mean 100x!! == 10 000% !\n\nobviously if you have a huge servlet which take 1 second to serve just 1 request you may be not seeing this issue because the bottleneck is not so evident so...", "is_private": false, "bug_id": 54036, "id": 162891, "time": "2012-10-23T08:59:59Z", "creator": "vigotab@gmail.com", "creation_time": "2012-10-23T08:59:59Z", "attachment_id": null}, {"count": 11, "tags": [], "bug_id": 54036, "attachment_id": null, "is_private": false, "id": 162904, "time": "2012-10-23T21:22:24Z", "creator": "bugzilla@pidster.com", "creation_time": "2012-10-23T21:22:24Z", "text": "Creating a load test that sends 100s of requests per second isn't so hard.\n\nA test case that reproduces a particular outcome is the only way for the Tomcat community to analyse and fix an issue."}, {"count": 12, "tags": [], "text": "The thread dump shows that you have a large number of new connections that are not sending any data. Hence, since BIO and APR/native both block during the reading of the request line, the threads will remain blocked until the read times out of the client sends more data.\n\nYou can see this by examining the stack trace. For example:\n\n\"http-apr-80-exec-96\" - Thread t@137\n   java.lang.Thread.State: RUNNABLE\n\tat org.apache.tomcat.jni.Socket.recvbb(Native Method)\n\tat org.apache.coyote.http11.InternalAprInputBuffer.fill(InternalAprInputBuffer.java:575)\n\tat org.apache.coyote.http11.InternalAprInputBuffer.parseRequestLine(InternalAprInputBuffer.java:134)\n\tat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:927)\n\nprocess(AbstractHttp11Processor.java:927)\nThis is when the processor triggers the read of the request line. The parameter it passes in (keptAlive) indicates is this is the first request received on the connection (keptAlive will be false) or a subsequent request (keptAlive will be true). It is important for this issue to know if keptAlive is true or false.\n\nparseRequestLine(InternalAprInputBuffer.java:134)\nThis is the reading data from the socket for the request line and parsing it. Since we are at line 134 we know useAvailableData == false (else the if block starting on line 131 would have been executed and the method would have returned false to the caller. If useAvailableData == false then keptAlive == false. This tells us that this is the first request on the connection.\n\nfill(InternalAprInputBuffer.java:575)\nThis is the InputBuffer attempting to read enough (actually any) data into the buffer so it can be parsed as a request line. It is blocking as there is no data to read.\n\n\nPutting everything together we know that:\n- this thread is processing a new connection from a client\n- the connection has been established but no data has been sent\n- Tomcat is waiting for data to arrive so it can parse the request line\n\n\nIf you have lots of clients doing this then you have some broken clients. Switching to the NIO connector may help since it is non-blocking while reading the request line and request headers (all connectors block while reading request bodies).\n\n\nI have no idea what it is between your CentOS 5 and CentOS 6 installations that is triggering this issue. It may be related to defer accept but still, the clients are misbehaving.\n\nThere is no Tomcat bug here. If you need further assistance, the Tomcat users mailing list is the place to seek help.", "is_private": false, "id": 162955, "creator": "markt@apache.org", "time": "2012-10-26T11:47:20Z", "bug_id": 54036, "creation_time": "2012-10-26T11:47:20Z", "attachment_id": null}, {"count": 13, "tags": [], "creator": "lovewill@naver.com", "text": "Hi, Francesco?\nI have a similar problem as you reported above. I'm using CentOS 6.3 And Tomcat 6.0.35. If you figured out this problem already, could you let me know how to? If you give me some advise I'll be very pleased!!! Please feel free to email me at 'lovewill@naver.com'. Thank you in advance. :)", "id": 166803, "time": "2013-04-24T07:14:39Z", "bug_id": 54036, "creation_time": "2013-04-24T07:14:39Z", "is_private": false, "attachment_id": null}]