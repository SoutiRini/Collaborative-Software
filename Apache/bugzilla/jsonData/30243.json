[{"count": 0, "tags": [], "bug_id": 30243, "is_private": false, "text": "The content cache doesn't appear to be caching successfully. Every time I \naccess content Extended Store attempt to recache the content.  I noticed a \nlong time ago on the dev list that someone mentioned that this wasn't working \ncorrecly(msg #06671).  Was this ever fixed? If it hasn't, here is my fix to \nthe put method of ByteSizeLimitedObjectCache which fixes the problem for me.\n\n\n        public Object put(Object key, Object value, long byteSize) {\n            // is it too big to be cached?\n            if (byteSize > maxByteSizePerEntry || byteSize > maxByteSize) {\n                if (loggingEnabled) {\n                    logger.log(txId + \" for '\" + key + \"' is too big to be \ncached\", logChannel, Logger.DEBUG);\n                }\n                Object oldValue = get(key);\n                // invalidate previous entry if present\n                // XXX this relies on an implementation detail in \nTxLRUByteCache.put\n                // removal from delete cache must be done before trying to add \nto\n                // change cache using this method; if not our undoing will be \npartly\n                // undone (again) in TxLRUObjectCache.put\n                invalidate(key);\n                return oldValue;\n            } else {\n                // be sure to return allocated bytes before readding\n                //freeBytes(key);\n\n                // ok, we decided to cache this entry, make room for it\n                for (int i = 0; globalSize + byteSize > maxByteSize && i < \nMAX_FREEING_TRIES; i++) {\n                    if (loggingEnabled) {\n                        logger.log(\n                            txId\n                                + \" for '\"\n                                + key\n                                + \"' needs \"\n                                + Long.toString(globalSize + byteSize - \nmaxByteSize)\n                                + \" bytes more to be cached. Freeing bytes!\",\n                            logChannel,\n                            Logger.DEBUG);\n                    }\n                \t// this will call back processRemovedLRU and will thus \nfree bytes\n                \tremoveLRU();\n\t\t\t\t}\n            }\n            // was this successful?\n            if (globalSize + byteSize <= maxByteSize) {\n                shadowSizes.put(key, new Long(byteSize));\n                globalSize += byteSize;\n                return super.put(key, value);\n            } else {\n                Object oldValue = get(key);\n                invalidate(key);\n                return oldValue;\n            }\n        }", "id": 60934, "time": "2004-07-21T20:34:01Z", "creator": "tara.talbott@pnl.gov", "creation_time": "2004-07-21T20:34:01Z", "attachment_id": null}, {"count": 1, "attachment_id": null, "bug_id": 30243, "is_private": false, "id": 60935, "time": "2004-07-21T20:57:30Z", "creator": "ozeigermann@apache.org", "creation_time": "2004-07-21T20:57:30Z", "tags": [], "text": "Could you explain your patch, please? The only part I do understand is\n\n            if (globalSize + byteSize <= maxByteSize) {\n\ninstead of\n\n            if (globalSize + byteSize > maxByteSize) {\n\nwhich really seems to be a bug, but most certainly it is nonsense to comment out\nfreeing byte counts of the resource that maybe get overwritten by the new put:\n\n                //freeBytes(key);\n\nAm I wrong?"}, {"count": 2, "attachment_id": null, "bug_id": 30243, "is_private": false, "id": 64214, "time": "2004-09-25T20:51:15Z", "creator": "ozeigermann@apache.org", "creation_time": "2004-09-25T20:51:15Z", "tags": [], "text": "This has been fixed, hasn't it?"}]