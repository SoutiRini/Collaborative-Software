[{"count": 0, "tags": [], "bug_id": 55171, "text": "Created attachment 30512\nfull thread stack dump\n\nI have a websocket test which performs some stress testing over an echo example server running on Tomcat 8. \nMy test is creating up to 200 websocket connections and perform some message sending/receiving. At some point of time after some test restarts, the server stops to respond. I have restarted the test with a simple single socket connection and the server was still not responding. I checked the threads stacks at the server side and it seems all of the Tomcat worker therads were blocked. \nAttaching the full stack trace of all threads.  \n\nIn the server's logs file I can see only several exceptions like this:\nJul 01, 2013 11:34:52 AM org.apache.tomcat.websocket.server.WsHttpUpgradeHandler destroy\nSEVERE: Failed to close WebConnection while destroying the WebSocket HttpUpgradeHandler\njava.io.IOException: Broken pipe\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:50)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:94)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:450)\n\tat org.apache.tomcat.util.net.SecureNioChannel.flush(SecureNioChannel.java:135)\n\tat org.apache.tomcat.util.net.SecureNioChannel.close(SecureNioChannel.java:385)\n\tat org.apache.coyote.http11.upgrade.NioServletInputStream.doClose(NioServletInputStream.java:107)\n\tat org.apache.coyote.http11.upgrade.AbstractServletInputStream.close(AbstractServletInputStream.java:128)\n\tat org.apache.coyote.http11.upgrade.AbstractProcessor.close(AbstractProcessor.java:59)\n\tat org.apache.tomcat.websocket.server.WsHttpUpgradeHandler.destroy(WsHttpUpgradeHandler.java:137)\n\tat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:684)\n\tat org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1584)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:789)\n\n\n\n\nNote: My test setup includes proxy and loadbalancer in the middle. In some cases the loadbalancer closes websocket connections which is seen as connection reset by the server and the client.", "id": 168176, "time": "2013-07-01T11:52:32Z", "creator": "s.boshev@gmail.com", "creation_time": "2013-07-01T11:52:32Z", "is_private": false, "attachment_id": 30512}, {"count": 1, "tags": [], "bug_id": 55171, "attachment_id": null, "text": "Thanks for the report and the stack trace.\n\nIt appears that there are multiple issues here. So far I have identified:\n1. Upgraded connection not marked as upgraded early enough and poller events are processed as if they were for a new connection.\n2. Connections closed by the client are not recognised as being closed by the WebSocket implementation.\n\nI have a fix for 1 that I'll commit shortly. I'm still looking into 2.", "id": 168229, "time": "2013-07-02T12:00:16Z", "creator": "markt@apache.org", "creation_time": "2013-07-02T12:00:16Z", "is_private": false}, {"count": 2, "tags": [], "bug_id": 55171, "attachment_id": null, "text": "I've tracked down the likely cause of 2 and have committed a fix to trunk.\n\nIt would be useful to know if the issue is now resolved when running your load test.", "id": 168232, "time": "2013-07-02T13:28:50Z", "creator": "markt@apache.org", "creation_time": "2013-07-02T13:28:50Z", "is_private": false}, {"count": 3, "tags": [], "bug_id": 55171, "attachment_id": null, "text": "I have found a way to reproduce this issue reliably with my tests.\n\nI will test the fixes and will provide feedback tomorrow.", "id": 168233, "time": "2013-07-02T13:56:16Z", "creator": "s.boshev@gmail.com", "creation_time": "2013-07-02T13:56:16Z", "is_private": false}, {"count": 4, "tags": [], "bug_id": 55171, "attachment_id": 30530, "text": "Created attachment 30530\nthread stack dump 2\n\nI was able to reproduce again. Attaching the new tread stack dump as there is some difference.", "id": 168257, "time": "2013-07-03T11:58:55Z", "creator": "s.boshev@gmail.com", "creation_time": "2013-07-03T11:58:55Z", "is_private": false}, {"count": 5, "tags": [], "bug_id": 55171, "attachment_id": null, "text": "Looking into stacktrace it seems that http* threads are blocked on a Future.get() call after an attempt to send close message to the peeras a result of an exception.\nShould we try to have the \"future\" get version with the timeout? Sending close as a result of an exception is an error situation anyway.", "id": 168264, "time": "2013-07-04T08:18:11Z", "creator": "nickytd@gmail.com", "creation_time": "2013-07-04T08:18:11Z", "is_private": false}, {"count": 6, "tags": [], "text": "(In reply to Niki Dokovski from comment #5)\n> Should we try to have the \"future\" get version with the timeout? Sending\n> close as a result of an exception is an error situation anyway.\n\nThat is certainly worth looking at. Looking at the code, I wonder if there is a wider problem with blocking writes. As I can't reproduce this issue at the moment, it would be very helpful if the original reporter (Stoyan Boshev) could do find out (e.g. with a debugger) the value of SocketProcessor.status for those threads that are blocked at line 1542 of NioEndpoint. I suspect that they will show Tomcat is trying to close the connection but can't because of the lock. If that is the case I need to do some more thinking on how the blocking writes are handled.", "attachment_id": null, "id": 168285, "creator": "markt@apache.org", "time": "2013-07-05T08:36:56Z", "bug_id": 55171, "creation_time": "2013-07-05T08:36:56Z", "is_private": false}, {"count": 7, "tags": [], "bug_id": 55171, "attachment_id": null, "text": "The SocketProcessor.status for those threads that are blocked at line 1542 of NioEndpoint is OPEN_READ.", "id": 168407, "time": "2013-07-09T10:10:04Z", "creator": "s.boshev@gmail.com", "creation_time": "2013-07-09T10:10:04Z", "is_private": false}, {"count": 8, "tags": [], "text": "OK. Not what I was expecting but thanks for the information.", "attachment_id": null, "id": 168429, "creator": "markt@apache.org", "time": "2013-07-09T18:25:10Z", "bug_id": 55171, "creation_time": "2013-07-09T18:25:10Z", "is_private": false}, {"count": 9, "tags": [], "text": "I'm not sure how it ended up this way (I cleary wasn't paying attention to this particular refactoring) but we have timeouts for non-blocking / async messages but not for blocking / sync messages. That looks very much like an oversight in the spec to me. I'll look into providing a default timeout for blocking messages (I'm thinking 5 seconds) and a way for users to override that - probably via setting a user property on the session.", "attachment_id": null, "bug_id": 55171, "id": 168430, "time": "2013-07-09T18:59:27Z", "creator": "markt@apache.org", "creation_time": "2013-07-09T18:59:27Z", "is_private": false}, {"count": 10, "attachment_id": null, "creator": "markt@apache.org", "text": "I've added a default timeout of 5s to all blocking writes to trunk. That should resolve the issues you see. If you could repeat your test and report back that would be very helpful.", "id": 168432, "time": "2013-07-09T20:09:17Z", "bug_id": 55171, "creation_time": "2013-07-09T20:09:17Z", "tags": [], "is_private": false}, {"count": 11, "tags": [], "bug_id": 55171, "attachment_id": null, "id": 168460, "time": "2013-07-10T15:01:07Z", "creator": "s.boshev@gmail.com", "creation_time": "2013-07-10T15:01:07Z", "is_private": false, "text": "Thanks a lot Mark!\nI confirm the issue is resolved now. \n\nHowever I think the timeout of 5 seconds is pretty low. Under load or slow network and big messages, it is pretty much easy to get a timeout even if there is no real problem with the locks. \n\nI suppose a suitable timeout is 30 seconds or more."}, {"count": 12, "attachment_id": null, "creator": "markt@apache.org", "text": "There is never going to be an ideal timeout for all use cases. I have increased the default to 20s. It is easy enough to change this from onOpen.", "id": 168463, "time": "2013-07-10T16:06:40Z", "bug_id": 55171, "creation_time": "2013-07-10T16:06:40Z", "tags": [], "is_private": false}]