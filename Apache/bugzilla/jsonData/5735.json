[{"count": 0, "text": "The conditions to reproduce this are not very clear, but it seems that under \nheavy load Tomcat can start creating new processors and eventually runs out of \navailable processors. This happens with a concurrency level of about 50 which \nis under the maximum allowed numbers of processors.", "bug_id": 5735, "attachment_id": null, "id": 9450, "time": "2002-01-08T04:55:43Z", "creator": "remm@apache.org", "creation_time": "2002-01-08T04:55:43Z", "tags": [], "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 5735, "text": "We are testing our application with the MS Application Stress tool. When we run \ntests for a long time (a weekend or so) then sometimes a new HTTP Processor is \ncreated (log/catalina_xxx.txt). Because the stress tool always runs with a \nfixed number of users the number of processors shouldn't increase. After such a \ntest it isn't possible to shutdown the tomcat (without killing the processes). \nI don't know if it's an error in our software or in tomcat. I believe it's in \ntomcat but I have no arguments... I will continue to search the cause.", "count": 1, "id": 10042, "time": "2002-01-29T21:07:52Z", "creator": "martin@mhengesbach.de", "creation_time": "2002-01-29T21:07:52Z", "is_private": false}, {"count": 2, "tags": [], "bug_id": 5735, "attachment_id": null, "text": "About the shutdown problem, it may be another problem which was fixed. Please\ntry to see if 4.0.2 b2 works better for that.", "id": 10043, "time": "2002-01-29T21:11:11Z", "creator": "remm@apache.org", "creation_time": "2002-01-29T21:11:11Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 5735, "text": "I think I found an explanation for at least part of the bug.\n\nLooking at the algorithm, it's very possible that it would use more processors\n(usually, no more than one or two) than active clients under very high loads,\nbecause there's a amount of time (very small, but not null) between the instant\nthe client is disconnected after the response is finished, and the instant where\nthe processor has finished being recycled and is ready to process requests again.\nThis behavior won't be fixed.\n\nOTOH, I have never seen the connector completely misbehave, run out of\nprocessors and die. If you have experienced it, and can help me reproduce it,\nI'd like to hear from you. Of course, if you can come up with a fix, that would\nbe awesome :)", "count": 3, "id": 10245, "time": "2002-02-02T22:39:21Z", "creator": "remm@apache.org", "creation_time": "2002-02-02T22:39:21Z", "is_private": false}, {"count": 4, "tags": [], "bug_id": 5735, "text": "*** Bug 6260 has been marked as a duplicate of this bug. ***", "id": 10460, "time": "2002-02-07T16:21:56Z", "creator": "remm@apache.org", "creation_time": "2002-02-07T16:21:56Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 5735, "text": "When trying to put a new site into production today I ran into a similar problem\nat least four times using mod_jk and Ajp13.  Each time I had to shutdown and\nrestart Tomcat.\n\nEverything would work fine for several hours.  Tomcat would end up with ~50\nAjp13Processors.  maxProcessors was 75.  Then something would change.  All\nof a sudden Tomcat would start creating additional Ajp13 processors one after\nanother, in a few minutes  hitting the max of 75, then it started rejecting\nconnections.  Because of the repeatable nature of this, and after reviewing\nlogs, this doesn't appear to be due to a sudden increase in traffic to the\nsite.\n\n\nThe Apache server has the following config:\n\nSolaris 7, Apache 1.3.22, mod_jk built from cvs ~ 1 week ago.\n\nTomcat is running on a different server.\n\nSolaris 8, Tomcat 4.1-dev built from cvs ~ 3-4 weeks ago, jk and ajp jars\nbuilt from cvs ~ 1 week ago.  Java(TM) 2 Runtime Environment, Standard Edition\n(build 1.3.1_02-b02)\n\nI reviewed all the logs including the apache mod_jk.log and could not\nfind anything obvious that may have triggered this behaviour.\n\nFYI, we have another site running with an identical Apache/Tomcat config,\nbut much lower volume.  It has been running fine for over a week.", "id": 10821, "time": "2002-02-17T03:50:00Z", "creator": "glenn@apache.org", "creation_time": "2002-02-17T03:50:00Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 5735, "text": "it's not surprising that the ajp connector/processors are exhibiting the same \nbehaviour as the http connector/processors, considering that the ajp versions \nwere created by copying, then modifying the http versions :)", "id": 10826, "time": "2002-02-17T15:57:41Z", "creator": "seguin@motive.com", "creation_time": "2002-02-17T15:57:41Z", "is_private": false, "attachment_id": null}, {"count": 7, "tags": [], "creator": "glenn@apache.org", "text": "I did a closer review of the apache access_log and the tomcat 4 catalina_log.\nWhen the Ajp13Connector starts spawning additional Ajp13Proccessors all of\na sudden there is a 1:1 correlation between apache requests which get forwarded\nto tomcat and the startup of Ajp13Processors.  Prior to the the runaway\ncreation of Ajp13Processors catalina had run 2 hours without creating any\nnew ones.\n\nSo it looks like some bug causes existing processors to no longer be available\nfor use, and any new ones created to only be used once.", "id": 10837, "time": "2002-02-17T23:36:38Z", "bug_id": 5735, "creation_time": "2002-02-17T23:36:38Z", "is_private": false, "attachment_id": null}, {"count": 8, "text": "Based on the apache logs it looks like when the Ajp13Processor's become\nunavailable for reuse they have successfully handled a request.  I can\ntell this due to entries existing in the apache access_log for image files,\netc. related to a request forwarded to Tomcat via mod_jk being requested.\n\nSo the problem which is causing the Ajp13Processor's to hang must be in\nAjp13Proccessor code after a request has been handled and before it calls\nconnector.recycle(this) to make itself available to process another Ajp request.", "bug_id": 5735, "attachment_id": null, "id": 10838, "time": "2002-02-18T00:30:21Z", "creator": "glenn@apache.org", "creation_time": "2002-02-18T00:30:21Z", "tags": [], "is_private": false}, {"count": 9, "tags": [], "text": "Recategorizing bug, and upgrading priority.\nThe 1 request <-> 1 new processor situation Glenn experienced could indeed be an\nimportant clue.", "is_private": false, "id": 10844, "creator": "remm@apache.org", "time": "2002-02-18T05:52:04Z", "bug_id": 5735, "creation_time": "2002-02-18T05:52:04Z", "attachment_id": null}, {"count": 10, "tags": [], "bug_id": 5735, "text": "I think we are getting close to identifying how this bug behaves.\n\nEverytime you get the following Exception:\n\njava.io.IOException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite(Native Method)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:96)\n        at org.apache.ajp.Ajp13.send(Ajp13.java:525)\n        at org.apache.ajp.RequestHandler.finish(RequestHandler.java:496)\n        at org.apache.ajp.Ajp13.finish(Ajp13.java:395)\n        at\norg.apache.ajp.tomcat4.Ajp13Response.finishResponse(Ajp13Response.java:192)\n        at\norg.apache.ajp.tomcat4.Ajp13Processor.process(Ajp13Processor.java:453)\n        at org.apache.ajp.tomcat4.Ajp13Processor.run(Ajp13Processor.java:540)\n        at java.lang.Thread.run(Thread.java:484)\n\nAjp13Connection spawns a new Ajp13Processor, if you have reached maxProccessors,\na connection is rejected.\n\nIn a long test run I am doing I see this maybe once an hour.\nA bunch of the above exceptions, with an identical number of attempts\nto spawn a new Ajp13Processor.", "id": 10871, "time": "2002-02-19T04:42:08Z", "creator": "glenn@apache.org", "creation_time": "2002-02-19T04:42:08Z", "is_private": false, "attachment_id": null}, {"count": 11, "tags": [], "bug_id": 5735, "text": "Interesting, but looking at the code, I don't see any problem it would cause\n(the exception gets caught right away, and the processor should get recycled\nnormally; I don't see any side effect or problems it would cause).", "id": 10872, "time": "2002-02-19T06:21:31Z", "creator": "remm@apache.org", "creation_time": "2002-02-19T06:21:31Z", "is_private": false, "attachment_id": null}, {"attachment_id": null, "tags": [], "creator": "remm@apache.org", "text": "I've just committed a fix for a problem which could cause this bug with the\nHTTP/1.1 connector. Could you test again with the latest code (form HEAD) ?", "count": 12, "id": 10958, "time": "2002-02-20T19:22:54Z", "bug_id": 5735, "creation_time": "2002-02-20T19:22:54Z", "is_private": false}, {"count": 13, "tags": [], "bug_id": 5735, "text": "The problem I was having with the Ajp connector looks like it is resolved.\nThe java -Xmx max memory usage was set too high.  With memory required by\nother processes on the system, the JVM would hit a memory usage threshhold\nbelow the -Xmx setting where it would have to start using swap.  Using swap\ndoes not make for efficient GC.\n\nTo debug GC problems start the JVM with the -verbose:gc arg.\nThis doesn't add alot of overhead, or too much logging.  But does\ngive you alot of valuable information.\n\nThe problem I had looks like it is resolved, although I am a bit embarressed\nthat I didn't find it earlier.  Thanks to all those who assisted.\n\nNow back to the original reported bug with HTTP processors.\n ", "id": 10974, "time": "2002-02-20T21:51:08Z", "creator": "glenn@apache.org", "creation_time": "2002-02-20T21:51:08Z", "is_private": false, "attachment_id": null}, {"count": 14, "tags": [], "bug_id": 5735, "text": "After further review of the Tomcat 4 logs, here is another indicator\nand/or impact of GC delays.\n\nIf you see the Ajp13Proccessor throw IOExceptions for\nBroken Pipe (Unix) or Socket Write Error (Windows) this could\nbe caused by GC delays.  While running Tomcat with -verbose:gc\nI noticed that GC delays > 1 second could cause the above\nexception to be thrown.", "id": 10990, "time": "2002-02-21T03:01:51Z", "creator": "glenn@apache.org", "creation_time": "2002-02-21T03:01:51Z", "is_private": false, "attachment_id": null}, {"attachment_id": null, "tags": [], "creator": "remm@apache.org", "text": "Since no one has been able to reproduce it for the HTTP connector (and the\nthread pooling code), I'll mark this bug as worksforme. I've added additional\ncode which could make the connector more robust (handling of unexpected\nexceptions and errors), but I now highly doubt there's any problem with the\nthreading code itself.", "count": 15, "id": 11270, "time": "2002-02-28T01:00:49Z", "bug_id": 5735, "creation_time": "2002-02-28T01:00:49Z", "is_private": false}, {"count": 16, "tags": [], "text": "Created attachment 1511\nserver.xml", "attachment_id": 1511, "bug_id": 5735, "id": 13135, "time": "2002-04-09T21:14:45Z", "creator": "wrath@jerky.net", "creation_time": "2002-04-09T21:14:45Z", "is_private": false}, {"count": 17, "tags": [], "bug_id": 5735, "attachment_id": null, "text": "I am experiencing this problem on a regular basis.  The specifics of my setup:\n\nlinux (kernel 2.2.19)\nglibc 2.1.3\nj2sdk1.4.0\nTomcat 4.0.3\nMax Heap: 512MB\nThread Stack: 512kb\nPhysical RAM: 1GB\n-server only runs tomcat\n\nTomcat is running on port 80 with the HTTP 1.1 connector only.  No other \nconnectors are configured.  I have attached a copy of my server.xml to this bug \nreport.\n\nThe specific problem I see, is that upon launching of tomcat, everything works \nfine and all dynamic content is served properly.  Then after a seemingly random \namount of time, I begin to get the following in my catalina log file:\n\n2002-04-09 19:23:02 HttpProcessor[80][80] Starting background thread\n2002-04-09 19:23:03 HttpProcessor[80][81] Starting background thread\n2002-04-09 19:23:05 HttpProcessor[80][82] Starting background thread\netc.... up to 400 (my max processors)\n\nOnce I get to 400 processors I start getting the following error for all \nadditional requests:\n\n2002-04-09 19:42:05 HttpConnector[80] No processor available, rejecting this con\nnection\n\nManual review of the log file reveals no other information.  However, I can \nconfirm that for each new request sent to the server, a new background thread \nis created.  So it seems that the existing threads aren't reused or killed, and \nnew ones are created which eventually hit the maximum.\n\nI'm in desperate shape, please help.  To correct the problem, I basically must \nrestart the server.  The error occurs about every 2-3 hours on my system.  It \nmostly seems to happen during the day which leads me to believe it is load \nrelated.  When the error occurs the system has plenty of resources available \nboth CPU and RAM.  Also, it is worth noting that I can't do a graceful shutdown \nof tomcat when this error occurs.", "id": 13136, "time": "2002-04-09T21:16:20Z", "creator": "wrath@jerky.net", "creation_time": "2002-04-09T21:16:20Z", "is_private": false}, {"count": 18, "tags": [], "bug_id": 5735, "text": "As this is not reproducable for me, I'm afraid I can't help much. It's also \nonly the second time I get that particular report. Hopefully the upcoming \nCoyote release will fix it for you.\nAs Glenn pointed out in his analysis, it could also be a problem with the GC \n(and then, after there are too many threads, the VM is sort of messed up).\nYou should lower the number of maxProcessors; 400 is a lot more than what the \nsystem can handle, so it wouldn't help (150 looks more reasonable).\n\nIf you're having problems with the standalone processor because the traffic is \ntoo high, maybe you also could give mod_jk a try.", "id": 13137, "time": "2002-04-09T21:32:55Z", "creator": "remm@apache.org", "creation_time": "2002-04-09T21:32:55Z", "is_private": false, "attachment_id": null}, {"count": 19, "tags": [], "text": "I can confirm that we are definately getting this same problem in our \nproduction environment.  This is a major and urgent issue for us, our business \nis riding on it.  \n\nWe are using:\n\nTomcat 4.0.3\nSolaris 2.7\nHeap 128 min, 256 max\nJSK 1.3.0.2\nThread pool min 5, 255 max\n\nWe actually run our of heap at around 140 threads, but are seeing the same \npattern.\n\n- Things are fine for a while\n- At a seemingly randon amount of time (usualy several hours) after restart,\nthe Catalina log shows thread creation going crazy:\n\n2002-04-17 01:54:03 HttpProcessor[443][11] Starting background thread\n2002-04-17 04:51:02 HttpProcessor[443][12] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][13] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][14] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][15] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][16] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][17] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][18] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][19] Starting background thread\n2002-04-17 04:52:45 HttpProcessor[443][20] Starting background thread\n2002-04-17 04:52:46 HttpProcessor[443][21] Starting background thread\n..  ..\n..  ..\n2002-04-17 04:56:43 HttpProcessor[443][127] Starting background thread\n2002-04-17 04:56:43 HttpProcessor[443][128] Starting background thread\n2002-04-17 04:56:43 HttpProcessor[443][129] Starting background thread\n2002-04-17 06:20:20 HttpConnector Opening server socket on all host IP addresses\n2002-04-17 06:20:20 HttpConnector Opening server socket on all host IP addresses\n\nThe gap in the log at 4:56 is the server getting an out-of-memory exception and \nbecoming unresponsive to further resquests (including shutdown).  We have to \nkill manually and restart.\n\nDuring this same time people, we see a huge jump in the amount of memory being \nused by the JVM for this process.  I guess that's why we run out of heap, \nbefore we can run out of threads.\n\nSorry for the lack of appropriate detail, I am not the sys admin.  Please let \nme know what better information we can provide.  This is happening daily for \nus, in our (24x7) prod system....\n\n", "is_private": false, "id": 13892, "creator": "drew.cox@epredix.com", "time": "2002-04-17T18:50:55Z", "bug_id": 5735, "creation_time": "2002-04-17T18:50:55Z", "attachment_id": null}, {"count": 20, "tags": [], "bug_id": 5735, "attachment_id": null, "id": 13903, "time": "2002-04-17T20:32:20Z", "creator": "remm@apache.org", "creation_time": "2002-04-17T20:32:20Z", "is_private": false, "text": "Did you read the comments above ?\nIf you get the OutOfMemory first (before the thread problems), it could be that \nthe server runs out of memory because of too many active sessions (or something \nlike that)."}, {"count": 21, "tags": [], "bug_id": 5735, "attachment_id": null, "text": "Yes, I read the entire thread.  \n\nWe keep an eye on the number of active sessions through the manager/list \ncommand are these are by no means extreme.  In fact this problem tends *not* to \nhappen during our periods of highest load, where we see our active session \ncount at it's max.  It seems to be in the morning when a bunch of people log in \nat once.\n\nWe are tracing process memory usage and see it double at the same time as the \nrush of new threads in the pool:\n\n04-17-02:04:30:00 VSZ RSS 194208 169088\n04-17-02:04:40:00 VSZ RSS 194272 169160\n04-17-02:04:50:00 VSZ RSS 194272 169160\n04-17-02:05:00:00 VSZ RSS 342440 315240\n04-17-02:05:10:00 VSZ RSS 342080 314880\n04-17-02:05:20:00 VSZ RSS 342080 314880\n\nVSZ=Virtual set size\nRSS=Resident set size\n\nOur problem (this morning) occured at 04:55.", "id": 13915, "time": "2002-04-17T21:43:43Z", "creator": "drew.cox@epredix.com", "creation_time": "2002-04-17T21:43:43Z", "is_private": false}, {"count": 22, "tags": [], "text": "Coyote 1.0 Beta 7 or later now uses different thread pooling code (from Tomcat \n3.3).\n\nCould you test using it ? If you're not experiencing the bug anymore, then it \nis likely the problem is with Tomcat 4 TP code.\n\nhttp://jakarta.apache.org/builds/jakarta-tomcat-connectors/coyote/release/v1.0-\nb8/\n\nTo configure Coyote with SSL, use:\n    <Connector className=\"org.apache.coyote.tomcat4.CoyoteConnector\"\n               port=\"443\" minProcessors=\"5\" maxProcessors=\"75\"\n               enableLookups=\"true\"\n\t       acceptCount=\"10\" debug=\"0\" scheme=\"https\" secure=\"true\"\n               useURIValidationHack=\"false\">\n      <Factory className=\"org.apache.coyote.tomcat4.CoyoteServerSocketFactory\"\n               clientAuth=\"false\" protocol=\"TLS\" />\n    </Connector>\n\nNote that the factory className is different.", "attachment_id": null, "bug_id": 5735, "id": 14044, "time": "2002-04-19T10:35:34Z", "creator": "remm@apache.org", "creation_time": "2002-04-19T10:35:34Z", "is_private": false}, {"count": 23, "tags": [], "bug_id": 5735, "attachment_id": null, "id": 14643, "time": "2002-04-24T17:04:20Z", "creator": "glenn@apache.org", "creation_time": "2002-04-24T17:04:20Z", "is_private": false, "text": "I would recommend that you dump the stack for all running threads when\nyou experience this problem.  This can help identify what is causing the\nproblem. By reviewing the stack dump for each thread you can determine\nwhether the problem is due to Tomcat or your application.\n\nOn unix you do a kill -QUIT {tomcat java pid) to cause the thread stack's \nto be dumped to catalina .out.\n\nA Processor for Tomcat runs your application code, delays in your code\ncan cause additional processor threads to be created to handle new requests.\nPossible application or configuration problems which can delay requests:\n\n   Connection delays due to networked services such as a db.\n   Connection delays due to running out of pooled resources.\n   Thread synchronization deadlocks.\n   A cascading affect where many new processors get created due to\n   excessively long JVM Garbage Collections.  start java with\n   -verbose:gc to detect this.\n"}, {"count": 24, "tags": [], "bug_id": 5735, "text": "FYI - I was experiencing a similar problem with Tomcat 4.0.3 on Solaris 8, \nwhere if there were more simultaneous connections to the server than specified \nin maxProcessors, instead of queueing the connections, there were \"connection \ntimeout\" messages and blank HTML pages being returned to the client.  A \nworkaround was to increase maxProcessors to something like 320 - that allowed \nme to have 300 simultaneous requests/connections.  But I tried Coyote as \nsuggested, and with maxProcessors set to only 15, I was able to get 300+ \nsimultaneous connections without error.  ", "id": 15074, "time": "2002-04-29T20:03:51Z", "creator": "lance_hartford@yahoo.com", "creation_time": "2002-04-29T20:03:51Z", "is_private": false, "attachment_id": null}, {"count": 25, "tags": [], "bug_id": 5735, "text": "I have a question: which version of Coyote are you using ?\nI'm asking that because only the recent versions are using the TC 3.3 TP code. \nSo if you're using one of the older versions which have the older thread pool, \nit would indicate that the problem is with GC (Coyote greatly reduces GC).", "id": 15075, "time": "2002-04-29T20:13:27Z", "creator": "remm@apache.org", "creation_time": "2002-04-29T20:13:27Z", "is_private": false, "attachment_id": null}, {"count": 26, "attachment_id": null, "bug_id": 5735, "text": "According to Lance report, the bug would be caused by either a problem with the\nTP code used in the old Catalina connectors (although I couldn't find any\nobvious problem after a very careful), or higher GC levels (which would cause a\nlong connection queue while GC occurs, and then massive creationof processors),\nor a combination of both.\n\nUsing the HTTP/1.1 connector included in Coyote 1.0 beta 8 apparently fixes the\nproblem, as it will produce lower GC levels, and uses different thread pooling code.", "id": 15157, "time": "2002-04-30T17:20:20Z", "creator": "remm@apache.org", "creation_time": "2002-04-30T17:20:20Z", "tags": [], "is_private": false}, {"count": 27, "tags": [], "bug_id": 5735, "text": "Glenn\n\nI was wondering, what is your hardware configuration, and what settings did you \nuse for -Xmx?  Also, did you run incgc?\n\nThanks,\n\nJim\n", "id": 27605, "time": "2002-12-10T15:18:35Z", "creator": "jchrysta@r5i.com", "creation_time": "2002-12-10T15:18:35Z", "is_private": false, "attachment_id": null}, {"count": 28, "attachment_id": null, "bug_id": 5735, "text": "The bug reporting system is for reporting bugs in the Tomcat code itself,\nnot for resolving configuration problems.\n\nPlease ask this question on the tomcat-users@jakarta.apache.org email list.", "id": 27656, "time": "2002-12-11T15:23:54Z", "creator": "glenn@apache.org", "creation_time": "2002-12-11T15:23:54Z", "tags": [], "is_private": false}]