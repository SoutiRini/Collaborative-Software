[{"count": 0, "tags": [], "bug_id": 53555, "text": "A high-traffic web server using event MPM and mostly receiving HTTPS requests frequently got the error \"scoreboard is full, not at MaxRequestWorkers\" and showed very bad performance.\n\nWe fixed the issue by reverting from 2.4.2 to 2.2.22, still using event MPM.\n\nRelated httpd.conf settings:\n\n StartServers 16\n MinSpareThreads 4\n MaxSpareThreads 4\n ListenBacklog 4096\n Timeout 5\n\nUnfortunately don't have a capture of the server status page and increasing the log level didn't seem to show much.", "id": 160681, "time": "2012-07-17T02:21:21Z", "creator": "astrange@ithinksw.com", "creation_time": "2012-07-17T02:21:21Z", "is_private": false, "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "This may be obvious, but the server-status page is a huge help in analyzing scoreboard full issues.  Do you remember what it looked like?  what state codes were most prevalent?  The scoreboard can fill up quickly if a back end server stalls.", "id": 164959, "time": "2013-01-31T18:54:25Z", "creator": "gregames@apache.org", "creation_time": "2013-01-31T18:54:25Z", "is_private": false}, {"count": 2, "tags": [], "text": "We've seen AH00485: scoreboard is full, not at MaxRequestWorkers on 2.4.4 with the event MPM, no SSL involved.\n\nHaven't figured out the exact conditions yet, but involved are:\n* High/varying load, causing worker processes to be spawned and killed,\n  filling up the scoreboard with G:s.\n* Server reloads due to config changes.\n\nI suspect the root cause is that server processes are flagged for killing, but later they're needed again but instead of reviving the existing process a new one is created. If you have a lot of slow connections (this is a file archive serving DVD-images etc) processes can add up.\n\nThe scoreboard can look like this after a while:\n\n----------8<----------------\nPID\tConnections \tThreads\tAsync connections\ntotal\taccepting\tbusy\tidle\twriting\tkeep-alive\tclosing\n14465\t94\tno\t0\t0\t72\t0\t21\n28881\t132\tyes\t0\t0\t79\t0\t6\n23632\t582\tno\t0\t0\t523\t0\t51\n32314\t43\tno\t0\t0\t28\t0\t15\n13766\t577\tno\t0\t0\t564\t1\t2\n337\t42\tno\t0\t0\t28\t0\t13\n19580\t39\tno\t0\t0\t27\t0\t12\n30603\t478\tno\t0\t0\t424\t0\t52\n32163\t177\tno\t0\t0\t136\t0\t24\n16159\t429\tno\t0\t0\t374\t0\t54\n15376\t93\tno\t0\t0\t45\t0\t47\n32478\t124\tno\t0\t0\t86\t0\t38\n30604\t395\tyes\t2\t48\t390\t3\t0\n30667\t61\tno\t0\t0\t38\t0\t17\n31569\t58\tno\t0\t0\t27\t0\t20\n19614\t161\tno\t0\t0\t117\t0\t44\n32286\t253\tyes\t0\t50\t252\t0\t0\n17643\t454\tyes\t2\t48\t445\t0\t3\n23353\t49\tno\t0\t0\t27\t2\t20\n31581\t145\tno\t0\t0\t106\t0\t34\nSum\t4386\t \t4\t146\t3788\t6\t473\n\nLGLGGGLLGLGLGLLLLLGLGLGLLLLLGLLLLLLLLLLLLLLGGGLGLLGGGGLGGLGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGLGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGLGLGGGGGLGLLGGGLGLLLLLLGGGLLLLLGGLGLGLLLGGGLGLLLGLGLLGL\nLGGLLLLGGGGGGLGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGLGL\nGGLLGGGLLGGLGLGGGGGLLGGGGLGLLLLLLGGGGGLGGGGGGLLLLLGLLLGLLLLLLLGL\nLLLGLLLGLGLGGGLGLGGGGLLGLGGLLLGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGLGGGGGGGGGGGGGGGGGGGGGGGGGLGGGGGGGGGGGGLGGGGGGGGGGGGGGGGGGGGGGG\nGGLLLLLGLLLLGLLLLGLGLLLLGGLGLLLLLGLLGLLLLLLLLLLGGLLLGLGGGGGGGGGG\nGGGGLGGGGLGGGLGGGGGGGLGGGGGGGGGGGGGGGGGGGGLGGGGGLLGGGGLLGGGLGLLG\nGGGLGGLLGGGGLGGLGGLGLGGL____________________WW__________________\n__________GGGGGGGGGGGGGGGGGGGGGGGGGGLGGLGLGGGGGGGGGGGGGGGGGGLLGG\nLGLLGLGLGGGLLGLGGLLLLGLGGGLGLLGGLGLLGLGLLGLGGGLGGGGGGGGGGGGGLGGG\nGLGGLGGGGGLGGGGGGGGGGLGLGGLLGLGG________________________________\n____________________W___W_______________________________________\n____GLGLLLLLLLGGGLLGGLLLGGLLLLLLGGLGLLGGLLGGGGLGLLLGGGLLGGLGLGGG\nLLGLGGLLLLGLGLLGGGGGGLLGGGGGGLLGGGGLGLGL\n----------8<----------------", "is_private": false, "id": 167047, "creator": "nikke@acc.umu.se", "time": "2013-05-05T21:33:14Z", "bug_id": 53555, "creation_time": "2013-05-05T21:33:14Z", "attachment_id": null}, {"count": 3, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 167894, "time": "2013-06-18T15:42:46Z", "creator": "gregames@apache.org", "creation_time": "2013-06-18T15:42:46Z", "is_private": false, "text": "(In reply to Niklas Edmundsson from comment #2)\n> We've seen AH00485: scoreboard is full, not at MaxRequestWorkers on 2.4.4\n> with the event MPM, no SSL involved.\n\n> PID\tConnections \tThreads\tAsync connections\n> total\taccepting\tbusy\tidle\twriting\tkeep-alive\tclosing\n> 14465\t94\tno\t0\t0\t72\t0\t21\n> 28881\t132\tyes\t0\t0\t79\t0\t6\n> 23632\t582\tno\t0\t0\t523\t0\t51\n> 32314\t43\tno\t0\t0\t28\t0\t15\n> 13766\t577\tno\t0\t0\t564\t1\t2\n\n> LGLGGGLLGLGLGLLLLLGLGLGLLLLLGLLLLLLLLLLLLLLGGGLGLLGGGGLGGLGGGGGG\n> GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGLGGGGGGGGGGGGGGGGGGGGGGGGG\n\nOK, there are many worker processes that hang while trying to shut down, probably due to traffic fluctuations. The only two states we see in the scoreboard are G and L.  The G should be transient and can probably be ignored.  The Ls look like the cause of the hangs.\n\nL means the threads are hung while trying to write to the log.  Normally you never see this with logs on a reasonably fast local hard drive.  Are the log files NFS mounted or something like that?\n\nGreg"}, {"count": 4, "tags": [], "bug_id": 53555, "text": "> OK, there are many worker processes that hang while trying to shut down,\n> probably due to traffic fluctuations. The only two states we see in the\n> scoreboard are G and L.  The G should be transient and can probably be\n> ignored.  The Ls look like the cause of the hangs.\n\nTransient for the G:s can mean days in this case, think slooow ADSL connection downloading a DVD image...\n\n> L means the threads are hung while trying to write to the log.  Normally you\n> never see this with logs on a reasonably fast local hard drive.  Are the log\n> files NFS mounted or something like that?\n\nNo, local filesystem. But I'll have to double check that we're not doing anything overly clever on the log front...", "id": 167898, "time": "2013-06-18T18:21:55Z", "creator": "nikke@acc.umu.se", "creation_time": "2013-06-18T18:21:55Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 53555, "text": "Greg,\n\nI didn't check the code, but to me it seems that a \"G\" letter does not mean there's no more work going on. The server-status on our own www.(eu|us).apache.org shows the same G plus L mixture for about a minute (varying) whenever a process dies due to MaxConnectionsPerChild. When I checked such processes, they had open client connections and were still sending data to the client. So it was correct they were still aorund, but the status letters \"G\" or \"L\" for those gracefully exiting children are not showing those details.", "id": 167900, "time": "2013-06-18T19:22:44Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2013-06-18T19:22:44Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 53555, "text": "I looked at apache.org and the code.  The Ls are normal when a gracefully exiting process had an active thread.  Sorry for jumping to conclusions.\n\nclose_listeners sets all the G states during graceful shutdown.  (Unfortunately this means we can no longer see which threads are active vs. idle - not sure having the G state is worth it.) Any active threads which finish their requests will log and set the L state before exiting.  The Gs that remain could represent exited threads or active requests - we can't tell from server-status.\n\nThe processes that didn't exit have active connections.  If they due to slow downloads, maybe the thing to do is to tune for less or no graceful process terminations when the traffic drops by raising MaxSpareThreads.", "id": 167916, "time": "2013-06-19T13:46:35Z", "creator": "gregames@apache.org", "creation_time": "2013-06-19T13:46:35Z", "is_private": false, "attachment_id": null}, {"count": 7, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 172656, "time": "2014-01-24T01:01:42Z", "creator": "daniel.lemsing@gmail.com", "creation_time": "2014-01-24T01:01:42Z", "is_private": false, "text": "Recently hit this error in a high traffic production web server (Apache 2.4.6) leading to an outage.\n\nHas anyone had success in overcoming this issue by amending Apache configuration ?\n\nIf so, what did you change ...\n\nAlso, can anyone offer any suggestions on what triggers this issue ?\n\nBeing a production server, rolling back to 2.2.22 is not preferable."}, {"count": 8, "tags": [], "bug_id": 53555, "text": "One of the gotchas with this is that the scoreboard seems to be sized to cater for MaxRequestWorkers, with no margins for server reloads etc.\n\nIn our case, when it can take days for processes to exit if people are downloading large files over slow connections, we can easily have the situation where multiple server reloads (due to config changes etc) causes the scoreboard to fill up with old server processes in graceful-shutdown mode and no space for new processes to do some actual work.\n\nI can see a few ways to work around this:\n\n1) Simply make the scoreboard bigger. I'd like a default size-multiplier of 2 for the event MPM, but configurable so we can set it to 4 or something for our setup. An alternative is to set a ridiculously large MaxRequestWorkers to get a big enough scoreboard, but one DOS and we're out of scoreboard anyway.\n\n2) Kill off the oldest gracefully-exiting processes when we can't spawn a new process to do useful work.\n\nThe ideal solution is probably a mix of these two.\n\nAlso, I'm wondering if this is also somehow related to the \"server dies for a while when doing reload\" issue. We're still at httpd 2.4.6 though, so I can't say for certain that some of these issues aren't already fixed.", "id": 172660, "time": "2014-01-24T08:19:07Z", "creator": "nikke@acc.umu.se", "creation_time": "2014-01-24T08:19:07Z", "is_private": false, "attachment_id": null}, {"count": 9, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "In case it matters any, this problem appears to be specific to the Event MPM. I had it happening on a server, and when I switched it to the Worker MPM, it stopped. However, what I did notice is that the same server periodically had all of its workers taken up with requests, so that may be relevant to the problem as well.", "id": 174296, "time": "2014-04-05T03:00:13Z", "creator": "deriamis@gmail.com", "creation_time": "2014-04-05T03:00:13Z", "is_private": false}, {"count": 10, "tags": [], "text": "I have a similar behavior as described here (with no ssl involved) with httpd 2.4.9.\n\nI got a lot of AH00485: \"scoreboard is full, not at MaxRequestWorkers\", httpd is still serving requests, however one worker is in graceful finishing state and is taking 100% CPU.\n\nThe worker was in this stat for about 24h, until I kill(1)ed it.\n\nThreads stats:\n\n__________________W_____________________________________________\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n\nUnfortunately I don't have any other info from the status page.\n\nstrace of the worker shows an epoll_wait infinite loop:\n\n    [...]\n    epoll_wait(10, {}, 128, 100)            = 0\n    epoll_wait(10, {}, 128, 100)            = 0\n    epoll_wait(10, {}, 128, 100)            = 0\n    [...]\n\nmpm event config:\n\n    StartServers         1\n    ServerLimit          4\n    MinSpareThreads      4\n    MaxRequestWorkers    128\n    ThreadsPerChild      64\n    ThreadLimit          64\n    AsyncRequestWorkerFactor 4", "is_private": false, "id": 175932, "creator": "p@hfox.org", "time": "2014-06-20T11:26:48Z", "bug_id": 53555, "creation_time": "2014-06-20T11:26:48Z", "attachment_id": null}, {"count": 11, "tags": [], "creator": "andrixnet@yahoo.com", "attachment_id": null, "id": 179621, "time": "2014-12-08T12:17:56Z", "bug_id": 53555, "creation_time": "2014-12-08T12:17:56Z", "is_private": false, "text": "Apache 2.4.10 on Slackware Linux 14.1 x86_64 platform.\n\nI am seeing this about once a minute in the logs:\nAH00485: scoreboard is full, not at MaxRequestWorkers\n\nI was able to recover only by a forced restart (stop then start)."}, {"count": 12, "tags": [], "text": "After migrating from worker MPM to event MPM with Apache 2.4.7 we are seeing this same problem.\n\nServer version: Apache/2.4.7 (Ubuntu)\nUbuntu Trusty 14.04.2 LTS\nLinux 3.13.0-40-generic #69-Ubuntu SMP Thu Nov 13 17:53:56 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\n\nWe explicitly moved to event MPM for this workload, which is a proxy of thousands of mostly-idle HTTP Keep-Alive connections - since event MPM doesn't require a thread per Keep-Alive connection. Although our number of clients is fairly consistent, and we have MaxConnectionsPerChild=0, we observe Apache processes going into GGGGGG state until eventually Apache no longer accepts connections.\n\nIf we set MinSpareThreads and MaxSpareThreads equal to MaxRequestWorkers (so Apache doesn't attempt to scale down processes), the issue goes away (as expected, but validates (maybe?) this has to do with Apache scale-down).\n\nSince client connections can be connected for hours or days, Apache processes stay in this state for a very long time, eventually rejecting client connections and becoming wedged.\n\nOur clients are not browsers - Apache is being used for a mid-tier load balancer/proxy with client connections that are very long lived (long Keep-Alive times).\n\n248 requests/sec - 0.7 MB/second - 3114 B/request\n2 requests currently being processed, 38 idle workers\nPID\tConnections\tThreads\tAsync connections\ntotal\taccepting\tbusy\tidle\twriting\tkeep-alive\tclosing\n28483\t1642\tno\t0\t0\t0\t1642\t0\n29672\t553\tyes\t1\t19\t0\t552\t0\n29696\t9\tno\t0\t0\t0\t9\t0\n29588\t173\tno\t0\t0\t0\t173\t0\n29618\t1\tno\t0\t0\t0\t1\t0\n29644\t6\tno\t0\t0\t0\t6\t0\n29719\t30\tno\t0\t0\t0\t30\t0\n29743\t237\tyes\t1\t19\t0\t236\t0\nSum\t2651\t \t2\t38\t0\t2649\t0\nGGGGGGGGGGGGGGGGGGGG________W___________GGGGGGGGGGGGGGGGGGGGGGWG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGG________W___________................................\n........", "is_private": false, "id": 183298, "creator": "lscotte@gmail.com", "time": "2015-06-03T00:42:35Z", "bug_id": 53555, "creation_time": "2015-06-03T00:42:35Z", "attachment_id": null}, {"count": 13, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "We are having the symptoms here :\n\nServer Version: Apache/2.4.7 (Ubuntu) SVN/1.8.8 mod_jk/1.2.37 OpenSSL/1.0.1f\nUbuntu 14.04.2 LTS\nLinux 3.13.0-52-generic #86-Ubuntu SMP Mon May 4 04:32:59 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n\nMany logs : \n[mpm_event:error] [pid 6332:tid 140558940702592] AH00485: scoreboard is full, not at MaxRequestWorkers\n\nFrom the server status \n\nRight after start : \n__RR___________R________________________W__________________W____\n___________.....................................................\n......................\n\nAfter one hour : \n\n___________________W_____GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGG_______W___W_____________............................\n......................\n\nTwo hours later : \n\nGGGGGGGGGGGGGGGGGGGGGGGGGW_W_____W________W____W__GGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGG\n\nIs there anything we can provide to help in the diagnostic of the issue ?\n\nDo you know of any workaround through configuration ?", "id": 183332, "time": "2015-06-05T11:36:29Z", "creator": "olivier.jaquemet@jalios.com", "creation_time": "2015-06-05T11:36:29Z", "is_private": false}, {"count": 14, "tags": [], "bug_id": 53555, "text": "In case others find it useful, the approach we used to mitigate this was several things:\n\n1. Increased MinSpareThreads and MaxSpareThreads, as well as the range between them. By making Apache less aggressive about scaling the number of servers down, it's less likely to run into this issue. Our new values are:\n\n   MinSpareThreads = MaxRequestWorkers / 4\n   MaxSpareThreads = MinSpareThreads * 3\n\n2. Lowered MaxKeepAliveRequests. By looking at a histogram of request counts per connection on an equivalent Apache running with worker MPM (first value in Acc column), I found a very long tail of few connections out to our old value, but a clear cluster at the lower end. Our new MaxKeepAliveRequests is a bit beyond the critical-mass cluster, but significantly lower than the old value. This will allow servers to recycle quicker when they scale down, but not cause any significant impact to client connections, since the relative number of connections we'll close early is small.\n\n3. Increased AsyncWorkerFactor. When Apache servers are scaling down (in Gracefully Finishing state), this allows other servers to pick up the slack by handling a larger number of total client connections (in HTTP Keep-Alive, this does not increase the number of workers), where before these processes had reached their limit of connections and were rejecting new ones. Event MPM does a reasonably good job of spreading load between processes, and with our larger spare threads range we now tend to have more alive processes as well.\n\nWe also considered lowering KeepAliveTimeout, but using a similar histogram as I did for KeepAliveRequests from a worker MPM configuration (using the SS column as a reasonable analog). That histogram showed a nice distribution for us, so lowering this would have affected clients and not helped for this workload.\n\nThese are the values that worked for us, with our workload, to mitigate this issue. Of course your workload and values will be different, but this may be a reasonable strategy to try as well.", "id": 183336, "time": "2015-06-05T16:38:39Z", "creator": "lscotte@gmail.com", "creation_time": "2015-06-05T16:38:39Z", "is_private": false, "attachment_id": null}, {"count": 15, "tags": [], "text": "2.4.16 and the following configuration hits scoreboard full with 3-4 reloads\n\n    StartServers        2\n    MinSpareThreads     50\n    MaxSpareThreads     150\n    ThreadsPerChild     25\n    MaxRequestWorkers   200\n    MaxConnectionsPerChild  10000\n\nAny advice?", "is_private": false, "id": 184420, "creator": "leho@kraav.com", "time": "2015-08-08T13:29:43Z", "bug_id": 53555, "creation_time": "2015-08-08T13:29:43Z", "attachment_id": null}, {"count": 16, "tags": [], "text": "This is certainly a bug and not a configuration issue. I have had this error happen with the default (Debian) configuration and other people online report the same. I have had this happen with mpm_event and mpm_worker.\n\nIt's very reproducible. It happens with almost any thread related settings I have tried. It stops new requests from being served and is a serious problem.\n\nThere is some bug with the way Apache handles its servers/threads. This is not something that can be fixed by tweaking the configuration. At best it might be mitigated by setting:\n\nStartServers           1\nServerLimit            X\nThreadsPerChild        XXX\nThreadLimit            <ThreadsPerChild>\nMaxRequestWorkers      <ServerLimit * ThreadLimit>\nMinSpareThreads        <MaxRequestWorkers>\nMaxSpareThreads        <MaxRequestWorkers>\nMaxRequestsPerChild    0\n\nIn other words, make it so a thread stays alive forever and therefore the buggy part of the code that is responsible for killing and reusing threads is never hit. Of course this requires always using the maximum amount of RAM since threads never die even when there is no traffic.", "is_private": false, "id": 184747, "creator": "gobbledance@safe-mail.net", "time": "2015-08-25T15:09:19Z", "bug_id": 53555, "creation_time": "2015-08-25T15:09:19Z", "attachment_id": null}, {"attachment_id": null, "tags": [], "creator": "leho@kraav.com", "is_private": false, "count": 17, "id": 184748, "time": "2015-08-25T15:50:48Z", "bug_id": 53555, "creation_time": "2015-08-25T15:50:48Z", "text": "(In reply to gobbledance from comment #16)\n> This is certainly a bug and not a configuration issue. I have had this error\n> happen with the default (Debian) configuration and other people online\n> report the same. I have had this happen with mpm_event and mpm_worker.\n> \n> It's very reproducible. It happens with almost any thread related settings I\n> have tried. It stops new requests from being served and is a serious problem.\n\nI have found no way around it with a variety of worker configuration parameters. Looks like the best bet would be to have fail2ban or similar monitor the error_log and restart the server when scoreboard hits the DoS condition."}, {"count": 18, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "I'm also affected by this bug running Apache/2.4.7 (Ubuntu) on 14.04. I setup a logfile watch daemon that force restarts apache2 if the line shows up in the error.log as a hotfix.\n\nHas anyone tested this with the current stable release 2.4.16?", "id": 185053, "time": "2015-09-09T10:06:25Z", "creator": "p.thurner@blunix.org", "creation_time": "2015-09-09T10:06:25Z", "is_private": false}, {"count": 19, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 185550, "time": "2015-09-29T22:17:45Z", "creator": "sf@sfritsch.de", "creation_time": "2015-09-29T22:17:45Z", "is_private": false, "text": "(In reply to ScottE from comment #12)\n> Our clients are not browsers - Apache is being used for a mid-tier load\n> balancer/proxy with client connections that are very long lived (long\n> Keep-Alive times).\n\nThis seems to be a problem that should not be too difficult to fix. When a process is shutting down, it should close its keepalive connections. Can you please check if the attached patch helps?\n\n\nThe case where long-running transfers are keeping a process from shutting down is much more difficult to fix."}, {"count": 20, "tags": [], "creator": "sf@sfritsch.de", "attachment_id": 33154, "id": 185551, "time": "2015-09-29T22:18:15Z", "bug_id": 53555, "creation_time": "2015-09-29T22:18:15Z", "is_private": false, "text": "Created attachment 33154\nclose keepalive connections if process is shutting down"}, {"count": 21, "tags": [], "bug_id": 53555, "text": "Created attachment 33158\nexit some threads early during gracful shutdown of a process\n\nThe attached diff against the 2.4.x branch makes unneeded threads exit earlier during graceful shutdown of a process. This then allows new processes to use the freed scoreboard slots.\n\nI am interested in real-live experiences with this patch. It has two known problems, though:\n\n- If httpd is shut down (ungracefully) while there are some old processes around serving long lasting requests, those processes won't die peacefully but will be SIGKILLed by the parent after 10 seconds.\n\n- server-status shows incomplete information (that is, even more incomplete than in 2.4 ;) )", "id": 185593, "time": "2015-10-03T15:31:46Z", "creator": "sf@sfritsch.de", "creation_time": "2015-10-03T15:31:46Z", "is_private": false, "attachment_id": 33158}, {"count": 22, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "I have applied the patch on our own production server, which experiences this problem sometimes twice a day, and sometimes not for a week or so.\n\nSo now we wait. I will report immediately if the problem recurs, and I will also report in a week if the problem does not recur.\n\nPS: If \"Graceful, but sigkill after 10 seconds\" were an actual option, I would probably use it all the time.", "id": 185608, "time": "2015-10-03T23:28:29Z", "creator": "bucky@mightytikigod.com", "creation_time": "2015-10-03T23:28:29Z", "is_private": false}, {"count": 23, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "(In reply to Stefan Fritsch from comment #21)\n> \n> - If httpd is shut down (ungracefully) while there are some old processes\n> around serving long lasting requests, those processes won't die peacefully\n> but will be SIGKILLed by the parent after 10 seconds.\n\nWasn't that already the case for ungraceful stop/restart?\n\n> \n> - server-status shows incomplete information (that is, even more incomplete\n> than in 2.4 ;) )\n\nHow about not setting SERVER_GRACEFUL in close_listeners() and worker_thread()?\nThe old generation's state could be relevent, since the new generation does not \"steal\" the scoreboard now (until the old worker exits).", "id": 185625, "time": "2015-10-05T08:37:17Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-10-05T08:37:17Z", "is_private": false}, {"count": 24, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 185637, "time": "2015-10-05T22:25:35Z", "creator": "sf@sfritsch.de", "creation_time": "2015-10-05T22:25:35Z", "is_private": false, "text": "(In reply to bucky from comment #22)\n> I have applied the patch on our own production server, which experiences\n> this problem sometimes twice a day, and sometimes not for a week or so.\n\nThanks for that already.\n\n\n(In reply to Yann Ylavic from comment #23)\n> (In reply to Stefan Fritsch from comment #21)\n> > - If httpd is shut down (ungracefully) while there are some old processes\n> > around serving long lasting requests, those processes won't die peacefully\n> > but will be SIGKILLed by the parent after 10 seconds.\n> \n> Wasn't that already the case for ungraceful stop/restart?\n\nNormally, those child process should react to the SIGTERM that is sent first. But that is currently broken by my patch.\n\n\n> > - server-status shows incomplete information (that is, even more incomplete\n> > than in 2.4 ;) )\n> \n> How about not setting SERVER_GRACEFUL in close_listeners() and\n> worker_thread()?\n> The old generation's state could be relevent, since the new generation does\n> not \"steal\" the scoreboard now (until the old worker exits).\n\nYes, that would proabaly be better, I'll have to test that. But it would not fix the incompleteness I was referring to: The old and the new process have only one process slot in the scoreboard, which makes the async overview table show sometimes the info from the old and sometimes from the new process, depending on who updated it last."}, {"count": 25, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 185638, "time": "2015-10-05T22:40:15Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-10-05T22:40:15Z", "is_private": false, "text": "(In reply to Stefan Fritsch from comment #24)\n> \n> (In reply to Yann Ylavic from comment #23)\n> > How about not setting SERVER_GRACEFUL in close_listeners() and\n> > worker_thread()?\n> > The old generation's state could be relevent, since the new generation does\n> > not \"steal\" the scoreboard now (until the old worker exits).\n> \n> Yes, that would proabaly be better, I'll have to test that. But it would not\n> fix the incompleteness I was referring to: The old and the new process have\n> only one process slot in the scoreboard, which makes the async overview\n> table show sometimes the info from the old and sometimes from the new\n> process, depending on who updated it last.\n\nIt seems to me that the new generation's worker threads are not started now unless their scoreboard slot is marked SERVER_DEAD (was also SERVER_GRACEFUL before attachment 33158).\nSo AIUI, there shouldn't be two workers using the same slot."}, {"count": 26, "tags": [], "creator": "sf@sfritsch.de", "attachment_id": null, "id": 185639, "time": "2015-10-05T23:00:30Z", "bug_id": 53555, "creation_time": "2015-10-05T23:00:30Z", "is_private": false, "text": "(In reply to Yann Ylavic from comment #25)\n\nThis technical discussion has been moved to the dev mailing list."}, {"count": 27, "tags": [], "bug_id": 53555, "text": "It's been a week.\n\nThe scoreboard errors haven't stopped altogether. Every so often I still get one a second for a short time, but now they last for about 1 or 2 minutes, and that's it.\n\nI haven't gotten any lockups since I applied the patch.", "id": 185716, "time": "2015-10-10T22:59:30Z", "creator": "bucky@mightytikigod.com", "creation_time": "2015-10-10T22:59:30Z", "is_private": false, "attachment_id": null}, {"count": 28, "tags": [], "creator": "leho@kraav.com", "attachment_id": null, "id": 185718, "time": "2015-10-11T07:33:53Z", "bug_id": 53555, "creation_time": "2015-10-11T07:33:53Z", "is_private": false, "text": "mod_h2 did some significant cleanups for resource handling in the 0.9.x branch. \"Scoreboard full\" errors seem to have been completely eliminated for me. Uptime of several weeks goes with no issues now. So looks like external modules' individual cleanup abilities are directly related to this issue."}, {"count": 29, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "I'm confused. To my knowledge, mod_h2 is a 3rd party module. It it somehow an integral part of the latest httpd (2.4.16)?", "id": 185723, "time": "2015-10-11T22:40:15Z", "creator": "bucky@mightytikigod.com", "creation_time": "2015-10-11T22:40:15Z", "is_private": false}, {"count": 30, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 185732, "time": "2015-10-12T05:44:27Z", "creator": "leho@kraav.com", "creation_time": "2015-10-12T05:44:27Z", "is_private": false, "text": "(In reply to bucky from comment #29)\n> I'm confused. To my knowledge, mod_h2 is a 3rd party module. It it somehow\n> an integral part of the latest httpd (2.4.16)?\n\nYes, it is already part of trunk and backported to 2.4.x."}, {"count": 31, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 185733, "time": "2015-10-12T07:18:05Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2015-10-12T07:18:05Z", "is_private": false, "text": "This may be related (In reply to Leho Kraav @lkraav from comment #28)\n> mod_h2 did some significant cleanups for resource handling in the 0.9.x\n> branch. \"Scoreboard full\" errors seem to have been completely eliminated for\n> me.\n\nmod_http2 (being released in 2.4.17) has its own connection handling (somehow appart from the MPM, for now), and shouldn't be seen as a workaround to this issue.\nThe more testing on Stefan's proposed patch (regarding MPM event), without mod_http2, the quicker it will be backported in a release."}, {"count": 32, "tags": [], "creator": "stefan@eissing.org", "attachment_id": null, "id": 185734, "time": "2015-10-12T07:36:41Z", "bug_id": 53555, "creation_time": "2015-10-12T07:36:41Z", "is_private": false, "text": "The fixes I did in mod_http2, mentioned by Leho, were just related to the fact that early 0.9.x version of that module did not properly mark connections for reclaiming, so cleanup work was not run all the time, leading to memory loss and scoreboard handle waste.\n\nThat has been fixed in mod_http2 alone and does not affect other connections. Since the bug happens without the module as well, its presence is not mitigation.\n\nIf the patch by Stefan does not fix it, we should review again if there are races that prevent cleanup from happening in the HTTP/1.1 cases."}, {"attachment_id": null, "tags": [], "creator": "thierryb@filewave.com", "is_private": false, "count": 33, "id": 186203, "time": "2015-11-02T18:28:43Z", "bug_id": 53555, "creation_time": "2015-11-02T18:28:43Z", "text": "WE got into a situation where the users of our product were stuck with G. We've got severe performance issues in those cases. We've tried patch https://bz.apache.org/bugzilla/attachment.cgi?id=33158&action=diff on a couple of installs and it made things much much better. On one install it would get stuck with 2000 clients coming in at roughly the same time. Now it can handle 10K gracefully.\nHope that helps."}, {"count": 34, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "I'm hitting this on a production server with 2.4.18 now. Can't apply custom patches here.\n\nServerLimit 30\nMaxRequestWorkers 30\nMaxConnectionsPerChild 600\nKeepAlive On\nKeepAliveTimeout 1\nMaxKeepAliveRequests 20\nTimeout 50\n\nmod_h2 isn't enabled here.\n\nFrom above discussion, I can't get a clear indiciation if any core developers have confirmed this to be a bug or a configuration issue?", "id": 189726, "time": "2016-03-25T17:20:42Z", "creator": "leho@kraav.com", "creation_time": "2016-03-25T17:20:42Z", "is_private": false}, {"count": 35, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "After applying the patch I ran into \"No space left on device: AH00023: Couldn't create the proxy mutex\" I haven't seen that issue without the patch.\n\nLog says:\n[Sat Mar 26 07:00:34.857694 2016] [core:emerg] [pid 787770:tid 140551243081696] (28)No space left on device: AH00023: Couldn't create the proxy mutex\n[Sat Mar 26 07:00:34.857764 2016] [proxy:crit] [pid 787770:tid 140551243081696] (28)No space left on device: AH02478: failed to create proxy mutex\nAH00016: Configuration Failed\n\n# ipcs -s\n\n------ Semaphore Arrays --------\nkey        semid      owner      perms      nsems\n0x00000000 0          root       600        1\n0x00000000 65537      root       600        1\n0x00000000 131074     apache     600        1\n0x7a00179d 59899907   zabbix     600        13\n0x00000000 3866628    apache     600        1\n0x00000000 3899397    apache     600        1\n0x00000000 3932166    apache     600        1\n0x00000000 21397511   apache     600        1\n0x00000000 21495816   apache     600        1\n0x00000000 21528585   apache     600        1\n0x00000000 21561354   apache     600        1\n0x00000000 21594123   apache     600        1\n0x00000000 21626892   apache     600        1\n0x00000000 21659661   apache     600        1\n0x00000000 29294606   apache     600        1\n0x00000000 29327375   apache     600        1\n0x00000000 29360144   apache     600        1\n0x00000000 29392913   apache     600        1\n0x00000000 29425682   apache     600        1\n0x00000000 29458451   apache     600        1\n0x00000000 29884436   apache     600        1\n0x00000000 29917205   apache     600        1\n0x00000000 29949974   apache     600        1\n0x00000000 29982743   apache     600        1\n0x00000000 30015512   apache     600        1\n0x00000000 30048281   apache     600        1\n0x00000000 30310426   apache     600        1\n0x00000000 30343195   apache     600        1\n0x00000000 30375964   apache     600        1\n0x00000000 30408733   apache     600        1\n0x00000000 30441502   apache     600        1\n0x00000000 30474271   apache     600        1\n0x00000000 30736416   apache     600        1\n0x00000000 30769185   apache     600        1\n0x00000000 30801954   apache     600        1\n0x00000000 30834723   apache     600        1\n0x00000000 30867492   apache     600        1\n0x00000000 30900261   apache     600        1\n0x00000000 30998566   apache     600        1\n0x00000000 31031335   apache     600        1\n0x00000000 31064104   apache     600        1\n0x00000000 31096873   apache     600        1\n0x00000000 31129642   apache     600        1\n0x00000000 31162411   apache     600        1\n0x00000000 31260716   apache     600        1\n0x00000000 31293485   apache     600        1\n0x00000000 31326254   apache     600        1\n0x00000000 31359023   apache     600        1\n0x00000000 31391792   apache     600        1\n0x00000000 31424561   apache     600        1\n0x00000000 37257266   apache     600        1\n0x00000000 37290035   apache     600        1\n0x00000000 37322804   apache     600        1\n0x00000000 37355573   apache     600        1\n0x00000000 37388342   apache     600        1\n0x00000000 37421111   apache     600        1\n0x00000000 37519416   apache     600        1\n0x00000000 37552185   apache     600        1\n0x00000000 37584954   apache     600        1\n0x00000000 37617723   apache     600        1\n0x00000000 37650492   apache     600        1\n0x00000000 37683261   apache     600        1\n0x00000000 37781566   apache     600        1\n0x00000000 37814335   apache     600        1\n0x00000000 37847104   apache     600        1\n0x00000000 37879873   apache     600        1\n0x00000000 37912642   apache     600        1\n0x00000000 37945411   apache     600        1\n0x00000000 38043716   apache     600        1\n0x00000000 38076485   apache     600        1\n0x00000000 38109254   apache     600        1\n0x00000000 38142023   apache     600        1\n0x00000000 38174792   apache     600        1\n0x00000000 38207561   apache     600        1\n0x00000000 41091146   apache     600        1\n0x00000000 41123915   apache     600        1\n0x00000000 41156684   apache     600        1\n0x00000000 41189453   apache     600        1\n0x00000000 41222222   apache     600        1\n0x00000000 41254991   apache     600        1\n0x00000000 44466256   apache     600        1\n0x00000000 44499025   apache     600        1\n0x00000000 44531794   apache     600        1\n0x00000000 44564563   apache     600        1\n0x00000000 44597332   apache     600        1\n0x00000000 44630101   apache     600        1\n0x00000000 49315926   apache     600        1\n0x00000000 49348695   apache     600        1\n0x00000000 49381464   apache     600        1\n0x00000000 49414233   apache     600        1\n0x00000000 49447002   apache     600        1\n0x00000000 49479771   apache     600        1\n0x00000000 49578076   apache     600        1\n0x00000000 49610845   apache     600        1\n0x00000000 49643614   apache     600        1\n0x00000000 49676383   apache     600        1\n0x00000000 49709152   apache     600        1\n0x00000000 49741921   apache     600        1\n0x00000000 55574626   apache     600        1\n0x00000000 55607395   apache     600        1\n0x00000000 55640164   apache     600        1\n0x00000000 55672933   apache     600        1\n0x00000000 55705702   apache     600        1\n0x00000000 55738471   apache     600        1\n0x00000000 58785896   apache     600        1\n0x00000000 58818665   apache     600        1\n0x00000000 58851434   apache     600        1\n0x00000000 58884203   apache     600        1\n0x00000000 58916972   apache     600        1\n0x00000000 58949741   apache     600        1\n0x00000000 61571182   apache     600        1\n0x00000000 61603951   apache     600        1\n0x00000000 61636720   apache     600        1\n0x00000000 61669489   apache     600        1\n0x00000000 61702258   apache     600        1\n0x00000000 61735027   apache     600        1\n0x00000000 63635572   apache     600        1\n0x00000000 63668341   apache     600        1\n0x00000000 63701110   apache     600        1\n0x00000000 63733879   apache     600        1\n0x00000000 63766648   apache     600        1\n0x00000000 63799417   apache     600        1\n0x00000000 65372282   apache     600        1\n0x00000000 65405051   apache     600        1\n0x00000000 65437820   apache     600        1\n0x00000000 65470589   apache     600        1\n0x00000000 65503358   apache     600        1", "id": 189771, "time": "2016-03-29T07:35:20Z", "creator": "sander@hoentjen.eu", "creation_time": "2016-03-29T07:35:20Z", "is_private": false}, {"count": 36, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "(In reply to Sander Hoentjen from comment #35)\n> After applying the patch I ran into \"No space left on device: AH00023:\n> Couldn't create the proxy mutex\" I haven't seen that issue without the patch.\n\nHi Sander, I don't believe this is related to the patch - I've seen this happen (on vanilla 2.4.7) with a bad configuration and something like daemontools constantly restarting Apache. This is likely a valid bug, where Apache can leak mutexes under some conditions, but I don't think it's caused by the patch.", "id": 189836, "time": "2016-03-30T16:53:49Z", "creator": "lscotte@gmail.com", "creation_time": "2016-03-30T16:53:49Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "sander@hoentjen.eu", "is_private": false, "count": 37, "id": 189855, "time": "2016-03-31T08:11:19Z", "bug_id": 53555, "creation_time": "2016-03-31T08:11:19Z", "text": "(In reply to ScottE from comment #36)\n> (In reply to Sander Hoentjen from comment #35)\n> > After applying the patch I ran into \"No space left on device: AH00023:\n> > Couldn't create the proxy mutex\" I haven't seen that issue without the patch.\n> \n> Hi Sander, I don't believe this is related to the patch - I've seen this\n> happen (on vanilla 2.4.7) with a bad configuration and something like\n> daemontools constantly restarting Apache. This is likely a valid bug, where\n> Apache can leak mutexes under some conditions, but I don't think it's caused\n> by the patch.\n\nWell, we have apache 2.4 in event model on tens of servers and besides the bug in this ticket they are doing fine. On one of them we applied the patch (no other changes) and got AH00023 so while I believe there are other ways to trigger it, it seems that the patch also can play a role in it."}, {"count": 38, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 190046, "time": "2016-04-07T14:36:03Z", "creator": "mike.williams@comodo.com", "creation_time": "2016-04-07T14:36:03Z", "is_private": false, "text": "(In reply to Thierry Bastian from comment #33)\n> WE got into a situation where the users of our product were stuck with G.\n> We've got severe performance issues in those cases. We've tried patch\n> https://bz.apache.org/bugzilla/attachment.cgi?id=33158&action=diff on a\n> couple of installs and it made things much much better. On one install it\n> would get stuck with 2000 clients coming in at roughly the same time. Now it\n> can handle 10K gracefully.\n> Hope that helps.\n\n\nI've been trying that today after an update from 2.2.something to 2.4.18.\nStill get the \"scoreboard is full, ...\" error though.\n\n\nOne server looks like this when emitting the \"scoreboard is full, ...\" error, a few moments before becoming entirely unresponsive.\n\n\n179 requests currently being processed, 461 idle workers\nPID\tConnections\tThreads\tAsync connections\ntotal\taccepting\tbusy\tidle\twriting\tkeep-alive\tclosing\n25580\t205\tno\t15\t49\t0\t147\t44\n21331\t293\tno\t0\t0\t0\t0\t292\n19389\t1\tyes\t0\t0\t0\t0\t0\n25924\t164\tno\t12\t52\t0\t151\t0\n23217\t432\tno\t15\t49\t0\t146\t270\n23361\t457\tno\t18\t46\t0\t140\t298\n24175\t458\tno\t13\t51\t0\t149\t297\n20428\t246\tyes\t0\t0\t0\t0\t244\n21641\t439\tno\t17\t47\t0\t145\t283\n21739\t435\tno\t16\t48\t0\t143\t277\n23506\t448\tno\t18\t46\t0\t139\t293\n26180\t30\tyes\t41\t23\t0\t3\t0\n20174\t2\tno\t0\t0\t0\t0\t1\n20527\t209\tno\t0\t0\t0\t0\t208\n22470\t448\tno\t14\t50\t0\t149\t287\n20551\t209\tno\t0\t0\t0\t0\t209\nSum\t4476\t \t179\t461\t0\t1312\t3003\n\nR_R_R______R_R___R_________W_______R__R_R_WR________R__R___R____\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n______W_R_R____R__R__R________________R__RR________R_____R_R____\n___R_RR________WRR____R__R_R______R__________WR______R____R___R_\nR________R______RR__R__RR___R______RR___RRR______R__R___RR_R____\nRR______________W__R_______R_________R_____RRW_____R____RWR_____\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n_____R___R_____R_R_R_RR_R___R_W_R__R__R___R______R____R_R_______\nR______RR__R_R__RR_________R____R___R___RRR________R_______R___R\n_________R__RR_______RR__R___R___R_____RRR____R_R_RR___R____R_W_\nR___R_RRW___RRRR_RRRRRRRR_WRR_RR_RRRRRRRRRRRRRR__RRRRR__________\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n_____W______R__R____R_________R_____R_RWR_RR_R_______R____R_____\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n\n\nShortly afterwards all the Gs are cleared and it gets back to doing useful work for a while.\nSometimes \"a while\" can be 15 minutes, other times less than 1 second."}, {"count": 39, "tags": [], "bug_id": 53555, "text": "As a summary, the problem is that old processes that are shutting down but are still processing some long lasting connetions take up all open scoreboard\nslots. It may be triggered in two ways:\n\na) when doing a graceful restart (apachectl graceful)\n\nb) when the server load goes down in a way that causes httpd to stop some processes. This is particularily problematic because when the load increases again, httpd will try to start more processes. If the pattern repeats, the number of processes can rise quite a bit.\n\nI think two things should be done:\n\n1) Allow to use some extra scoreboard slots for processes that are gracefully shutting down. This is necessary to fix a) and will help a bit with b). To avoid these extra processes taking too much resources, they should try to free resources to the OS as soon as possible. \n\n2) When some process is doing idle shutdown in situation b) and httpd wants more active processes due to rising load, it should not start new processes but rather tell the finishing processes to abort shutdown and resume full\noperation. This helps with b) but not with a). It is also a lot more invasive\nto implement than 1).\n\n\nMy previous patch https://bz.apache.org/bugzilla/attachment.cgi?id=33158 did 1) to some extent by allowing re-use of some scoreboard slots. I will post a new patch in a minute.\n\n\nAs configuration, I recommend (this one is true even if not using any patch):\n\nMaxspareThreads - MinSpareThreads >= 2 * ThreadsPerChild\n\nHigher values of the difference may work better. This reduces the likelyhood of situation b) appearing.", "id": 190150, "time": "2016-04-11T20:56:58Z", "creator": "sf@sfritsch.de", "creation_time": "2016-04-11T20:56:58Z", "is_private": false, "attachment_id": null}, {"count": 40, "tags": [], "bug_id": 53555, "attachment_id": 33749, "id": 190152, "time": "2016-04-11T20:59:05Z", "creator": "sf@sfritsch.de", "creation_time": "2016-04-11T20:59:05Z", "is_private": false, "text": "Created attachment 33749\nAllow to use more scoreboad slots\n\nThe new patch goes a step further and allows in total 10 times as many\nprocesses as configured by MaxRequestWorkers / ThreadsPerChild , though\nServerLimit is still honored. The number 10 is currently hard-coded but would\nprobably be configurable in the end.\n\n\nIf using the patch, you should also set\n\nServerLimit >= 10 * MaxRequestWorkers / ThreadsPerChild\n\nThough a smaller value may make sense if you are short of RAM."}, {"count": 41, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 190153, "time": "2016-04-11T21:05:09Z", "creator": "sf@sfritsch.de", "creation_time": "2016-04-11T21:05:09Z", "is_private": false, "text": "(In reply to Sander Hoentjen from comment #35)\n> After applying the patch I ran into \"No space left on device: AH00023:\n> Couldn't create the proxy mutex\" I haven't seen that issue without the patch.\n> \n> Log says:\n> [Sat Mar 26 07:00:34.857694 2016] [core:emerg] [pid 787770:tid\n> 140551243081696] (28)No space left on device: AH00023: Couldn't create the\n> proxy mutex\n> [Sat Mar 26 07:00:34.857764 2016] [proxy:crit] [pid 787770:tid\n> 140551243081696] (28)No space left on device: AH02478: failed to create\n> proxy mutex\n> AH00016: Configuration Failed\n> \n\nYou could try using different Mutex types. On Linux, pthread may work best. Or  you may try to increase the allowed ressources, possibly shared memory. How that is done depends on your OS."}, {"count": 42, "tags": [], "bug_id": 53555, "text": "Created attachment 33750\nsame as above, but for trunk\n\nAttaching the same patch, but for trunk.\n\n\n(In reply to Stefan Fritsch from comment #40)\n> Created attachment 33749 [details]\n> Allow to use more scoreboad slots\n\nThat patch is for 2.4 and also includes these commits from trunk:\n\nhttps://svn.apache.org/r1703241\nhttps://svn.apache.org/r1705922\nhttps://svn.apache.org/r1706523\nhttps://svn.apache.org/r1738464\nhttps://svn.apache.org/r1738466\nhttps://svn.apache.org/r1738486\nhttps://svn.apache.org/r1738631\nhttps://svn.apache.org/r1738632\nhttps://svn.apache.org/r1738633\nhttps://svn.apache.org/r1738635", "id": 190156, "time": "2016-04-11T21:18:39Z", "creator": "sf@sfritsch.de", "creation_time": "2016-04-11T21:18:39Z", "is_private": false, "attachment_id": 33750}, {"count": 43, "tags": [], "text": "(In reply to Stefan Fritsch from comment #41)\n> (In reply to Sander Hoentjen from comment #35)\n> > After applying the patch I ran into \"No space left on device: AH00023:\n> > Couldn't create the proxy mutex\" I haven't seen that issue without the patch.\n> > \n> > Log says:\n> > [Sat Mar 26 07:00:34.857694 2016] [core:emerg] [pid 787770:tid\n> > 140551243081696] (28)No space left on device: AH00023: Couldn't create the\n> > proxy mutex\n> > [Sat Mar 26 07:00:34.857764 2016] [proxy:crit] [pid 787770:tid\n> > 140551243081696] (28)No space left on device: AH02478: failed to create\n> > proxy mutex\n> > AH00016: Configuration Failed\n> > \n> \n> You could try using different Mutex types. On Linux, pthread may work best.\n> Or  you may try to increase the allowed ressources, possibly shared memory.\n> How that is done depends on your OS.\n\nBut is there anything in the patch that changes this? Because without your patch we never ran into that issue.\nWould the new patch behave differently in this regard?", "is_private": false, "id": 190174, "creator": "sander@hoentjen.eu", "time": "2016-04-12T07:16:04Z", "bug_id": 53555, "creation_time": "2016-04-12T07:16:04Z", "attachment_id": null}, {"count": 44, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 190175, "time": "2016-04-12T07:49:37Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2016-04-12T07:49:37Z", "is_private": false, "text": "(In reply to Sander Hoentjen from comment #43)\n> Would the new patch behave differently in this regard?\n\nYour issue is probably not related to the patch.\nIt is usually caused by an unclean shutdown of httpd (eg. kill -9), or a crash of the parent process (you should see this in the system logs), possibly if you upgraded the binaries while httpd was still running.\nThe number of IPC SysV semaphores is limited on the system, if the previous ones were not cleanly deleted on shutdown, the new startup won't complete.\nAs suggested by Stefan, you could use another Mutex mechanism (pthread) which does not leak on unclean shutdown (even if httpd is killed)."}, {"count": 45, "tags": [], "text": "I was able to manage this issue by reducing GracefulShutdownTimeout value and increasing MaxClients / MaxRequestWorkers value to make more room for Apache scoreboard . \n\nAlso I reduce no of MaxKeepAliveRequests Apache global level. \n\nFor more info :- https://www.tectut.com/2016/04/workaround-for-scoreboard-is-full-not-at-maxrequestworkers", "attachment_id": null, "bug_id": 53555, "id": 190651, "time": "2016-04-30T05:36:48Z", "creator": "shehan9@gmail.com", "creation_time": "2016-04-30T05:36:48Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "gjorgjioski@gmail.com", "text": "Hitting me as well and making lot of troubles. \n\nWhen is this going to be fixed? \n\nWhat it the recommendation for production server? \n\nIs it better if upgrade to 2.4.18? 2.4.10 backport? \n\nor going back to which one is the best for 14.04.5 LTS ?", "count": 46, "id": 193461, "time": "2016-09-02T11:59:02Z", "bug_id": 53555, "creation_time": "2016-09-02T11:59:02Z", "is_private": false}, {"count": 47, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 193462, "time": "2016-09-02T15:24:08Z", "creator": "bucky@mightytikigod.com", "creation_time": "2016-09-02T15:24:08Z", "is_private": false, "text": "(In reply to Valentin Gjorgjioski from comment #46)\n> Hitting me as well and making lot of troubles. \n> Is it better if upgrade to 2.4.18? 2.4.10 backport? \n\nUpgrading to 2.4.18 hasn't helped everyone, but it did help me. The \"centos-sclo-rh\" repository was a solution in my situation."}, {"count": 48, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 193463, "time": "2016-09-02T15:48:45Z", "creator": "toscano.luca@gmail.com", "creation_time": "2016-09-02T15:48:45Z", "is_private": false, "text": "(In reply to Valentin Gjorgjioski from comment #46)\n> Hitting me as well and making lot of troubles. \n\nHi Valentin,\n\ncan you give us a bit more details about your use case? Does the max scoreboard issue happens regularly after certain events or randomly? What is your configuration (if you can share it) and httpd version? It would help a lot :)\n\nLuca"}, {"attachment_id": null, "tags": [], "creator": "gjorgjioski@gmail.com", "text": "Hi,\n\n\nThis started happening after recent upgrade of Ubuntu. Apache was the same, and now it is the same.  Ubuntu is 14.04.5 LTS, Apache is 2.4.7. \n\nThis is high load, production server. Working for 1.5 year without any problems so far. \n\nHere is some log of that update, when the problem started: \n\n[UPGRADE] apache2:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n[UPGRADE] apache2-bin:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n[UPGRADE] apache2-data:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n[UPGRADE] apache2-mpm-worker:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n[UPGRADE] apache2-utils:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n[INSTALL] php5-mysqlnd:amd64\n[UPGRADE] php5-cli:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-common:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-curl:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-fpm:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-gd:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-intl:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-pgsql:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-pspell:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-readline:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-recode:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-sqlite:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-tidy:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-xmlrpc:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5-xsl:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n[UPGRADE] php5:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n\n\nHere is what I nailed it down to: \n1. After this upgrade I needed to DISABLE the opcache in PHP, because problems started with fatal errors and segmentation faults with wordpress. \n2. Because of the 1. the server got even higher load. \n3. Higher load caused full scoreboard, and maxRequestWorkersk. \n\nWhat I found were two problems: \n\n1. When high load occurs and MaxReqeustWorkers is hit, the apache stops responding (dies). It should slow down, should not accept new requests until free slot, but it shouldn't stop responding.  I think I saw this reported somewhere else, e.g.: \nhttps://www.digitalocean.com/community/questions/apache2-crash-on-ubuntu-14-04-maxrequestworkers-issue\n\n2. When I found a way to solve the problem with high load (enable wp cache plugins), now the second problem started, mainly on apache reload (log rotation) or even on regular basis WHEN MaxConnectionsPerChild is different from 0,  and/or when pm.max_requests is different from 0. Why this is a problem - because children are dying after certain numbers of requests, and then they get stuck into \"G\" state, and never completing. This is filling your scoreboard and you are ending with that error. Once you set these to 0, problem more or less disappears. \n\nWorkaround is setting these to 0, and hoping all scripts are good, no memory leaks, lowering memory usage in php.ini, and restaring the server each day (on logrotate restart and not reload). \n\n\nVery important trick that I learned in during this is also this one: ALWAYS restart php-fpm and apache together. Failing to do so leads to some instabilities. \n\nFor me that workaround work, but I would like to hear why this happens, and how we can prevent it (especially the problem when Apache dies when MaxRequestWorkers is readched).", "count": 49, "id": 193464, "time": "2016-09-02T16:38:42Z", "bug_id": 53555, "creation_time": "2016-09-02T16:38:42Z", "is_private": false}, {"count": 50, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 193490, "time": "2016-09-05T13:18:21Z", "creator": "toscano.luca@gmail.com", "creation_time": "2016-09-05T13:18:21Z", "is_private": false, "text": "Thanks a lot for the details Valentin, will try to add my thoughts inline:\n\n(In reply to Valentin Gjorgjioski from comment #49)\n\n> This started happening after recent upgrade of Ubuntu. Apache was the same,\n> and now it is the same.  Ubuntu is 14.04.5 LTS, Apache is 2.4.7. \n\nThis is a very old version of httpd, so if you could if would be really great to upgrade Trusty to something more recent to see the differences.\n\n> This is high load, production server. Working for 1.5 year without any\n> problems so far. \n> \n> Here is some log of that update, when the problem started: \n> \n> [UPGRADE] apache2:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n> [UPGRADE] apache2-bin:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n> [UPGRADE] apache2-data:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n> [UPGRADE] apache2-mpm-worker:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n> [UPGRADE] apache2-utils:amd64 2.4.7-1ubuntu4.9 -> 2.4.7-1ubuntu4.13\n> [INSTALL] php5-mysqlnd:amd64\n> [UPGRADE] php5-cli:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-common:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-curl:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-fpm:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-gd:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-intl:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-pgsql:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-pspell:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-readline:amd64 5.5.9+dfsg-1ubuntu4.14 ->\n> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-recode:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-sqlite:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-tidy:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-xmlrpc:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5-xsl:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> [UPGRADE] php5:amd64 5.5.9+dfsg-1ubuntu4.14 -> 5.5.9+dfsg-1ubuntu4.19\n> \n> \n> Here is what I nailed it down to: \n> 1. After this upgrade I needed to DISABLE the opcache in PHP, because\n> problems started with fatal errors and segmentation faults with wordpress. \n> 2. Because of the 1. the server got even higher load. \n> 3. Higher load caused full scoreboard, and maxRequestWorkersk.\n\nStating the obvious but the httpd issue seems to be a consequence of all the php upgrades happened at the same time. Have you tried to rollback the last upgrade to see if the issue persists?\n\n\n>\n> What I found were two problems: \n> \n> 1. When high load occurs and MaxReqeustWorkers is hit, the apache stops\n> responding (dies). It should slow down, should not accept new requests until\n> free slot, but it shouldn't stop responding.  I think I saw this reported\n> somewhere else, e.g.: \n> https://www.digitalocean.com/community/questions/apache2-crash-on-ubuntu-14-\n> 04-maxrequestworkers-issue\n\nWould you mind to include the logs and/or more details about this? Again it would be really great to know if the problem is the same with a more recent version of httpd. \n\n> \n> 2. When I found a way to solve the problem with high load (enable wp cache\n> plugins), now the second problem started, mainly on apache reload (log\n> rotation) or even on regular basis WHEN MaxConnectionsPerChild is different\n> from 0,  and/or when pm.max_requests is different from 0. Why this is a\n> problem - because children are dying after certain numbers of requests, and\n> then they get stuck into \"G\" state, and never completing. This is filling\n> your scoreboard and you are ending with that error. Once you set these to 0,\n> problem more or less disappears. \n\nDo you have long timeouts (proxy, etc..) in your httpd configuration? This would be a useful information for us, it happened in the past that long proxy timeouts where exacerbating the issue that you described.\n\n> \n> Workaround is setting these to 0, and hoping all scripts are good, no memory\n> leaks, lowering memory usage in php.ini, and restaring the server each day\n> (on logrotate restart and not reload). \n\n> \n> Very important trick that I learned in during this is also this one: ALWAYS\n> restart php-fpm and apache together. Failing to do so leads to some\n> instabilities. \n> \n> For me that workaround work, but I would like to hear why this happens, and\n> how we can prevent it (especially the problem when Apache dies when\n> MaxRequestWorkers is readched).\n\n\nAs written above it would be great to know more about the \"Apache dies\" part. Any detail that you could share with us would be really appreciated.\n\nThanks!\n\nLuca"}, {"count": 51, "tags": [], "text": "Hi Luca,\n\nat the moment upgrading to trusty is not really an option, scared mostly from PHP7, and compatibility issues that might arise. Maybe next year. \n\nHaven't tried to rollback, was not even sure how to do that, and if that is easy. \n\nthe link to digitalocean is another user, but I'm experiencing exactly. Unfortunately nothing in the log. Except the message stated there. \n\nI'm not sure what long timeout is, but probably default of (300seconds?!) for php-fpm using sockets is long. And yes, I guess this exacerbating the issue.  No proxies defined. To me it seems like when some processes hang on php side, they are not getting killed on the apache side and connection is not released. Not even after those 5minutes. It gets stuck there and that's it. \n\nApache dies means - apache processes are there, using no cpu, accepting no connections, and only restart helps. Nothing in the logs.\n\nI just went to prefork. I think it will be stable for now. I had tons of problems these 5 days, I don't know why I didn't switch to prefork earlier. It seems like e good workaround for me right now.", "attachment_id": null, "bug_id": 53555, "id": 193494, "time": "2016-09-05T15:57:47Z", "creator": "gjorgjioski@gmail.com", "creation_time": "2016-09-05T15:57:47Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "gjorgjioski@gmail.com", "text": "Hi,\n\nnow I believe I have clear picture what it is going on:\n\n1. I'm using FastCGI, obviously dead project and not supported ?! \n\n2. I'm not sure whether there is a directive such as connect timeout (fcgid has this). It seems either there is no timeout or it is quite big. \n\n3. When Apache get hardly hit, then php-fpm get hardly hit as well. In my case PHP-FPM started having problems to do its job when I disabled the opcache mentioned earlier. So it get stuck with a longer and longer queue. Then apache continue sending processes to php-gpm even when php-fpm reached the limit (pm.max_children). In such scenario php-fpm stops opening new processes, but somehow old processes get stuck. Then apache continue doing this until full scoreboard. And now CPU usage is very low, it seems like some I/O block, many apache processes (1500?! ) waiting to open socket, but the socket is not available. \n\nHowever, at this point it is not very clear to me why Apache builds up the queue and the queue is not getting emptied  - there is no high processor usage, it seems that php-fpm/apache got stuck and nothing can be done. Could be this apache not handling sockets properly? \n\n4. Even with prefork this happens, it's not the mpm_event problem in this case.\n\n\nWorkaround for the next month or so: Optimize work of PHP, lower the load so PHP-FPM can handl timely. Also, ubuntu upgrade and including more stable php opcache will help towards this. \n\nLong time solution: There must be a solution for this problem in general.  Either it is time to move to nginx, or it is time to move to better module for fastcgi. By the way, what will you sugest at this point, what is the easier migration path from fastcgi to another apache module?", "count": 52, "id": 193495, "time": "2016-09-05T19:20:46Z", "bug_id": 53555, "creation_time": "2016-09-05T19:20:46Z", "is_private": false}, {"count": 53, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 193496, "time": "2016-09-05T20:48:21Z", "creator": "covener@gmail.com", "creation_time": "2016-09-05T20:48:21Z", "is_private": false, "text": "\n> However, at this point it is not very clear to me why Apache builds up the\n> queue and the queue is not getting emptied  - there is no high processor\n> usage, it seems that php-fpm/apache got stuck and nothing can be done. Could\n> be this apache not handling sockets properly? \n\n\nI'd suggest starting a thread on users@httpd.apache.org.\n\nIf you can get this error, you should be able to find some processes trying to exit but hanging on the way out waiting for requests to complete.   Showing their backtrace with gdb (or pstack) will tell us exactly what they're doing.\n\nYour MPM configuration will also tell us if you have unnecessary process churn."}, {"count": 54, "tags": [], "bug_id": 53555, "attachment_id": 34201, "text": "Created attachment 34201\nUse all scoreboard entries up to ServerLimit, for trunk\n\nNew patch: This time use the whole scoreboard up to the configured ServerLimit. Also fixed some issues with the previous patch.", "id": 193497, "time": "2016-09-05T21:45:56Z", "creator": "sf@sfritsch.de", "creation_time": "2016-09-05T21:45:56Z", "is_private": false}, {"count": 55, "tags": [], "bug_id": 53555, "text": "Created attachment 34202\nUse all scoreboard entries up to ServerLimit, for 2.4\n\nSame as above, but for 2.4.\n\nThis contains the trunk patch plus these commits from trunk:\n\nr1705922\nr1706523\nr1738464\nr1738466\nr1738486\nr1738628\nr1738631\nr1738632\nr1738633\nr1738635\nr1756848\nr1757009\nr1757011\nr1757029\nr1757030\nr1757031\nr1757056\nr1757061\n\nIt would be really nice if someone could give this a try in a real-life setup.", "id": 193498, "time": "2016-09-05T21:50:31Z", "creator": "sf@sfritsch.de", "creation_time": "2016-09-05T21:50:31Z", "is_private": false, "attachment_id": 34202}, {"count": 56, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 193499, "time": "2016-09-05T21:58:13Z", "creator": "gjorgjioski@gmail.com", "creation_time": "2016-09-05T21:58:13Z", "is_private": false, "text": "from what I understand, it seems that Apache can't do anything about this, it seems correct behavior. It waits on the socket for its output. Timeouts are high (30 seconds) so on a busy server if all php-fpm processes working on that socket are occupied (not returning result), queue is getting bigger and bigger. \n\nAnd indeed every-time this crashed happened I found timeout in error logs (just for certain web sites), which I have missed previously.  \n\nIt seems like the problem is in php-fpm, that started with my recent upgrade. Problems with the opcache started also there. And I replaced mysql with mysqlnd in that update. So many changes, something was broken, but I think there is nothing wrong with apache. Problem should be either in php-fpm or php-mysqlnd or maybe in the web-sites themselves. \n\n\nAt the end it will be great if apache provides ability to limit number of processes per virtual host (as php-gpm allows this). This way it will be also much easier to isolate/solve the problem."}, {"count": 57, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "Hi Stefan,\n\nthanks for trying to solve the \"scoreboard full\" issue :)\n\nI've been hit by it badly today, the affected machine\nis a forward proxy and stalls the traffic almost completely.\n\nSome background info:\n- event mpm on httpd 2.4.23\n- forward proxy setup via mod_proxy\n- 280 real users + other machines. ~370 clients\n- server load is around 0.2, plenty of free RAM\n- file descriptor limit is 1024\n- logrotate sends a graceful restart every hour\n\nIf the problem occurs, httpd doesn't even respond\nto the /server-status page reliably.\n\nA small script logs the /server-status page every 30s to disk.\nSpecific case: logrotate sends a \"graceful restart\" at 13h.\n\n/server-status output at 13:04:24h:\n-------------------\nTotal accesses: 8801 - Total Traffic: 74.6 MB\n75 requests currently being processed, 125 idle workers\n+---------------------------------------------------------------------------+\n|       |    Connections    |   Threads   |        Async connections        |\n|  PID  |-------------------+-------------+---------------------------------|\n|       | total | accepting | busy | idle | writing | keep-alive | closing ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 14906 | 7     | yes       | 6    | 44   | 0       | 1          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 14959 | 9     | yes       | 9    | 41   | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15014 | 3     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15015 | 49    | yes       | 50   | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15329 | 3     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15893 | 15    | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 17762 | 11    | yes       | 10   | 40   | 0       | 1          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| Sum   | 97    | \u00a0         | 75   | 125  | 0       | 2          | 0       ||\n+---------------------------------------------------------------------------+\n\n_________R_____R__________________R___R___R__R________R______R_R\nR_____R__R_________________R__R____RGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGRRRRRRRRRRRRRRRRRRRRRRRRRRRWRRRRRRRRRRRRRR\nRRRRRRRRGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGR__________R__R_____\n_______R_RR_________R_RR_R____\n-------------------\n\n\n/server-status output at 13:15:25h:\n-------------------\nTotal accesses: 12929 - Total Traffic: 90.9 MB\n87 requests currently being processed, 63 idle workers\n+---------------------------------------------------------------------------+\n|       |    Connections    |   Threads   |        Async connections        |\n|  PID  |-------------------+-------------+---------------------------------|\n|       | total | accepting | busy | idle | writing | keep-alive | closing ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 14906 | 18    | yes       | 16   | 34   | 0       | 2          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 14959 | 27    | yes       | 26   | 24   | 0       | 2          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15014 | 2     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15015 | 2     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15329 | 2     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 18564 | 45    | yes       | 45   | 5    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 17762 | 39    | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 18078 | 44    | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| Sum   | 179   |           | 87   | 63   | 0       | 4          | 0       ||\n+---------------------------------------------------------------------------+\n\n_____R__R___R_RR_RR_R_RR__R_____R_R___R_R_____R___W_RR__RR_RR__R\nRR__R_RR____RRRRR_R_RR___R_RR_RR____GGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGRRRRRR\nRRRRRRRRR_RRRRRRRRR_RRRR_RRRRRRRRRRR_R_RRRRRGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGG\n-------------------\n\n\n/server-status at 13:25:20h:\n(httpd hardly responding anymore):\n-------------------\nTotal accesses: 14630 - Total Traffic: 97.4 MB\n50 requests currently being processed, 0 idle workers\n+---------------------------------------------------------------------------+\n|       |    Connections    |   Threads   |        Async connections        |\n|  PID  |-------------------+-------------+---------------------------------|\n|       | total | accepting | busy | idle | writing | keep-alive | closing ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 14906 | 36    | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 14959 | 2     | yes       | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15014 | 2     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15015 | 2     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 15329 | 2     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 18564 | 50    | yes       | 50   | 0    | 0       | 1          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 17762 | 3     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| 18078 | 1     | no        | 0    | 0    | 0       | 0          | 0       ||\n|-------+-------+-----------+------+------+---------+------------+---------||\n| Sum   | 98    |           | 50   | 0    | 0       | 1          | 0       ||\n+---------------------------------------------------------------------------+\n\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGWRRRRR\nRRRRWRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGG\n-------------------\n\nI can provide more /server-status output if needed.\n\nAfter around 30 mins, the external \"mon\" watchdog\nkills httpd and restarts it. Traffic continues to flow.\n\n\nhttpd config:\n-------------------\nTimeout 300\nKeepAliveTimeout 300\n\n<IfModule mpm_event_module>\n  # Number of concurrent connections is: ServerLimit * ThreadsPerChild\n  # Result: 16 * 50 -> 800\n  #\n  StartServers 1\n  ServerLimit 16\n  ThreadLimit 50\n  ThreadsPerChild 50\n  MaxConnectionsPerChild  1000\n</IfModule>\n\nNo other performance related settings.\n\n-------------------\n\nI've now increased ServerLimit to 32 and disabled\nlogrotate as a quick fix. It holds so far.\nOccasionally I still see the \"scoreboard full\" message,\neven though there are just ~160 active connections and some processes\nare (still?) in the graceful shutdown state.\n\n\nI'll put the patch from #55 on the productive machine tomorrow :o)\nIt already runs on my own proxy and the one from my department.\n\nAnything else to watch out for?\n\nI can provide gdb backtraces if you tell\nme to look for something specific, too.\n\nTriggering a graceful restart during peak traffic might be a good test...\n\nCheers,\nThomas", "id": 194673, "time": "2016-10-25T21:46:14Z", "creator": "thomas.jarosch@intra2net.com", "creation_time": "2016-10-25T21:46:14Z", "is_private": false}, {"count": 58, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 194675, "time": "2016-10-26T06:39:03Z", "creator": "thomas.jarosch@intra2net.com", "creation_time": "2016-10-26T06:39:03Z", "is_private": false, "text": "Another info about my setup:\n\nThere are two other httpd instances running on different ports.\nOne is using the event MPM, the other one prefork MPM.\n\nI didn't configure an explicit ScoreBoardFile, so the scoreboard is in anonymous shared memory. Could there be cross-talk of those three httpds?"}, {"count": 59, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 194678, "time": "2016-10-26T13:44:54Z", "creator": "thomas.jarosch@intra2net.com", "creation_time": "2016-10-26T13:44:54Z", "is_private": false, "text": "Hi Stefan,\n\nthe patch from #55 seems to make things scale a lot better.\nAlso the status output is very helpful.\n\nServerLimit was changed back to 16 before the tests.\nI did a graceful restart at 13:09:35h.\n\n/server-status at 14:19:36h (*before* the next graceful restart):\n-----------------------\nTotal accesses: 23693 - Total Traffic: 200.0 MB\n100 requests currently being processed, 150 idle workers\n+--------------------------------------------------------------------------------------------+\n|      |       |          |    Connections    |   Threads   |       Async connections        |\n| Slot |  PID  | Stopping |-------------------+-------------+--------------------------------|\n|      |       |          | total | accepting | busy | idle | writing | keep-alive | closing |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|0     |19952  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|1     |20006  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|2     |20060  |yes (old  |5      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|3     |20160  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|4     |20224  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|5     |20725  |no        |2      |yes        |2     |48    |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|6     |27470  |no        |50     |yes        |50    |0     |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|7     |24389  |yes       |3      |no         |0     |0     |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|8     |27104  |no        |18     |yes        |18    |32    |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|9     |27346  |no        |3      |yes        |3     |47    |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|10    |22579  |yes       |2      |no         |0     |0     |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|11    |27674  |no        |29     |yes        |27    |23    |0        |3           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|13    |25055  |yes       |8      |no         |0     |0     |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|14    |25350  |yes       |2      |no         |0     |0     |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|15    |25475  |yes       |5      |no         |0     |0     |0        |0           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|Sum   |15     |10        |137    |\u00a0          |100   |150   |0        |3           |0        |\n+--------------------------------------------------------------------------------------------+\n\n.G.G...............G............................................\n..............G.....G.....G.........G..............G............\n.........G.....G...G..................GG........................\n...........................G........G.....................______\n___________R_______________R________________RRRRRRRRRRRRRRRRRRRR\nRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR.....................G............\n.....G...G......___R____R_RR_R__R______RRRRR__R__R______RR__R_R_\n_R________________R__________________R_____________RGG__RRRRRRR_\n_RRRR___R____RR__RR____R__R_W__RRRRR_RRRGGGGGGGGGGGGGGG\n\n-----------------------\n\nAs you can see, there are still processes from \"old gen\" after one hour.\nThis is due to long running HTTP CONNECT requests to google / dropbox / etc.\n\nProbably GracefulShutdownTimeout will help here, may be\nhaving a default value of one hour might make sense\nfor httpd in general?\n\n\nNext graceful restart at 14:19:51h.\n\nErrors start to appear in the log two seconds later:\n\n[Wed Oct 26 14:19:53.926229 2016] [mpm_event:error] [pid 19951:tid 3071850240] AH: scoreboard is full, not at MaxRequestWorkers.Increase ServerLimit.\n\n\n/server-status at 14:20:06h:\n-----------------------\nTotal accesses: 23744 - Total Traffic: 200.9 MB\n8 requests currently being processed, 42 idle workers\n+--------------------------------------------------------------------------------------------+\n|      |       |          |    Connections    |   Threads   |       Async connections        |\n| Slot |  PID  | Stopping |-------------------+-------------+--------------------------------|\n|      |       |          | total | accepting | busy | idle | writing | keep-alive | closing |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|0     |19952  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|1     |20006  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|2     |20060  |yes (old  |5      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|3     |20160  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|4     |20224  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|5     |20725  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|6     |27470  |yes (old  |42     |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|7     |24389  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|8     |27104  |yes (old  |18     |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|9     |27346  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|10    |22579  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|11    |27674  |yes (old  |24     |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|12    |28054  |no        |9      |yes        |8     |42    |0        |2           |0        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|13    |25055  |yes (old  |8      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|14    |25350  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|15    |25475  |yes (old  |5      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|Sum   |16     |15        |133    |\u00a0          |8     |42    |0        |2           |0        |\n+--------------------------------------------------------------------------------------------+\n\n.G.G...............G............................................\n..............G.....G.....G.........G..............G............\n.........G.....G...G..................GG........................\n...........................G........G...........................\n...........G...............G................G.GGGGG.G.G..GGGGGG.\nGGGGGGGGGGGGGG.GGGGGGGGGGG.GGG.....................G............\n.....G...G......GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG_\n______RRRR____RRRW_______________________________GGGGGGGGGGGGGGG\n-----------------------\n\n\n\nThe forward proxy became unresponsive again.\n/server-status at 14:29:16h:\n-----------------------\nTotal accesses: 24453 - Total Traffic: 226.8 MB\n50 requests currently being processed, 0 idle workers\n+--------------------------------------------------------------------------------------------+\n|      |       |          |    Connections    |   Threads   |       Async connections        |\n| Slot |  PID  | Stopping |-------------------+-------------+--------------------------------|\n|      |       |          | total | accepting | busy | idle | writing | keep-alive | closing |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|0     |19952  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|1     |20006  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|2     |20060  |yes (old  |5      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|3     |20160  |yes (old  |1      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|4     |20224  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|5     |20725  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|6     |27470  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|7     |24389  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|8     |27104  |yes (old  |1      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|9     |27346  |yes (old  |1      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|10    |22579  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|11    |27674  |yes (old  |3      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|12    |28054  |no        |51     |yes        |50    |0     |0        |0           |1        |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|13    |25055  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|14    |25350  |yes (old  |2      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|15    |25475  |yes (old  |4      |no         |0     |0     |0        |0           |0        |\n|      |       |gen)      |       |           |      |      |         |            |         |\n|------+-------+----------+-------+-----------+------+------+---------+------------+---------|\n|Sum   |16     |15        |86     |\u00a0          |50    |0     |0        |0           |1        |\n+--------------------------------------------------------------------------------------------+\n\n.G.G...............G............................................\n..............G.....G.....G.........G..............G............\n.........G.....G...G...................G........................\n...........................G........G...........................\n...........G...............G....................................\n...........G.............G.........................G............\n.....G..........GGGGGGGRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\nRRRRRRRWRGGGGGGGG\n-----------------------\n\nAs you can see, there was plenty of room in the scoreboard now,\nbut the process list slots were used up by old processes\nserving just a handful of connections.\n\n\nOne option would be to increase ServerLimit to let's say 128,\nbut that also raises the resource limits during normal operation.\nIf I raise ServerLimit too much, I have to lower the thread count again.\nSounds a bit like the prefork mpm...\n\nAnother option would be to add a config setting to ignore\nprocesses for the ServerLimit calculation if they are\nin graceful shutdown mode. They probably don't consume\na lot of resources and we can have a GracefulShutdownTimeout\nof one hour to expire them, too.\n\nThird option (preferred one): Have an own GracefulShutdownLimit\nthat's separate from ServerLimit. If we have too many processes,\nstart killing of oldest process from the graceful shutdown list.\nProcess in graceful shutdown mode don't count for ServerLimit.\n\n\nI've raised ServerLimit to 32 on the box again.\nThe users can't be annoyed too much ;)\n\nCheers,\nThomas\n\nPS: Forget about the idea about cross-talk of anonymous shared memory segments from #58. It's not the case."}, {"count": 60, "tags": [], "text": "(In reply to Thomas Jarosch from comment #59)\n> the patch from #55 seems to make things scale a lot better.\n> Also the status output is very helpful.\n\nGlad to hear that and thanks for testing it.\n\n> As you can see, there are still processes from \"old gen\" after one hour.\n> This is due to long running HTTP CONNECT requests to google / dropbox / etc.\n\nThere is no way to determine if such connections can be \"safely\" interrupted or if they are in the middle of a long download.\n\n> \n> Probably GracefulShutdownTimeout will help here, may be\n> having a default value of one hour might make sense\n> for httpd in general?\n\nCurrently the children won't honor GracefulShutdownTimeout. But that should be added.\n\n> As you can see, there was plenty of room in the scoreboard now,\n> but the process list slots were used up by old processes\n> serving just a handful of connections.\n> \n> \n> One option would be to increase ServerLimit to let's say 128,\n> but that also raises the resource limits during normal operation.\n> If I raise ServerLimit too much, I have to lower the thread count again.\n> Sounds a bit like the prefork mpm...\n\nDuring normal operation, the number of threads will be limited by MaxRequestWorkers. The idea of my patch is that you can increase Serverlimit quite a bit without using too many ressources. The processes serving old connections should terminate most of their threads and free most of their memory, so the resource usage should not be too much. But it of course depends on how may old connections are still open.\n\n> Another option would be to add a config setting to ignore\n> processes for the ServerLimit calculation if they are\n> in graceful shutdown mode. They probably don't consume\n> a lot of resources and we can have a GracefulShutdownTimeout\n> of one hour to expire them, too.\n\nYou are confusing ServerLimit with MaxRequestWorkers here. While the latter is a number of threads and not processes, it does what you think ServerLimit should do.\n\n> Third option (preferred one): Have an own GracefulShutdownLimit\n> that's separate from ServerLimit. If we have too many processes,\n> start killing of oldest process from the graceful shutdown list.\n> Process in graceful shutdown mode don't count for ServerLimit.\n\nYes, we could do that, too. But first I need something like GracefulShutdownTimeout to work for the old child processes.\n\n\nIf you have any more experiences with the patch I am certainly interested. Even if it has simply run for some time without (new) bugs exposed.\n\nCheers,\nStefan", "is_private": false, "id": 194815, "creator": "sf@sfritsch.de", "time": "2016-11-04T13:57:23Z", "bug_id": 53555, "creation_time": "2016-11-04T13:57:23Z", "attachment_id": null}, {"count": 61, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "Some quick note about the patch (unfortunately I could not carry out my testing since a colleague reused the machine, resetting my local patches/work altogether...).\n\nAnyway, there is possibly an issue with retained->total_daemons which is incremented (unconditionally) whenever a child is created (make_child), but not always decremented when one finishes (server_main_loop, depending on whether or not it died smoothly and it still uses a scoreboard slot).\n\nIOW, I think this hunk:\n                 ps->quiescing = 0;\n+                retained->total_daemons--;\n\nshould probably be moved up here:\n         ap_wait_or_timeout(&exitwhy, &status, &pid, pconf, ap_server_conf);\n         if (pid.pid != -1) {\n+            retained->total_daemons--;\n\nWill restart my tests ASAP...", "id": 194828, "time": "2016-11-04T23:04:30Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2016-11-04T23:04:30Z", "is_private": false}, {"count": 62, "tags": [], "bug_id": 53555, "attachment_id": null, "id": 194846, "time": "2016-11-06T20:36:40Z", "creator": "sf@sfritsch.de", "creation_time": "2016-11-06T20:36:40Z", "is_private": false, "text": "(In reply to Yann Ylavic from comment #61)\n> Anyway, there is possibly an issue with retained->total_daemons which is\n> incremented (unconditionally) whenever a child is created (make_child), but\n> not always decremented when one finishes (server_main_loop, depending on\n> whether or not it died smoothly and it still uses a scoreboard slot).\n> \n> IOW, I think this hunk:\n>                  ps->quiescing = 0;\n> +                retained->total_daemons--;\n> \n> should probably be moved up here:\n>          ap_wait_or_timeout(&exitwhy, &status, &pid, pconf, ap_server_conf);\n>          if (pid.pid != -1) {\n> +            retained->total_daemons--;\n\nNo, I think the code in the patch is correct: There is only one case where the code will return from the function before reaching the \"if (child_slot >= 0) {\" block which contains the \"retained->total_daemons--;\" line. And in this case the whole server will exit, so correct counting is not an issue any more.\n\nOn the other hand, total_daemons must not be decremented if child_slot < 0, because in this case the dead process was not a worker process (but e.g. a cgid-process).\n\nBut this should be made clearer, either by rearranging the code or by adding some comments."}, {"count": 63, "tags": [], "text": "We have successfully used patch in #55 for 50 days now on mid-sized production server with 1-2 million hits per day. No issues encountered. Previous issues disappeared (we think the original bug had been abused in DoS attack, but we might be wrong on this).", "attachment_id": null, "bug_id": 53555, "id": 194848, "time": "2016-11-07T10:24:40Z", "creator": "email_apache.org@tikon.ch", "creation_time": "2016-11-07T10:24:40Z", "is_private": false}, {"attachment_id": 34202, "tags": [], "creator": "jim@apache.org", "is_private": false, "count": 64, "id": 195122, "time": "2016-11-21T20:21:36Z", "bug_id": 53555, "creation_time": "2016-11-21T20:21:36Z", "text": "Comment on attachment 34202\nUse all scoreboard entries up to ServerLimit, for 2.4\n\nThis looks good. Should be proposed for back port!!"}, {"count": 65, "tags": [], "bug_id": 53555, "attachment_id": null, "text": "Rest of the trunk patch committed as\n\nr1770750\nr1770752", "id": 195127, "time": "2016-11-21T20:48:11Z", "creator": "sf@sfritsch.de", "creation_time": "2016-11-21T20:48:11Z", "is_private": false}, {"count": 66, "tags": [], "bug_id": 53555, "text": "Fixed in 2.4.25", "id": 195740, "time": "2016-12-31T00:18:31Z", "creator": "covener@gmail.com", "creation_time": "2016-12-31T00:18:31Z", "is_private": false, "attachment_id": null}, {"count": 67, "tags": [], "text": "Hi Stefan,\n\n(In reply to Stefan Fritsch from comment #60)\n> > the patch from #55 seems to make things scale a lot better.\n> > Also the status output is very helpful.\n> \n> Glad to hear that and thanks for testing it.\n\nSorry, I didn't see your reply as bugzilla\ndidn't add me to CC: automatically. Which is rather\nodd since it's the default setting. \n\nBack to the topic:\n\n> > Probably GracefulShutdownTimeout will help here, may be\n> > having a default value of one hour might make sense\n> > for httpd in general?\n> \n> Currently the children won't honor GracefulShutdownTimeout. But that should\n> be added.\n\nvery nice.\n\n> > Third option (preferred one): Have an own GracefulShutdownLimit\n> > that's separate from ServerLimit. If we have too many processes,\n> > start killing of oldest process from the graceful shutdown list.\n> > Process in graceful shutdown mode don't count for ServerLimit.\n> \n> Yes, we could do that, too. But first I need something like\n> GracefulShutdownTimeout to work for the old child processes.\n\nok. \n\nIn the meantime I've decreased the ServerLimit/ThreadLimit to 5 and increased the ServerLimit 160 and more. The results with these settings are very good, no more user complaints (see below).\n\nOtherwise those long running HTTP CONNECT sessions were still maxing out the total number of allowed processes.\n\n> If you have any more experiences with the patch I am certainly interested.\n> Even if it has simply run for some time without (new) bugs exposed.\n\nthe patch had been deployed to about ~3.000 servers since November 2016 with different work loads from 10 users to 400+ users. After applying your patch + the ThreadLimit change, there were no more complaints :)\n\nI've also diffed httpd 2.4.23 + the patch with the version of the code that landed in 2.4.25 and it's exactly the same. I'm soon going to roll out 2.4.25 to those boxes.\n\nThanks again!\nThomas", "attachment_id": null, "bug_id": 53555, "id": 196393, "time": "2017-01-25T12:00:44Z", "creator": "thomas.jarosch@intra2net.com", "creation_time": "2017-01-25T12:00:44Z", "is_private": false}, {"count": 68, "tags": [], "text": "*** Bug 56101 has been marked as a duplicate of this bug. ***", "attachment_id": null, "bug_id": 53555, "id": 196589, "time": "2017-01-31T10:06:49Z", "creator": "toscano.luca@gmail.com", "creation_time": "2017-01-31T10:06:49Z", "is_private": false}]