[{"count": 0, "tags": [], "creator": "Srinivas.Srirangam@ge.com", "text": "We have got a multi-threaded enterprise application running on highly loaded \nweblogic server. We are getting this below error very frequently and causing \nour production server to go down. \n\nPls see below Thread trace :\n\n\"ExecuteThread: '33' for queue: 'weblogic.kernel.Default'\" daemon prio=5 \ntid=0x30a8f0 nid=0x2d waiting for monitor entry [8fcfe000..8fd019bc]\n\tat org.apache.log4j.RollingFileAppender.rollOver\n(RollingFileAppender.java:126)\n\tat org.apache.log4j.RollingFileAppender.subAppend\n(RollingFileAppender.java:228)\n\tat org.apache.log4j.WriterAppender.append(WriterAppender.java:150)\n\tat org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:221)\n\t- locked <b71560a8> (a org.apache.log4j.RollingFileAppender)\n\tat org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders\n(AppenderAttachableImpl.java:57)\n\tat org.apache.log4j.Category.callAppenders(Category.java:187)\n\t- locked <b715d2d0> (a org.apache.log4j.Logger)\n\tat org.apache.log4j.Category.forcedLog(Category.java:372)\n\nThread dump 1/4\n\" Thread-25402 <threadFilter.do?mode=ModeAllThreads&FullThreadIndex=0>\" prio=5 \ntid=0x14f92d0 nid=0x638b waiting for monitor entry [8a301000..8a3019bc]\n    at org.apache.log4j.Category.callAppenders(Category.java:185)\n    - waiting to lock <b715d2d0> (a org.apache.log4j.Logger)\n    at org.apache.log4j.Category.forcedLog(Category.java:372)\n    at org.apache.log4j.Category.log(Category.java:864)\n\nIs there any know solution/patch for this error?\n\nThanks\nSrinivas", "id": 97087, "time": "2006-12-19T09:29:35Z", "bug_id": 41214, "creation_time": "2006-12-19T09:29:35Z", "is_private": false, "attachment_id": null}, {"attachment_id": null, "tags": [], "bug_id": 41214, "text": "*** Bug 41215 has been marked as a duplicate of this bug. ***", "count": 1, "id": 97090, "time": "2006-12-19T09:47:33Z", "creator": "Srinivas.Srirangam@ge.com", "creation_time": "2006-12-19T09:47:33Z", "is_private": false}, {"count": 2, "tags": [], "bug_id": 41214, "attachment_id": null, "text": "Hello Srinivas,\n\nWhich exact version of log4j are you using? Which JDK?", "id": 97091, "time": "2006-12-19T09:55:01Z", "creator": "ceki@apache.org", "creation_time": "2006-12-19T09:55:01Z", "is_private": false}, {"count": 3, "attachment_id": null, "creator": "Srinivas.Srirangam@ge.com", "text": "Hello Ceki,\n\nWe are currently using log4j which is of two to 3 years old(don't have source \ncode and not sure how to check version of log4j). JDK version is 1.3.1\n\nThanks,\nSrinivas\n", "id": 97092, "time": "2006-12-19T11:22:55Z", "bug_id": 41214, "creation_time": "2006-12-19T11:22:55Z", "tags": [], "is_private": false}, {"count": 4, "tags": [], "creator": "Srinivas.Srirangam@ge.com", "text": "From log4j.dtd i can see the version as 1.2\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n\n<!-- Authors: Chris Taylor, Ceki Gulcu. -->\n\n<!-- Version: 1.2 -->", "id": 97094, "time": "2006-12-19T13:45:46Z", "bug_id": 41214, "creation_time": "2006-12-19T13:45:46Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 41214, "text": "\nIt must be an early version in the 1.2 series. What is the size in bytes of the\nlog4j.jar file? By the way, the log4j jar file usually contains the version\nnumber in its name.\n\n", "id": 97110, "time": "2006-12-20T00:54:23Z", "creator": "ceki@apache.org", "creation_time": "2006-12-20T00:54:23Z", "is_private": false, "attachment_id": null}, {"attachment_id": null, "tags": [], "bug_id": 41214, "text": "The size in bytes is 352,668 bytes. It looks like my earlier team here has just \nrenamed the file to log4.jar.\n\n", "count": 6, "id": 97123, "time": "2006-12-20T06:53:03Z", "creator": "Srinivas.Srirangam@ge.com", "creation_time": "2006-12-20T06:53:03Z", "is_private": false}, {"count": 7, "tags": [], "bug_id": 41214, "text": "The file size is the same as log4j-1.2.8.jar, however the line number from RollingFileAppender does not \ncorrespond to any synchronization block in that version (the link number from Category does).  If you \ncould do a \"unzip -l log4j.jar\", you should see the timestamps within the jar file.  log4j-1.2.8.jar would \nhave timestamps of 2003-02-20.  If you extract the members of log4j-1.2.8.jar (\"jar xf log4j-1.2.8.jar\"), \nthe META-INF/MANIFEST.MF file would have an Implementation-Version of 1.2.8.\n\nI've reviewed the subversion history for RollingFileAppender in the v1_2-branch.  Line 126 falls within \nRFA.rollOver in revisions 309239 (2001-07-20) and later.  The implementation of RFA has not been \nsubstantially changed in that period and that line number corresponds to either a blank line, a File \nconstructor or a File.exists call, none of which should acquire a lock on a category.", "id": 97128, "time": "2006-12-20T09:34:49Z", "creator": "carnold@apache.org", "creation_time": "2006-12-20T09:34:49Z", "is_private": false, "attachment_id": null}, {"count": 8, "tags": [], "bug_id": 41214, "text": "I just unzipped the jar and find the timestamps as : 02/20/2003\n\nHere's the content of Manifest file:\n\nManifest-Version: 1.0\nCreated-By: Apache Ant 1.5.1\n\nName: org/apache/log4j/\nImplementation-Title: log4j\nImplementation-Version: 1.2.8\nImplementation-Vendor: \"Apache Software Foundation\"", "id": 97129, "attachment_id": null, "creator": "Srinivas.Srirangam@ge.com", "creation_time": "2006-12-20T09:47:23Z", "time": "2006-12-20T09:47:23Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 41214, "text": "I could not find the 1.2.8 distribution via log4j's binary download page [1]. It\nappears that log4j versions prior to log4j 1.2.9 are no longer available.\n\nHowever, looking at the source code as available on Subversion[2], it is not\neasy to identify the code holding locks. Line 126 of RollingFileAppender\nreads:\n\n  target = new File(fileName + '.' + (i + 1));\n\nIs it possible that the File constructors tries to access a lock and gets\nblocked? It may be a JDK 1.3 problem more than a log4j bug.\n\n\n[1] http://logging.apache.org/site/binindex.cgi\n[2]\nhttp://svn.apache.org/viewvc/logging/log4j/tags/v1_2_7/src/java/org/apache/log4j/RollingFileAppender.java?annotate=309639", "count": 9, "id": 97398, "time": "2006-12-28T01:49:47Z", "creator": "ceki@apache.org", "creation_time": "2006-12-28T01:49:47Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "Srinivas.Srirangam@ge.com", "is_private": false, "count": 10, "id": 97539, "time": "2007-01-02T10:46:57Z", "bug_id": 41214, "creation_time": "2007-01-02T10:46:57Z", "text": "\nDo you think this issue will get resolved, if i update the existing version of \nlog4j to latest version. \n\nThanks,\nSrinivas"}, {"count": 11, "tags": [], "bug_id": 41214, "attachment_id": null, "text": "\nI am sorry to say that given the difficulty to identify the problem,\nthere is no guarantee that upgrading will resolve the matter. Having\nsaid that, upgrading might help, either by solving the problem or by\nhelping us identify it.\n", "id": 97720, "time": "2007-01-06T09:53:25Z", "creator": "ceki@apache.org", "creation_time": "2007-01-06T09:53:25Z", "is_private": false}, {"text": "Pls let me know if you want me to send the log4j binary file that we were using \nin our application.\n\nThanks,\nSrinivas", "tags": [], "bug_id": 41214, "attachment_id": null, "count": 12, "id": 97775, "time": "2007-01-08T06:39:19Z", "creator": "Srinivas.Srirangam@ge.com", "creation_time": "2007-01-08T06:39:19Z", "is_private": false}, {"text": "\n\nBtw, the 1.2.8 log4j download seems to be linked at the bottom of the page\n('earlier versions') -> http://archive.apache.org/dist/logging/log4j/ .\n\nIt's MD5 is 18a4ca847248e5b8606325684342701c. Your manifest matches the 1.2.8\nmanifest.\n\nLine 185 of Category synchronizes on the Category and then its parents (in the\nfirst stack... this ends up in RollingFileAppender), and as Ceki said, appears\nto be blocked in the File call.\nThe second stack is waiting to enter that lock.\n\nIs it always the same stack trace blocked on line 126 of RollingFileAppender? If\nso, upgrading the JVM (or switching from Sun to IBM or IBM to Sun) might be of\nvalue - though I suspect you're constrained by the vendor's versioning.\n\nKnowing the platform and vendor of the JVM might be of value to anyone else who\nhits the problem, but I suspect there's little log4j could be doing in this\nsituation.", "tags": [], "bug_id": 41214, "attachment_id": null, "count": 13, "id": 98262, "time": "2007-01-19T07:40:59Z", "creator": "bayard@apache.org", "creation_time": "2007-01-19T07:40:59Z", "is_private": false}, {"count": 14, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 98768, "time": "2007-01-30T10:15:46Z", "creator": "jared.oberhaus@scalent.com", "creation_time": "2007-01-30T10:15:46Z", "is_private": false, "text": "I have also seen this happen at least once. A thread just sits in the writeBytes call:\n\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:260)\n\tat sun.nio.cs.StreamEncoder$CharsetSE.writeBytes(StreamEncoder.java:336)\n\tat sun.nio.cs.StreamEncoder$CharsetSE.implFlushBuffer(StreamEncoder.java:404)\n\tat sun.nio.cs.StreamEncoder$CharsetSE.implFlush(StreamEncoder.java:408)\n\tat sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:152)\n\tat java.io.OutputStreamWriter.flush(OutputStreamWriter.java:213)\n\tat org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:57)\n\tat org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:315)\n\tat org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:236)\n\tat org.apache.log4j.WriterAppender.append(WriterAppender.java:159)\n\tat org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230)\n\tat org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders\n(AppenderAttachableImpl.java:65)\n\tat org.apache.log4j.Category.callAppenders(Category.java:203)\n\tat org.apache.log4j.Category.forcedLog(Category.java:388)\n\tat org.apache.log4j.Category.log(Category.java:835)\n\tat com.scalent.shared.util.Logger.log(Logger.java:352)\n\nI am using:\n\nlog4j 1.2.14\n\n$ java -v\njava version \"1.5.0_08\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_08-b03)\nJava HotSpot(TM) Server VM (build 1.5.0_08-b03, mixed mode)\n\n$ uname -a\nSunOS build-sparc10 5.10 Generic sun4u sparc SUNW,Sun-Fire-V210"}, {"attachment_id": null, "tags": [], "creator": "jared.oberhaus@scalent.com", "is_private": false, "count": 15, "id": 98774, "time": "2007-01-30T10:56:08Z", "bug_id": 41214, "creation_time": "2007-01-30T10:56:08Z", "text": "I'm very confused about what could be causing this, but here's some more information that I have:\n\n* We run the same code with the same JVM version on Linux 2.6, Solaris (Sparc/x86), and Windows.\n* We see these problems only on Solaris machines (Sparc and x86).\n* We have hundreds of unit tests, and they all using logging a lot. In several consecutive runs, the exact \nsame logging statement as described in Additional Comment #14 caused the deadlock, not any other of \nthe thousands of logging statements.\n* I see one other case during this test run of a deadlock, which is described below.\n* Ocassionally I will see a similar deadlock in a completely different piece of code that has nothing to do \nwith log4j. A thread will be sitting in Object.wait(), when I know it must have had Object.notify() \ndelivered to it. I have never seen this happen with the same unit test on Linux/Windows. This unit test \nruns many times a day on all our platforms. This makes me think this problem is a very rare bug in \nSolaris JVM or Solaris itself, that is triggered by a specific sequence of events in our code. Although that \nseems unlikely...\n\nHere's another instance of our \"deadlock\":\n\tat org.apache.log4j.helpers.CountingQuietWriter.write(CountingQuietWriter.java:45)\n\tat org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:301)\n\tat org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:236)\n\tat org.apache.log4j.WriterAppender.append(WriterAppender.java:159)\n\tat org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230)\n\tat org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders\n(AppenderAttachableImpl.java:65)\n\tat org.apache.log4j.Category.callAppenders(Category.java:203)\n\tat org.apache.log4j.Category.forcedLog(Category.java:388)\n\tat org.apache.log4j.Category.log(Category.java:835)\n\tat com.scalent.shared.util.Logger.log(Logger.java:352)\n"}, {"count": 16, "tags": [], "text": "*** Bug 42213 has been marked as a duplicate of this bug. ***", "attachment_id": null, "bug_id": 41214, "id": 107116, "time": "2007-08-22T20:06:45Z", "creator": "carnold@apache.org", "creation_time": "2007-08-22T20:06:45Z", "is_private": false}, {"count": 17, "attachment_id": null, "bug_id": 41214, "text": "We yesterday had  - on a linux box - the same situation:\n\njava -version =\n\njava version \"1.5.0_11\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_11-b03)\nJava HotSpot(TM) Server VM (build 1.5.0_11-b03, mixed mode)\n\nlog4J Version is 1.2.8\n\n\n2007-10-17 14:37:14,207 ERROR\n[net.portrix.powerdial.adapters.ceemes.CeemesListener] CeemesListener\ntimeout/deadlock Stacktraces:\n\"OpenCallGroup Thread\"  prio=5 Thread id=36 BLOCKED\n        org.apache.log4j.Category.callAppenders(Category.java:185)\n        org.apache.log4j.Category.forcedLog(Category.java:372)\n        org.apache.log4j.Category.debug(Category.java:241)\n       \nnet.portrix.powerdial.adapters.asterisk.AsteriskCall.setState(AsteriskCall.java:216)\n       \nnet.portrix.powerdial.adapters.asterisk.AsteriskCall.eventStart(AsteriskCall.java:51)\n       \nnet.portrix.powerdial.adapters.asterisk.AsteriskCallManager.startCall(AsteriskCallManager.java:220)\n--\n\n2007-10-17 14:37:14,207 ERROR\n[net.portrix.powerdial.adapters.ceemes.CeemesListener] CeemesListener\ntimeout/deadlock Stacktraces:\n\"ManagerConnection-0-Reader-0\" daemon prio=5 Thread id=30 BLOCKED\n        org.apache.log4j.Category.callAppenders(Category.java:185)\n        org.apache.log4j.Category.forcedLog(Category.java:372)\n        org.apache.log4j.Category.debug(Category.java:241)\n       \nnet.portrix.powerdial.adapters.asterisk.AsteriskCallManager.onManagerEvent(AsteriskCallManager.java:873)\n       \norg.asteriskjava.manager.internal.ManagerConnectionImpl.fireEvent(ManagerConnectionImpl.java:1121)\n       \norg.asteriskjava.manager.internal.ManagerConnectionImpl.dispatchEvent(ManagerConnectionImpl.java:1105)\n", "id": 109447, "time": "2007-10-18T00:55:24Z", "creator": "k.mueller@portrix.net", "creation_time": "2007-10-18T00:55:24Z", "tags": [], "is_private": false}, {"attachment_id": null, "tags": [], "creator": "dima@mailvision.com", "is_private": false, "count": 18, "id": 112300, "time": "2007-12-30T07:34:56Z", "bug_id": 41214, "creation_time": "2007-12-30T07:34:56Z", "text": "Found one Java-level deadlock:\n=============================\n\"http-8080-8\":\n  waiting to lock monitor 0x0809e77c (object 0x7b8aff60, a org.apache.log4j.Logger),\n  which is held by \"pool-1-thread-6\"\n\"pool-1-thread-6\":\n  waiting to lock monitor 0x0809e6bc (object 0x7b8a2258, a\norg.apache.log4j.RollingFileAppender),\n  which is held by \"http-8080-8\"\n\nJava stack information for the threads listed above:\n===================================================\n\"http-8080-8\":\n\tat org.apache.log4j.Category.callAppenders(Category.java:204)\n\t- waiting to lock <0x7b8aff60> (a org.apache.log4j.Logger)\n\tat org.apache.log4j.Category.forcedLog(Category.java:391)\n\tat org.apache.log4j.Category.log(Category.java:856)\n\tat org.apache.commons.logging.impl.Log4JLogger.debug(Log4JLogger.java:110)\n\tat org.hibernate.jdbc.AbstractBatcher.log(AbstractBatcher.java:401)\n\tat\norg.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:482)\n\tat\norg.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:423)\n\tat\norg.hibernate.jdbc.AbstractBatcher.prepareQueryStatement(AbstractBatcher.java:139)\n\tat org.hibernate.loader.Loader.prepareQueryStatement(Loader.java:1547)\n\tat org.hibernate.loader.Loader.doQuery(Loader.java:673)\n\tat\norg.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:236)\n\tat org.hibernate.loader.Loader.loadCollection(Loader.java:1994)\n\tat\norg.hibernate.loader.collection.CollectionLoader.initialize(CollectionLoader.java:36)\n\tat\norg.hibernate.persister.collection.AbstractCollectionPersister.initialize(AbstractCollectionPersister.java:565)\n\tat\norg.hibernate.event.def.DefaultInitializeCollectionEventListener.onInitializeCollection(DefaultInitializeCollectionEventListener.java:60)\n\tat org.hibernate.impl.SessionImpl.initializeCollection(SessionImpl.java:1716)\n\tat\norg.hibernate.collection.AbstractPersistentCollection.initialize(AbstractPersistentCollection.java:344)\n\tat\norg.hibernate.collection.AbstractPersistentCollection.read(AbstractPersistentCollection.java:86)\n\tat org.hibernate.collection.PersistentSet.toString(PersistentSet.java:309)\n\tat java.lang.String.valueOf(String.java:2615)\n\tat java.lang.StringBuilder.append(StringBuilder.java:116)\n\tat com.company.ws.product.services.commons.User.toString(User.java:314)\n\tat sun.reflect.GeneratedMethodAccessor1319.invoke(Unknown Source)\n\tat\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:585)\n\tat\norg.hibernate.proxy.pojo.cglib.CGLIBLazyInitializer.invoke(CGLIBLazyInitializer.java:157)\n\tat\ncom.company.ws.product.services.commons.User$$EnhancerByCGLIB$$9ab22e0e.toString(<generated>)\n\tat java.lang.String.valueOf(String.java:2615)\n\tat java.lang.StringBuilder.append(StringBuilder.java:116)\n\tat\ncom.company.ws.product.criterion.SimpleExpression.toString(SimpleExpression.java:125)\n\tat java.lang.String.valueOf(String.java:2615)\n\tat java.util.AbstractCollection.toString(AbstractCollection.java:454)\n\tat java.lang.String.valueOf(String.java:2615)\n\tat java.lang.StringBuilder.append(StringBuilder.java:116)\n\tat com.company.ws.product.SearchCriteria.toString(SearchCriteria.java:377)\n\tat org.apache.log4j.or.DefaultRenderer.doRender(DefaultRenderer.java:36)\n\tat org.apache.log4j.or.RendererMap.findAndRender(RendererMap.java:80)\n\tat org.apache.log4j.spi.LoggingEvent.getRenderedMessage(LoggingEvent.java:362)\n\tat\norg.apache.log4j.helpers.PatternParser$BasicPatternConverter.convert(PatternParser.java:403)\n\tat org.apache.log4j.helpers.PatternConverter.format(PatternConverter.java:65)\n\tat org.apache.log4j.PatternLayout.format(PatternLayout.java:502)\n\tat org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:302)\n\tat org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:263)\n\tat org.apache.log4j.WriterAppender.append(WriterAppender.java:160)\n\tat org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n\t- locked <0x7b8a2258> (a org.apache.log4j.RollingFileAppender)\n\tat\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n\tat org.apache.log4j.Category.callAppenders(Category.java:206)\n\t- locked <0x7b8af630> (a org.apache.log4j.Logger)\n\tat org.apache.log4j.Category.forcedLog(Category.java:391)\n\tat org.apache.log4j.Logger.trace(Logger.java:172)\n\tat\ncom.company.ws.product.HibernateUtil.getHibernateCriteria(HibernateUtil.java:611)\n\tat com.company.ws.product.HibernateUtil.get(HibernateUtil.java:1075)\n\tat com.company.ws.product.HibernateUtil.get(HibernateUtil.java:1028)\n\tat\ncom.company.ws.product.services.callscreening.CallScreeningManagerImpl.get(CallScreeningManagerImpl.java:58)\n\tat\ncom.company.ws.product.services.location.engine.LocationTargetsMgr.getExtensionLocationDialPlan(LocationTargetsMgr.java:686)\n\tat\ncom.company.ws.product.services.location.engine.LocationTargetsMgr.getExtensionLocationEntity(LocationTargetsMgr.java:457)\n\tat\ncom.company.ws.product.services.location.engine.LocationTargetsMgr.getLocationTargets(LocationTargetsMgr.java:224)\n\tat\ncom.company.ws.product.services.location.engine.LocationEngineServiceImpl.getLocationTargets(LocationEngineServiceImpl.java:59)\n\tat sun.reflect.GeneratedMethodAccessor1128.invoke(Unknown Source)\n\tat\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:585)\n\tat com.sun.xml.ws.api.server.InstanceResolver$1.invoke(InstanceResolver.java:246)\n\tat com.sun.xml.ws.server.InvokerTube$2.invoke(InvokerTube.java:146)\n\tat\ncom.sun.xml.ws.server.sei.EndpointMethodHandler.invoke(EndpointMethodHandler.java:257)\n\tat com.sun.xml.ws.server.sei.SEIInvokerTube.processRequest(SEIInvokerTube.java:93)\n\tat com.sun.xml.ws.api.pipe.Fiber.__doRun(Fiber.java:595)\n\tat com.sun.xml.ws.api.pipe.Fiber._doRun(Fiber.java:554)\n\tat com.sun.xml.ws.api.pipe.Fiber.doRun(Fiber.java:539)\n\tat com.sun.xml.ws.api.pipe.Fiber.runSync(Fiber.java:436)\n\t- locked <0x7f5ce188> (a com.sun.xml.ws.api.pipe.Fiber)\n\tat com.sun.xml.ws.server.WSEndpointImpl$2.process(WSEndpointImpl.java:243)\n\tat\ncom.sun.xml.ws.transport.http.HttpAdapter$HttpToolkit.handle(HttpAdapter.java:444)\n\tat com.sun.xml.ws.transport.http.HttpAdapter.handle(HttpAdapter.java:244)\n\tat\ncom.sun.xml.ws.transport.http.servlet.ServletAdapter.handle(ServletAdapter.java:135)\n\tat\ncom.sun.xml.ws.transport.http.servlet.WSServletDelegate.doGet(WSServletDelegate.java:129)\n\tat\ncom.sun.xml.ws.transport.http.servlet.WSServletDelegate.doPost(WSServletDelegate.java:160)\n\tat com.sun.xml.ws.transport.http.servlet.WSServlet.doPost(WSServlet.java:75)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:710)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:803)\n\tat\norg.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n\tat\norg.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n\tat\norg.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n\tat\norg.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n\tat\norg.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:263)\n\tat org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n\tat\norg.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:584)\n\tat org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n\tat java.lang.Thread.run(Thread.java:595)\n\"pool-1-thread-6\":\n\tat org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:231)\n\t- waiting to lock <0x7b8a2258> (a org.apache.log4j.RollingFileAppender)\n\tat\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n\tat org.apache.log4j.Category.callAppenders(Category.java:206)\n\t- locked <0x7b8aff60> (a org.apache.log4j.Logger)\n\tat org.apache.log4j.Category.forcedLog(Category.java:391)\n\tat org.apache.log4j.Category.log(Category.java:856)\n\tat org.apache.commons.logging.impl.Log4JLogger.debug(Log4JLogger.java:110)\n\tat org.hibernate.jdbc.AbstractBatcher.log(AbstractBatcher.java:401)\n\tat\norg.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:482)\n\tat\norg.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:423)\n\tat\norg.hibernate.jdbc.AbstractBatcher.prepareQueryStatement(AbstractBatcher.java:139)\n\tat org.hibernate.loader.Loader.prepareQueryStatement(Loader.java:1547)\n\tat org.hibernate.loader.Loader.doQuery(Loader.java:673)\n\tat\norg.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:236)\n\tat org.hibernate.loader.Loader.doList(Loader.java:2220)\n\tat org.hibernate.loader.Loader.listUsingQueryCache(Loader.java:2136)\n\tat org.hibernate.loader.Loader.list(Loader.java:2096)\n\tat org.hibernate.loader.criteria.CriteriaLoader.list(CriteriaLoader.java:94)\n\tat org.hibernate.impl.SessionImpl.list(SessionImpl.java:1569)\n\tat org.hibernate.impl.CriteriaImpl.list(CriteriaImpl.java:283)\n\tat com.company.ws.product.HibernateUtil.get(HibernateUtil.java:1082)\n\tat\ncom.company.ws.product.services.callmonitor.CallMonitorManagerImpl.getCallMonEntries(CallMonitorManagerImpl.java:89)\n\tat\ncom.company.ws.product.services.callmonitor.CallMonitorManagerImpl.locateEntry(CallMonitorManagerImpl.java:209)\n\tat\ncom.company.ws.product.services.callmonitor.CallMonitorManagerImpl.handleCallUpdatedEvent(CallMonitorManagerImpl.java:184)\n\tat\ncom.company.product.CallStateTransactionalHandlerList.handleCallUpdatedEvent(CallStateTransactionalHandlerList.java:95)\n\tat com.company.product.CallUpdateSubJob.call(CallUpdateSubJob.java:46)\n\tat com.company.product.CallUpdateSubJob.call(CallUpdateSubJob.java:18)\n\tat com.company.mvutil.collection.MultiPartJob.executeSubJob(MultiPartJob.java:114)\n\tat com.company.mvutil.collection.MultiPartJob.executeSubJobs(MultiPartJob.java:96)\n\tat com.company.mvutil.collection.MultiPartJob.run(MultiPartJob.java:60)\n\tat\njava.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)\n\tat java.lang.Thread.run(Thread.java:595)\n\tat\ncom.company.product.CallStateDispatcher$SessionedThread.run(CallStateDispatcher.java:313)\n\nFound 1 deadlock.\n\n\nroot /opt/tomcat/logs > /usr/java/j2se5/bin/java -version\njava version \"1.5.0_12\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_12-b04)\nJava HotSpot(TM) Client VM (build 1.5.0_12-b04, mixed mode)\nroot@qamd2 /opt/tomcat/logs > uname -a\nLinux qamd2 2.6.16.33-xen #1 SMP Thu Mar 22 12:04:09 IST 2007 x86_64 x86_64\nx86_64 GNU/Linux\nroot@qamd2 /opt/tomcat/logs >\n"}, {"attachment_id": null, "tags": [], "creator": "benoitx@yahoo.com", "text": "We have the same deadlock issue under windows XP and log4j-1.2.15. Is there a\nworkaround? Is it specific to using the AsyncAppender with RollingFileAppender?\n\n\"movementMDPListenerContainer-1\" prio=6 tid=0x5edfa8d0 nid=0x1bcc waiting for\nmonitor entry [0x60c5d000..0x60c5fa18]\n        at org.apache.log4j.Category.callAppenders(Category.java:185)\n        - waiting to lock <0x13b78530> (a org.apache.log4j.spi.RootCategory)\n        at org.apache.log4j.Category.forcedLog(Category.java:372)\n        at org.apache.log4j.Category.log(Category.java:864)\n        at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)\n        at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.commons.logging.impl.Log4jProxy.log(Log4jProxy.java:288)\n        at org.apache.commons.logging.impl.Log4jProxy.debug(Log4jProxy.java:235)\n        at org.apache.commons.logging.impl.Log4JLogger.debug(Log4JLogger.java:84)\n        at org.hibernate.jdbc.AbstractBatcher.log(AbstractBatcher.java:393)\n        at\norg.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:474)\n        at\norg.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:415)\n        at\norg.hibernate.jdbc.AbstractBatcher.prepareQueryStatement(AbstractBatcher.java:139)\n        at org.hibernate.loader.Loader.prepareQueryStatement(Loader.java:1560)\n        at org.hibernate.loader.Loader.doQuery(Loader.java:661)\n        at\norg.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:224)\n        at org.hibernate.loader.Loader.doList(Loader.java:2144)\n        at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2028)\n        at org.hibernate.loader.Loader.list(Loader.java:2023)\n        at org.hibernate.loader.criteria.CriteriaLoader.list(CriteriaLoader.java:95)\n        at org.hibernate.impl.SessionImpl.list(SessionImpl.java:1569)", "count": 19, "id": 112876, "time": "2008-01-15T08:54:57Z", "bug_id": 41214, "creation_time": "2008-01-15T08:54:57Z", "is_private": false}, {"count": 20, "attachment_id": null, "bug_id": 41214, "text": "Sorry I forgot to post the other thread:\n\nI believe that there is a conflict in the use of synchronized between the\nAsyncAppender.append and the Category.callAppenders; Any solution available???\n\n\"movementMDPListenerContainer-2\" prio=6 tid=0x5f1efe28 nid=0x1f8c in\nObject.wait() [0x60b1f000..0x60b1fc98]\n        at java.lang.Object.wait(Native Method)\n        - waiting on <0x13b80ce8> (a org.apache.log4j.helpers.BoundedFIFO)\n        at java.lang.Object.wait(Object.java:474)\n        at org.apache.log4j.AsyncAppender.append(AsyncAppender.java:85)\n        - locked <0x13b80ce8> (a org.apache.log4j.helpers.BoundedFIFO)\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:221)\n        - locked <0x13b65418> (a org.apache.log4j.AsyncAppender)\n        at\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:57)\n        at org.apache.log4j.Category.callAppenders(Category.java:187)\n        - locked <0x13b78530> (a org.apache.log4j.spi.RootCategory)\n        at org.apache.log4j.Category.forcedLog(Category.java:372)\n        at org.apache.log4j.Category.info(Category.java:674)\n\n(In reply to comment #19)\n> We have the same deadlock issue under windows XP and log4j-1.2.15. Is there a\n> workaround? Is it specific to using the AsyncAppender with RollingFileAppender?\n> \n> \"movementMDPListenerContainer-1\" prio=6 tid=0x5edfa8d0 nid=0x1bcc waiting for\n> monitor entry [0x60c5d000..0x60c5fa18]\n>         at org.apache.log4j.Category.callAppenders(Category.java:185)\n>         - waiting to lock <0x13b78530> (a org.apache.log4j.spi.RootCategory)\n>         at org.apache.log4j.Category.forcedLog(Category.java:372)\n>         at org.apache.log4j.Category.log(Category.java:864)\n>         at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)\n>         at\n>\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n>         at java.lang.reflect.Method.invoke(Method.java:585)\n>         at org.apache.commons.logging.impl.Log4jProxy.log(Log4jProxy.java:288)\n>         at org.apache.commons.logging.impl.Log4jProxy.debug(Log4jProxy.java:235)\n>         at org.apache.commons.logging.impl.Log4JLogger.debug(Log4JLogger.java:84)\n>         at org.hibernate.jdbc.AbstractBatcher.log(AbstractBatcher.java:393)\n>         at\n> org.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:474)\n>         at\n> org.hibernate.jdbc.AbstractBatcher.getPreparedStatement(AbstractBatcher.java:415)\n>         at\n> org.hibernate.jdbc.AbstractBatcher.prepareQueryStatement(AbstractBatcher.java:139)\n>         at org.hibernate.loader.Loader.prepareQueryStatement(Loader.java:1560)\n>         at org.hibernate.loader.Loader.doQuery(Loader.java:661)\n>         at\n>\norg.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:224)\n>         at org.hibernate.loader.Loader.doList(Loader.java:2144)\n>         at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2028)\n>         at org.hibernate.loader.Loader.list(Loader.java:2023)\n>         at\norg.hibernate.loader.criteria.CriteriaLoader.list(CriteriaLoader.java:95)\n>         at org.hibernate.impl.SessionImpl.list(SessionImpl.java:1569)\n\n", "id": 112877, "time": "2008-01-15T09:03:32Z", "creator": "benoitx@yahoo.com", "creation_time": "2008-01-15T09:03:32Z", "tags": [], "is_private": false}, {"count": 21, "attachment_id": null, "creator": "jared.oberhaus@scalent.com", "text": "Take a look at bug44157. I'm not sure I can say that these are related, but bug44157 was what was \ncausing most of our deadlocks on Solaris only, because we would interrupt threads. See my Comment #14 \nand Comment #15.", "id": 112916, "time": "2008-01-16T10:32:40Z", "bug_id": 41214, "creation_time": "2008-01-16T10:32:40Z", "tags": [], "is_private": false}, {"count": 22, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 112961, "time": "2008-01-17T09:32:57Z", "creator": "carnold@apache.org", "creation_time": "2008-01-17T09:32:57Z", "is_private": false, "text": "Comments 18-20 do not appear to be related to the initial reported bug which from all the discussion \nappears to be a Solaris JVM bug and not a log4j bug.\n\nComment 18 appears to be occurring due to Hibernate using an Object as a message and that Object's \ntoString() method in turn triggers logging calls.  Unfortunately log4j 1.2 is not designed to be \nreentrant, you can't make log4j logging calls reliably from within log4j.  Options would be to configure \nan Object renderer that would avoid calling the problematic toString() call or removing the logging calls \nin the problematic toString() call.\n\nThe stack traces in 19 and 20 clearly look like the problematic pre-log4j 1.2.14 AsyncAppender which \nhad many synchronization issues.  The suggest resolution there would be to upgrade to log4j 1.2.15 or \nlater.\n\nIf there are new issues, then should be logged as new bug reports."}, {"count": 23, "tags": [], "bug_id": 41214, "attachment_id": null, "text": "If this is resolved in 1.2.15 does that mean that this issue is resolved as fixed?", "id": 119268, "time": "2008-08-02T12:50:16Z", "creator": "thorbjoern@gmail.com", "creation_time": "2008-08-02T12:50:16Z", "is_private": false}, {"text": "It's somewhat unclear from the above posts if we're talking about a deadlock or just a lock contention/scalability issue. I can confirm that there does seem to be a scalability issue. This is with log4j 1.2.5, using it in a webapp that runs under tomcat, so there will be tens or hundreds of threads calling in to log4j simultaneosly.\n\nWhat were seeing is typically myriads of these:\n\"TP-Processor28\" daemon prio=10 tid=0x08449800 nid=0x361 waiting for monitor ent\nry [0x06c6d000..0x06c6e050]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.log4j.Category.callAppenders(Category.java:204)\n        - waiting to lock <0x841e4b10> (a org.apache.log4j.spi.RootLogger)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.debug(Category.java:260)\nAll blocking against one that looks like similar to this:\n\"TP-Processor120\" daemon prio=10 tid=0x0852e400 nid=0x4c6 waiting for monitor en\ntry [0x07c81000..0x07c821d0]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.log4j.Category.callAppenders(Category.java:204)\n        - locked <0x841e4b10> (a org.apache.log4j.spi.RootLogger)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.info(Category.java:666)\n\nThis isn't a deadlock, it's just showing that the coding in Category.callAppenders doesn't scale well, it synchronizes at a too high level:\n  public\n  void callAppenders(LoggingEvent event) {\n    int writes = 0;\n\n    for(Category c = this; c != null; c=c.parent) {\n      // Protected against simultaneous call to addAppender, removeAppender,...\n      synchronized(c) {\n        if(c.aai != null) {\n          writes += c.aai.appendLoopOnAppenders(event);\n        }\n        if(!c.additive) {\n          break;\n        }\n      }\n    }\n\n    if(writes == 0) {\n      repository.emitNoAppenderWarning(this);\n    }\n  }\n\nNow, fixing the overeager synchronization here would just expose the next bottleneck, which is the AppenderSkeleton.doAppend. synchronization should be moved down to the append method I guess.", "tags": [], "bug_id": 41214, "attachment_id": null, "count": 24, "id": 120515, "time": "2008-09-10T07:15:17Z", "creator": "robert@manamind.com", "creation_time": "2008-09-10T07:15:17Z", "is_private": false}, {"count": 25, "tags": [], "text": "The problem is pretty consistent and seems to be happening on high traffic servers. Here is one that I caught on my weblogic server. This lock will eventually bring the server down since the request processor / servlet engines will be unable to process any further requests.\n\nI am not sure about the lock scalability thing. My logs indicate a positive deadlock condition. Is this going to be fixed anytime soon ? \n\njava version \"1.5.0_06\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_06-b05)\nBEA JRockit(R) (build R26.4.0-63_CR302700-72606-1.5.0_06-20061127-1108-win-ia32, )\n\nOS - Windows 2000 SP4\n\nLog4j version - 1.2.15\n\nServer - Weblogic 10 SP1\n\nDeadlocks encountered on a web application built on Struts 1.1\n\nCircular (deadlocked) lock chains\n\n=================================\nChain 2:\n=================================\n\n            \"[STUCK] ExecuteThread: '39' for queue: 'weblogic.kernel.Default (self-tuning)'\" id=11799 idx=0xee tid=6520 waiting for org/apache/log4j/Logger@0x05AE0EF0 held by:\n\n            \"[STUCK] ExecuteThread: '28' for queue: 'weblogic.kernel.Default (self-tuning)'\" id=6044 idx=0x9c tid=6496 waiting for org/apache/log4j/RollingFileAppender@0x0B1E4CB8 held by:\n\n            \"[STUCK] ExecuteThread: '39' for queue: 'weblogic.kernel.Default (self-tuning)'\" id=11799 idx=0xee tid=6520\n\n\n=================================\nThread 39\n=================================\n\n \"[STUCK] ExecuteThread: '39' for queue: 'weblogic.kernel.Default (self-tuning)'\" id=11799 idx=0xee tid=6520 prio=1 alive, in native, blocked, daemon\n\n          \n                -- Blocked trying to get lock: org/apache/log4j/Logger@0x05AE0EF0[thin lock]\n\n          \n                at jrockit/vm/Threads.sleep(I)V(Native Method)\n\n          \n                at jrockit/vm/Locks.waitForThinRelease(Ljava/lang/Object;I)I(Unknown Source)\n\n          \n                at jrockit/vm/Locks.monitorEnterSecondStage(Ljava/lang/Object;I)Ljava/lang/Object;(Unknown Source)\n\n          \n                at jrockit/vm/Locks.monitorEnter(Ljava/lang/Object;)Ljava/lang/Object;(Unknown Source)\n\n          \n                at org/apache/log4j/Category.callAppenders(Lorg/apache/log4j/spi/LoggingEvent;)V(Category.java:201)\n\n          \n                at org/apache/log4j/Category.forcedLog(Ljava/lang/String;Lorg/apache/log4j/Priority;Ljava/lang/Object;Ljava/lang/Throwable;)V(Category.java:388)\n\n          \n                at org/apache/log4j/Category.debug(Ljava/lang/Object;)V(Category.java:257)\n\n=================================\nThread 28\n=================================\n\n\"[STUCK] ExecuteThread: '28' for queue: 'weblogic.kernel.Default (self-tuning)'\" id=6044 idx=0x9c tid=6496 prio=1 alive, in native, blocked, daemon\n\n          \n                -- Blocked trying to get lock: org/apache/log4j/RollingFileAppender@0x0B1E4CB8[thin lock]\n\n          \n                at jrockit/vm/Threads.sleep(I)V(Native Method)\n\n          \n                at jrockit/vm/Locks.waitForThinRelease(Ljava/lang/Object;I)I(Unknown Source)\n\n          \n                at jrockit/vm/Locks.monitorEnterSecondStage(Ljava/lang/Object;I)Ljava/lang/Object;(Unknown Source)\n\n          \n                at jrockit/vm/Locks.monitorEnter(Ljava/lang/Object;)Ljava/lang/Object;(Unknown Source)\n\n          \n                at org/apache/log4j/AppenderSkeleton.doAppend(Lorg/apache/log4j/spi/LoggingEvent;)V(AppenderSkeleton.java:210)\n\n          \n                at org/apache/log4j/helpers/AppenderAttachableImpl.appendLoopOnAppenders(Lorg/apache/log4j/spi/LoggingEvent;)I(AppenderAttachableImpl.java:65)\n\n          \n                at org/apache/log4j/Category.callAppenders(Lorg/apache/log4j/spi/LoggingEvent;)V(Category.java:203)\n\n          \n                ^-- Holding lock: org/apache/log4j/Logger@0x05AE0EF0[thin lock]\n\n          \n                at org/apache/log4j/Category.forcedLog(Ljava/lang/String;Lorg/apache/log4j/Priority;Ljava/lang/Object;Ljava/lang/Throwable;)V(Category.java:388)\n\n          \n                at org/apache/log4j/Category.debug(Ljava/lang/Object;)V(Category.java:257)\n\n          \n", "attachment_id": null, "id": 122950, "creator": "bdeepak.x@gmail.com", "time": "2008-12-01T00:41:17Z", "bug_id": 41214, "creation_time": "2008-12-01T00:41:17Z", "is_private": false}, {"count": 26, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 126430, "time": "2009-04-22T05:26:55Z", "creator": "vanantharamu@airvana.com", "creation_time": "2009-04-22T05:26:55Z", "is_private": false, "text": "Yet another case of deadlock we encountered, \nThis is reported by the Java Threaddump, in the log4JAPi, causing the process going down.\nThis problem seems to be very frequent as the load of the server is quite high.\n\nThe Thread dump indicates that the deadlock is caused by the \"RollingFileAppender\". \n\nPlease reply in here, do we have any solution boiled down on this issue.  \n\nWe are on Java Version : java version \"1.5.0_11\"\nLog4J version : log4j-1.2.13\nThe platform is Sun solaris \"SunOS XXX 5.10 Generic_127111-11 sun4u sparc SUNW,Sun-Fire-V240\"\n\nThe Deadloack threaddump encountered snippet \n\nFound one Java-level deadlock:\n=============================\n\"AlarmCounter-Thread\":\n  waiting to lock monitor 0x00149d20 (object 0xa86e6988, a java.io.PrintStream),\n  which is held by \"EventListenerThreadPool:Thread-1087\"\n\"EventListenerThreadPool:Thread-1087\":\n  waiting to lock monitor 0x00149c90 (object 0xa6804bd8, a org.apache.log4j.RollingFileAppender),\n  which is held by \"AlarmCounter-Thread\"\n\nJava stack information for the threads listed above:\n===================================================\n\"AlarmCounter-Thread\":\n        at java.io.PrintStream.println(Unknown Source)\n        - waiting to lock <0xa86e6988> (a java.io.PrintStream)\n        at org.apache.log4j.helpers.LogLog.error(LogLog.java:142)\n        at org.apache.log4j.helpers.OnlyOnceErrorHandler.error(OnlyOnceErrorHandler.java:77)\n        at org.apache.log4j.helpers.OnlyOnceErrorHandler.error(OnlyOnceErrorHandler.java:67)\n        at org.apache.log4j.helpers.CountingQuietWriter.write(CountingQuietWriter.java:48)\n        at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:301)\n        at org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:234)\n        at org.apache.log4j.WriterAppender.append(WriterAppender.java:159)\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230)\n        - locked <0xa6804bd8> (a org.apache.log4j.RollingFileAppender)\n        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65)\n        at org.apache.log4j.Category.callAppenders(Category.java:203)\n        - locked <0xa680b0b8> (a org.apache.log4j.Logger)\n        at org.apache.log4j.Category.forcedLog(Category.java:388)\n        at org.apache.log4j.Category.error(Category.java:319)\n        at com.airvana.faultServer.db.SQLUtil.executeQuery(SQLUtil.java:480)\n        at com.airvana.faultServer.db.SQLUtil.executeQuery(SQLUtil.java:496)\n        at com.airvana.faultServer.db.AlarmCounter.initializeFromDb(AlarmCounter.java:71)\n        - locked <0xa8710e60> (a com.airvana.faultServer.db.AlarmCounter)\n        at com.airvana.faultServer.db.AlarmCounter.run(AlarmCounter.java:173)\n        at java.lang.Thread.run(Unknown Source)\n\"EventListenerThreadPool:Thread-1087\":\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:210)\n        - waiting to lock <0xa6804bd8> (a org.apache.log4j.RollingFileAppender)\n        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65)\n        at org.apache.log4j.Category.callAppenders(Category.java:203)\n        - locked <0xa68047a8> (a org.apache.log4j.spi.RootLogger)\n        at org.apache.log4j.Category.forcedLog(Category.java:388)\n        at org.apache.log4j.Category.log(Category.java:835)\n        at com.airvana.faultServer.utils.LoggingOutputStream.flush(LoggingOutputStream.java:189)\n        at java.io.PrintStream.write(Unknown Source)\n        - locked <0xa86e6988> (a java.io.PrintStream)\n        at sun.nio.cs.StreamEncoder$CharsetSE.writeBytes(Unknown Source)\n        at sun.nio.cs.StreamEncoder$CharsetSE.implWrite(Unknown Source)\n        at sun.nio.cs.StreamEncoder.write(Unknown Source)\n        - locked <0xa8809018> (a java.io.OutputStreamWriter)\n        at java.io.OutputStreamWriter.write(Unknown Source)\n        at java.io.BufferedWriter.flushBuffer(Unknown Source)\n        - locked <0xa8809018> (a java.io.OutputStreamWriter)\n        at java.io.BufferedWriter.write(Unknown Source)\n        - locked <0xa8809018> (a java.io.OutputStreamWriter)\n        at java.io.Writer.write(Unknown Source)\n        at java.io.PrintStream.write(Unknown Source)\n        - locked <0xa86e6988> (a java.io.PrintStream)\n        at java.io.PrintStream.print(Unknown Source)\n        at java.lang.ThreadGroup.uncaughtException(Unknown Source)\n        at java.lang.ThreadGroup.uncaughtException(Unknown Source)\n        at java.lang.Thread.dispatchUncaughtException(Unknown Source)\n\nFound 1 deadlock."}, {"count": 27, "tags": [], "bug_id": 41214, "text": "Hi All,\n\nI wanted to add more data points on behalf of Vijay (post #26) above.\n\n1. The disk space on the partition to which the logs are written was 100% full at some point in time on the server. Though we have not tried to reproduce it this way yet, but needed some confirmation on Log4J behavior under this condition. We are not even sure whether disk full can cause this issue.\n\nCould this have caused the deadlock? How does Log4J behave upon a disk full condition?\n\n2. As per the stack trace thread (EventListenerThreadPool:Thread-1087) had encountered an uncaught-exception. We are not yet sure what this exception was, BUT sadly that same call/thread 'deadlocked' and no logs (diskfull?) got dumped.\n\n.\n.\n at java.io.PrintStream.print(Unknown Source)\n at java.lang.ThreadGroup.uncaughtException(Unknown Source)\n at java.lang.ThreadGroup.uncaughtException(Unknown Source)\n at java.lang.Thread.dispatchUncaughtException(Unknown Source)\n\nI am not sure whether this is a known issue/behavior in Log4J. But wanted to understand how we could resolve this. Will upgrading to Log4J 1.2.15 help? 1.2.14 did have deadlock fixes for AsyncAppender, but RollingFileAppender also seem to have the same bug.", "id": 126431, "attachment_id": null, "creator": "mkhanna@gmail.com", "creation_time": "2009-04-22T05:53:07Z", "time": "2009-04-22T05:53:07Z", "is_private": false}, {"count": 28, "tags": [], "bug_id": 41214, "attachment_id": null, "text": "Hi \n\nWe are using Log4j 1.2_14 under jboss environment (i.e log4j-boot.jar). Frequent jvm crashes are happening in high production environments because of Log4j RollingFileAppender. \n\nLog Level is DEBUG.\n\nHere is the stack trace:\n\nThread: http-0.0.0.0-8080-4 : priority:5, demon:true, threadId:104, threadState:BLOCKED, lockName:org.jboss.logging.appender.RollingFileAppender@718bc0c4\n\n                org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:210)\n                org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65)\n                org.apache.log4j.Category.forcedLog(Category.java:388)\n                org.apache.log4j.Category.info(Category.java:663)\n\n\nApplied patch proposed by http://marc.info/?l=log4j-dev&m=121271153819013&w=2 but still problem  doesn't solved.\n\nThis is the problem with synchronization and multiple threads like 5000 or more per sec (in production)trying to access the application and respective log statements will be called for each thread. \n\nOur application strictly enforces using smartDebug, smartError, smartInfo etc...  \n\nTried using AysncAppender with the above patch. It solved the synchronization problem in callAppenders() method but now AppenderSkeleton's doAppend() is synchronized.\n\norg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:231)\n                org.apache.log4j.helpers.AppenderAttachableImpl5.appendLoopOnAppenders(AppenderAttachableImpl5.java:79)\n                org.apache.log4j.Category.callAppenders(Category.java:203)\n                org.apache.log4j.Category.forcedLog(Category.java:387)\n                org.apache.log4j.Category.info(Category.java:662)\n\nFor sure this is a synchronization problem irrespective on which application sever you are using. Synchronization needs to be moved to lower level i.e (append method).\n\nJboss tightly integrates log4j and applying patch is also cumbersome.\n\nBefore posting here tried every single option to solve the issue but no luck.", "id": 129054, "time": "2009-07-22T08:23:42Z", "creator": "satesh.x.anumala@chase.com", "creation_time": "2009-07-22T08:23:42Z", "is_private": false}, {"count": 29, "tags": [], "text": "Any updates on this?\n\nI have the same problem:\n\n@see:\n\n\"Thread-6\" daemon prio=10 tid=0x00007feb02a29800 nid=0x387f waiting for monitor entry [0x00000000418bb000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.log4j.Category.callAppenders(Category.java:204)\n        - waiting to lock <0x00007feb091fab30> (a org.apache.log4j.spi.RootLogger)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.log(Category.java:856)\n        at org.apache.commons.logging.impl.Log4JLogger.debug(Log4JLogger.java:177)\n        at org.apache.jcs.engine.memory.shrinking.ShrinkerThread.shrink(ShrinkerThread.java:124)\n        at org.apache.jcs.engine.memory.shrinking.ShrinkerThread.run(ShrinkerThread.java:96)\n        at EDU.oswego.cs.dl.util.concurrent.ClockDaemon$RunLoop.run(Unknown Source)\n        at java.lang.Thread.run(Thread.java:619)", "attachment_id": null, "id": 136343, "creator": "tobias.lindenmann@1und1.de", "time": "2010-04-23T05:55:59Z", "bug_id": 41214, "creation_time": "2010-04-23T05:55:59Z", "is_private": false}, {"count": 30, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 136612, "time": "2010-05-03T17:06:07Z", "creator": "rob_gar_esp@hotmail.com", "creation_time": "2010-05-03T17:06:07Z", "is_private": false, "text": "(In reply to comment #29)\n> Any updates on this?\n> \n> I have the same problem:\n>\n> \"Thread-6\" daemon prio=10 tid=0x00007feb02a29800 nid=0x387f waiting for monitor\n> entry [0x00000000418bb000]\n>    java.lang.Thread.State: BLOCKED (on object monitor)\n>         at org.apache.log4j.Category.callAppenders(Category.java:204)\n>         - waiting to lock <0x00007feb091fab30> (a\n\n\nI have a very similar problem:\n\n\"http-13180-143\" daemon prio=10 tid=0x00002aac3c5bb000 nid=0x262c2 waiting for monitor entry\n[0x00002aac300bc000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.log4j.Category.callAppenders(Category.java:201)\n        - locked <0x00002aaab8919168> (a org.apache.log4j.spi.RootLogger)\n        at org.apache.log4j.Category.forcedLog(Category.java:388)\n        at org.apache.log4j.Category.error(Category.java:302)\n\n\nPlease assist."}, {"count": 31, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 137096, "time": "2010-05-25T05:45:51Z", "creator": "nchakraborty@airvana.com", "creation_time": "2010-05-25T05:45:51Z", "is_private": false, "text": "Below Log4J dead lock was seen when running on 1.2.15. We have upgraded to 1.2.16 now, but not sure whether this issue is already fixed in the latest version.\n\n\n\nFound one Java-level deadlock:\n=============================\n\"pool-3-thread-13\":\n  waiting to lock monitor 0x001782d8 (object 0xa85627f0, a java.io.PrintStream),\n  which is held by \"pool-3-thread-14\"\n\"pool-3-thread-14\":\n  waiting to lock monitor 0x00178320 (object 0xa6400ee0, a org.apache.log4j.RollingFileAppender),\n  which is held by \"pool-3-thread-13\"\n\nJava stack information for the threads listed above:\n===================================================\n\"pool-3-thread-13\":\n        at java.lang.Throwable.printStackTrace(Throwable.java:462)\n        - waiting to lock <0xa85627f0> (a java.io.PrintStream)\n        at java.lang.Throwable.printStackTrace(Throwable.java:452)\n        at org.apache.log4j.helpers.LogLog.error(LogLog.java:145)\n        at org.apache.log4j.helpers.OnlyOnceErrorHandler.error(OnlyOnceErrorHandler.java:78)\n        at org.apache.log4j.helpers.OnlyOnceErrorHandler.error(OnlyOnceErrorHandler.java:68)\n        at org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:60)\n        at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:316)\n        at org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:263)\n        at org.apache.log4j.WriterAppender.append(WriterAppender.java:160)\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n        - locked <0xa6400ee0> (a org.apache.log4j.RollingFileAppender)\n        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n        at org.apache.log4j.Category.callAppenders(Category.java:206)\n        - locked <0xa640eed8> (a org.apache.log4j.Logger)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.warn(Category.java:1043)\n        at com.airvana.faultServer.eventEngine.AirvanaPollServer.handleNodeNotReachableState(AirvanaPollServer.java:192)\n        at com.airvana.faultServer.niohandlers.NioNotificationHandler.declareNodeUnreachable(NioNotificationHandler.java:593)\n        at com.airvana.faultServer.niohandlers.NioNotificationHandler.channelDisconnected(NioNotificationHandler.java:198)\n        - locked <0xa86e8d48> (a com.airvana.faultServer.db.NodeObj)\n        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:137)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:567)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:803)\n        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:76)\n        at org.jboss.netty.handler.execution.OrderedMemoryAwareThreadPoolExecutor$ChildExecutor.run(OrderedMemoryAwareThreadPoolExecutor.java:314)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:651)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:676)\n        at java.lang.Thread.run(Thread.java:595)\n\"pool-3-thread-14\":\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:231)\n        - waiting to lock <0xa6400ee0> (a org.apache.log4j.RollingFileAppender)\n        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n        at org.apache.log4j.Category.callAppenders(Category.java:206)\n        - locked <0xa6400b18> (a org.apache.log4j.spi.RootLogger)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.log(Category.java:838)\n        at com.airvana.faultServer.utils.LoggingOutputStream.flush(LoggingOutputStream.java:185)\n        at java.io.PrintStream.write(PrintStream.java:414)\n        - locked <0xa85627f0> (a java.io.PrintStream)\n        at sun.nio.cs.StreamEncoder$CharsetSE.writeBytes(StreamEncoder.java:336)\n        at sun.nio.cs.StreamEncoder$CharsetSE.implFlushBuffer(StreamEncoder.java:404)\n        at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:115)\n        - locked <0xa8c04e00> (a java.io.OutputStreamWriter)\n        at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:169)\n        at java.io.PrintStream.write(PrintStream.java:459)\n        - locked <0xa85627f0> (a java.io.PrintStream)\n        at java.io.PrintStream.print(PrintStream.java:602)\n        at java.io.PrintStream.println(PrintStream.java:739)\n        - locked <0xa85627f0> (a java.io.PrintStream)\n        at org.apache.log4j.helpers.LogLog.error(LogLog.java:143)\n        at org.apache.log4j.helpers.OnlyOnceErrorHandler.error(OnlyOnceErrorHandler.java:78)\n        at org.apache.log4j.helpers.OnlyOnceErrorHandler.error(OnlyOnceErrorHandler.java:68)\n        at org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:60)\n        at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:316)\n        at org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:263)\n        at org.apache.log4j.WriterAppender.append(WriterAppender.java:160)\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n        - locked <0xa6401628> (a org.apache.log4j.RollingFileAppender)\n        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n        at org.apache.log4j.Category.callAppenders(Category.java:206)\n        - locked <0xa6401388> (a org.apache.log4j.Logger)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.info(Category.java:666)\n        at com.airvana.faultServer.niohandlers.NioNotificationHandler.declareNodeUnreachable(NioNotificationHandler.java:586)\n        - locked <0xa8596850> (a java.lang.Integer)\n        at com.airvana.faultServer.niohandlers.NioNotificationHandler.channelDisconnected(NioNotificationHandler.java:198)\n        - locked <0xa89b0a90> (a com.airvana.faultServer.db.NodeObj)\n        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:137)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:567)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:803)\n        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:76)\n        at org.jboss.netty.handler.execution.OrderedMemoryAwareThreadPoolExecutor$ChildExecutor.run(OrderedMemoryAwareThreadPoolExecutor.java:314)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:651)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:676)\n        at java.lang.Thread.run(Thread.java:595)\n\nFound 1 deadlock."}, {"count": 32, "tags": [], "bug_id": 41214, "text": "Log4J version 1.2.16:\nDeadlock:  \n[2/1/11 15:22:44:977 EST] 00000005 TimeoutManage I   WTRN0124I: When the timeout occurred the thread with which the transaction is, or was most recently, associated was Thread[WMQJCAResourceAdapter : 3,5,main]. The stack trace of this thread when the timeout occurred was: \n\tjava.io.UnixFileSystem.getBooleanAttributes0(Native Method)\n\tjava.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:240)\n\tjava.io.File.exists(File.java:733)\n\torg.apache.log4j.RollingFileAppender.rollOver(RollingFileAppender.java:151)\n\torg.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:280)\n\torg.apache.log4j.WriterAppender.append(WriterAppender.java:162)\n\torg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n\torg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n\torg.apache.log4j.Category.callAppenders(Category.java:206)\n\torg.apache.log4j.Category.forcedLog(Category.java:391)\n\torg.apache.log4j.Category.debug(Category.java:260)\n\tComponent1Impl.logDebug(Component1Impl.java:140)\n\nMultiple-threaded application, installed on WebSphere Application Server v7, picking messages from a JMS Queue on WebSphere MQ v7. At the time 10 connections where opened retrieving messages from the queue and writing to a log file.\n Application stopped while Log4J tried to rollover the log file, timing out the session.\n\n So, how can we fix this?\n Thanks!\n Adilson Dias", "id": 143846, "attachment_id": null, "creator": "adilson.dias@cgu.com.au", "creation_time": "2011-02-01T00:47:05Z", "time": "2011-02-01T00:47:05Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 41214, "text": " Replaced log4j with logback and everything works fine. No deadlock.\n Cheers,\n Adilson Dias\n\n\n\n(In reply to comment #32)\n> Log4J version 1.2.16:\n> Deadlock:  \n> [2/1/11 15:22:44:977 EST] 00000005 TimeoutManage I   WTRN0124I: When the\n> timeout occurred the thread with which the transaction is, or was most\n> recently, associated was Thread[WMQJCAResourceAdapter : 3,5,main]. The stack\n> trace of this thread when the timeout occurred was: \n>     java.io.UnixFileSystem.getBooleanAttributes0(Native Method)\n>     java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:240)\n>     java.io.File.exists(File.java:733)\n>     org.apache.log4j.RollingFileAppender.rollOver(RollingFileAppender.java:151)\n>    \n> org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:280)\n>     org.apache.log4j.WriterAppender.append(WriterAppender.java:162)\n>     org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n>    \n> org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n>     org.apache.log4j.Category.callAppenders(Category.java:206)\n>     org.apache.log4j.Category.forcedLog(Category.java:391)\n>     org.apache.log4j.Category.debug(Category.java:260)\n>     Component1Impl.logDebug(Component1Impl.java:140)\n> \n> Multiple-threaded application, installed on WebSphere Application Server v7,\n> picking messages from a JMS Queue on WebSphere MQ v7. At the time 10\n> connections where opened retrieving messages from the queue and writing to a\n> log file.\n>  Application stopped while Log4J tried to rollover the log file, timing out the\n> session.\n> \n>  So, how can we fix this?\n>  Thanks!\n>  Adilson Dias", "count": 33, "id": 143939, "time": "2011-02-02T19:05:49Z", "creator": "adilson.dias@cgu.com.au", "creation_time": "2011-02-02T19:05:49Z", "is_private": false}, {"count": 34, "tags": [], "bug_id": 41214, "text": "hi Adilson, \n\ncan you say what version of logback you used?\n\nI am using Logback 0.9.18 (released 03-Dec-2009) and still seeing what looks like the same problem - one thread (\"http-8443-6\") doing a log message, deadlocked waiting on another thread (\"pool-4-thread-1\") doing a log within a file compression operation:\n\nJava stack information for the threads listed above:\n===================================================\n\"pool-4-thread-1\":\n\tat ch.qos.logback.core.rolling.RollingFileAppender.subAppend(RollingFileAppender.java:141)\n\t- waiting to lock <0x00002aaad13089f8> (a ch.qos.logback.core.rolling.TimeBasedRollingPolicy)\n\tat ch.qos.logback.core.WriterAppender.append(WriterAppender.java:120)\n\tat ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:93)\n\tat ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:64)\n\tat ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:275)\n\tat ch.qos.logback.classic.Logger.callAppenders(Logger.java:262)\n\tat ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:465)\n\tat ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:419)\n\tat ch.qos.logback.classic.Logger.info(Logger.java:623)\n\tat com.aepona.tws.portal.common.logging.LogbackStatusChangeListener.addStatusEvent_aroundBody0(LogbackStatusChangeListener.java:43)\n\tat com.aepona.tws.portal.common.logging.LogbackStatusChangeListener.addStatusEvent_aroundBody1$advice(LogbackStatusChangeListener.java:61)\n\tat com.aepona.tws.portal.common.logging.LogbackStatusChangeListener.addStatusEvent(LogbackStatusChangeListener.java:41)\n\tat ch.qos.logback.core.BasicStatusManager.fireStatusAddEvent(BasicStatusManager.java:86)\n\t- locked <0x00002aaace451c30> (a java.lang.Object)\n\tat ch.qos.logback.core.BasicStatusManager.add(BasicStatusManager.java:58)\n\tat ch.qos.logback.core.spi.ContextAwareBase.addStatus(ContextAwareBase.java:71)\n\tat ch.qos.logback.core.spi.ContextAwareBase.addInfo(ContextAwareBase.java:76)\n\tat ch.qos.logback.core.rolling.helper.Compressor.compress(Compressor.java:59)\n\tat ch.qos.logback.core.rolling.helper.CompressionRunnable.run(CompressionRunnable.java:31)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n\"http-8443-6\":\n\tat ch.qos.logback.core.BasicStatusManager.fireStatusAddEvent(BasicStatusManager.java:85)\n\t- waiting to lock <0x00002aaace451c30> (a java.lang.Object)\n\tat ch.qos.logback.core.BasicStatusManager.add(BasicStatusManager.java:58)\n\tat ch.qos.logback.core.spi.ContextAwareBase.addStatus(ContextAwareBase.java:71)\n\tat ch.qos.logback.core.spi.ContextAwareBase.addInfo(ContextAwareBase.java:76)\n\tat ch.qos.logback.core.rolling.helper.DefaultArchiveRemover.clean(DefaultArchiveRemover.java:78)\n\tat ch.qos.logback.core.rolling.TimeBasedRollingPolicy.rollover(TimeBasedRollingPolicy.java:139)\n\tat ch.qos.logback.core.rolling.RollingFileAppender.rollover(RollingFileAppender.java:110)\n\t- locked <0x00002aaad1307328> (a ch.qos.logback.core.rolling.RollingFileAppender)\n\tat ch.qos.logback.core.rolling.RollingFileAppender.subAppend(RollingFileAppender.java:142)\n\t- locked <0x00002aaad13089f8> (a ch.qos.logback.core.rolling.TimeBasedRollingPolicy)\n\tat ch.qos.logback.core.WriterAppender.append(WriterAppender.java:120)\n\tat ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:93)\n\tat ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:64)\n\tat ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:275)\n\tat ch.qos.logback.classic.Logger.callAppenders(Logger.java:262)\n\tat ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:465)\n\tat ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:419)\n\tat ch.qos.logback.classic.Logger.info(Logger.java:623)\n\n\n(In reply to comment #33)\n>  Replaced log4j with logback and everything works fine. No deadlock.\n>  Cheers,\n>  Adilson Dias\n>", "id": 144134, "attachment_id": null, "creator": "geoff.macartney@aepona.com", "creation_time": "2011-02-10T10:00:59Z", "time": "2011-02-10T10:00:59Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 41214, "text": "####<Feb 16, 2011 7:05:25 PM GST> <Critical> <WebLogicServer> <camp1s> <CAMAProd> <[ACTIVE] ExecuteThread: '6' for queue: 'weblogic.kernel.Default (self-tuning)'> <<WLS Kernel>> <> <> <1297868725143> <BEA-000394> <\n\nDEADLOCK DETECTED:\n==================\n\n[deadlocked thread] [ACTIVE] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)':\n--------------------------------------------------------------------------------------------------\nThread '[ACTIVE] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)'' is waiting to acquire lock 'org.apache.log4j.RollingFileAppender@18cceaf' that is held by thread '[ACTIVE] ExecuteThread: '0' for queue: 'weblogic.kernel.Default (self-tuning)''\n\nStack trace:\n------------\n\torg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:210)\n\torg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65)\n\torg.apache.log4j.Category.callAppenders(Category.java:203)\n\torg.apache.log4j.Category.forcedLog(Category.java:388)\n\torg.apache.log4j.Category.debug(Category.java:257)\n\tcom.company.product.jdbc.MyJDBC.selectAllObjects(MyJDBC.java:294)\n\tcom.company.product.business.MyBusinessMgrImpl.selectAllObjects(MyBusinessMgrImpl.java:155)\n\t...\n\n[deadlocked thread] [ACTIVE] ExecuteThread: '0' for queue: 'weblogic.kernel.Default (self-tuning)':\n--------------------------------------------------------------------------------------------------\nThread '[ACTIVE] ExecuteThread: '0' for queue: 'weblogic.kernel.Default (self-tuning)'' is waiting to acquire lock 'org.apache.log4j.Logger@155742b' that is held by thread '[ACTIVE] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)''\n\nStack trace:\n------------\n\torg.apache.log4j.Category.callAppenders(Category.java:201)\n\torg.apache.log4j.Category.forcedLog(Category.java:388)\n\torg.apache.log4j.Category.warn(Category.java:1008)\n\tcom.company.product.datatypes.BusinessException.toString(BusinessException.java:757)\n\torg.apache.log4j.spi.VectorWriter.println(ThrowableInformation.java:97)\n\tjava.lang.Throwable.printStackTrace(Throwable.java:510)\n\torg.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:59)\n\torg.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:342)\n\torg.apache.log4j.WriterAppender.subAppend(WriterAppender.java:304)\n\torg.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:234)\n\torg.apache.log4j.WriterAppender.append(WriterAppender.java:159)\n\torg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230)\n\torg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65)\n\torg.apache.log4j.Category.callAppenders(Category.java:203)\n\torg.apache.log4j.Category.forcedLog(Category.java:388)\n\torg.apache.log4j.Category.error(Category.java:319)\n\tcom.company.framework.ejb.connectivity.BaseMessageDrivenBean.doRollback(BaseMessageDrivenBean.java:111)\n\t...\n\n\nBefore diving into the explanation, you need a few code samples to fully understand:\nThe BusinessException extends Exception and implements the toString() method with some logging inside.\npublic class BusinessException extends Exception {\n    private static final Logger log = Logger.getLogger(BusinessException.class);\n\n    @Override\n    public String toString() {\n        ...\n        log.debug(\"some logging in the toString()\");\n        ...\n    }\n}\n\n\nThe Category.callAppenders() method makes its way up the Logger hierarchy (log4j Logger hierarchy knowledge needed). On each iteration the Logger is locked with a synchronized block. Once in the synchronized block, if  an AppenderAttachableImpl exists on the current Logger, it calls AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent).\npublic class Category implements AppenderAttachable {\n    public void callAppenders(LoggingEvent event) {\n        int writes = 0;\n\n        for(Category c = this; c != null; c=c.parent) {\n            // Protected against simultaneous call to addAppender, removeAppender,...\n            synchronized(c) {\n                if(c.aai != null) {\n                    writes += c.aai.appendLoopOnAppenders(event);\n                }\n                if(!c.additive) {\n                    break;\n                }\n            }\n        }\n\n        if(writes == 0) {\n            repository.emitNoAppenderWarning(this);\n        }\n    }\n}\n\nIn AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), the method Appender.doAppend(LoggingEvent) is called.\npublic class AppenderAttachableImpl implements AppenderAttachable {\n    public int appendLoopOnAppenders(LoggingEvent event) {\n        int size = 0;\n        Appender appender;\n\n        if(appenderList != null) {\n            size = appenderList.size();\n            for(int i = 0; i < size; i++) {\n                appender = (Appender) appenderList.elementAt(i);\n                appender.doAppend(event);\n            }\n        }\n        return size;\n    }\n}\n\nFinally, the RollingFileAppender indirectly implementes Appender via the indirectly extended AppenderSkeleton class. AppenderSkeleton.doAppend(LoggingEvent) is a synchonized method, so calling RollingFileAppender.doAppend(LoggingEvent) will lock the current RollingFileAppender object.\npublic abstract class AppenderSkeleton implements Appender, OptionHandler {\n    public synchronized void doAppend(LoggingEvent event) {\n        ...\n    }\n}\n\n\n\nWhat follows is what I found was happening.\nWe have the following Logger hierarchy:\n\n\nRootLogger\n +- com.company     - RollingFileAppender attached\n     +- com.company.product.jdbc.MyJDBC                         (ExecuteThread: '4')\n     +- com.company.product.datatypes.BusinessException          (ExecuteThread: '0')\n     +- framework.core.ejb.connectivity.BaseMessageDrivenBean   (ExecuteThread: '0')        - RollingFileAppender attached\n\n\nI think the sequence of event is something like this:\n1.\tExecuteThread: '0' \u2013 is logging on the BaseMessageDrivenBean logger with a BusinessException (which implements toString())\n2.\tExecuteThread: '0' - in Category.callAppenders(), it starts to make its way up in the Logger hierarchy (locking the current Logger on each iteration).\n3.\tExecuteThread: '0' - it reaches and locks the \"framework.core.ejb.connectivity.BaseMessageDrivenBean\" Logger, where it finds a RollingFileAppender attached. It calls AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), which ends up calling synchronized method doAppend(LoggingEvent) on a RollingFileAppender object, locking it at the same time.\n4.\tExecuteThread: '4' \u2013 is logging something on the MyJDBC logger\n5.\tExecuteThread: '4' - in Category.callAppenders(), it starts to make its way up in the Logger hierarchy (locking the current Logger on each iteration).\n6.\tExecuteThread: '4' - it reaches and locks the \"com.company\" Logger, where it finds a RollingFileAppender attached. It calls AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), which ends up calling the synchronized method doAppend(LoggingEvent) on the _already locked_ RollingFileAppender object. *ExecuteThread: '4' is blocked waiting on ExecuteThread: '0'*\n7.\tExecuteThread: '0' \u2013 goes on with its execution inside the RollingFileAppender. This ends up logging the BusinessException, which indirectly calls BusinessException.toString(). Inside the toString() method come logging is called on the BusinessException Logger.\n8.\tExecuteThread: '0' - in Category.callAppenders(), it starts to make its way up in the Logger hierarchy (locking the current Logger on each iteration).\n9.\tExecuteThread: '0' - it reaches \"com.company\" and tries to lock it, but it is already locked by ExecuteThread: '4'. *ExecuteThread: '0' is blocked waiting on ExecuteThread: '4'*\n\nTHIS IS A DEADLOCK SITUATION.\n\nThis is not really a problem with log4j's itself. The problem lies more in how log4j is configured, and a complex sequence of method calls exhibiting a deadlock situation with java locks on synchronized blocks. Using the RollingFileAppender probably makes things worse: if a file rolling (very slow compared to running java code) is happening in this sequence of events, then it greatly increases the chances of reaching such problem.", "count": 35, "id": 144535, "time": "2011-02-25T08:38:57Z", "creator": "djanoiup@yahoo.fr", "creation_time": "2011-02-25T08:38:57Z", "is_private": false}, {"text": "Agree that the deadlock is not caused by log4j itself. It's caused by the design of the given app, i.e.:\n- the fact that logging an exception through category A causes the exception to log an event to log4j through category B; ... and both categories use the same appender\n- using renderers that log to log4j inside their code in a scenrio similar to the one described above\n\nWhile it's not log4j that is the direct cause of these issues, if log4j classes were synchronized differently, these problems would not be observed. If org.apache.log4j.Category.callAppenders() would use a reentrant read write lock,  thread that called category A, then called appender which caused category B to be called would not be blocked on org.apache.log4j.Category.callAppenders(). What is more, as org.apache.log4j.Category.callAppenders() is no longer a problem, this thread would be able to reenter the lock on org.apache.log4j.AppenderSkeleton.doAppend(). Deadlock would not be observed.\n\nI also mentioned changing locking mechanism used in org.apache.log4j.Category here https://issues.apache.org/bugzilla/show_bug.cgi?id=50213#c6\nI'll create a seperate Bugzilla ticket for this propsed refactoring.\n\nOh, of course introducing async appenders could be used as a workaround for these serious issues - for deadlocks. However, org.apache.log4j.Category.callAppenders() is anyway suboptimal.\n\nBartek", "tags": [], "bug_id": 41214, "attachment_id": null, "count": 36, "id": 145689, "time": "2011-04-11T05:05:56Z", "creator": "kowalewski.bartosz@gmail.com", "creation_time": "2011-04-11T05:05:56Z", "is_private": false}, {"count": 37, "tags": [], "bug_id": 41214, "text": "I've just created https://issues.apache.org/bugzilla/attachment.cgi?bugid=51047 (Move org.apache.log4j.Category to reentrant read/write locks). This change should get rid of issues with synchronization and deadlocks.", "id": 145694, "time": "2011-04-11T05:51:58Z", "creator": "kowalewski.bartosz@gmail.com", "creation_time": "2011-04-11T05:51:58Z", "is_private": false, "attachment_id": null}, {"count": 38, "attachment_id": null, "bug_id": 41214, "text": "If we start requiring Java 5 for log4j, then there's a nice solution here for callAppenders: use a CopyOnWriteArrayList in AppenderAttachableImpl and thus avoid any locking when iterating over appenders.  Without Java 5 you could implement your own CopyOnWriteArrayList, of course.\n\nThat doesn't solve some application design issues, of course -- as AppenderSkeleton has a synchronized doAppend().  Many subclasses rely on this locking, so it can't be blithely removed.\n\nRemoving the locking across all appenders for a given logger does eliminate some such issues, though.  More importantly, it eliminates a key bottleneck in log4j.  It makes sense that only one thread can write to a text file at a time.  It makes no sense that no thread can write to a text file just because some other thread is writing to the console (or some such), though.\n\nAlso many threads should be able to determine whether the filter is going to reject the event, format the event going to the text file, etc, at the same time.  The locking really should be *very* last minute.  Getting rid of the locking at the callAppenders() level is a big step in the right direction, though -- one which I've long since been taking by patching in an alternate implementation of AppenderAttachableImpl that uses CopyOnWriteArrayList as pre-Java-6 JVMs are ancient history to me at this point.", "id": 145697, "time": "2011-04-11T07:10:58Z", "creator": "jessh@ptc.com", "creation_time": "2011-04-11T07:10:58Z", "tags": [], "is_private": false}, {"attachment_id": null, "tags": [], "creator": "leigh.klotz@xerox.com", "text": "(In reply to comment #38)\n\nCan you move this comment over to 51047?", "count": 39, "id": 145989, "time": "2011-04-25T19:18:39Z", "bug_id": 41214, "creation_time": "2011-04-25T19:18:39Z", "is_private": false}, {"count": 40, "attachment_id": null, "bug_id": 41214, "text": "(In reply to comment #38)\n> If we start requiring Java 5 for log4j, then there's a nice solution here for\n> callAppenders: use a CopyOnWriteArrayList in AppenderAttachableImpl and thus\n> avoid any locking when iterating over appenders.  Without Java 5 you could\n> implement your own CopyOnWriteArrayList, of course.\n\nThere is very little chance Log4j 1.x is ever going to upgrade to Java 5. I have been working on Log4j 2.0 in an experimental branch which addresses this issue as well as many others, including some design defects that are present in Logback. Unfortunately, Log4j 2.0 still needs a fair amount of work. Volunteers are welcome!", "id": 145990, "time": "2011-04-25T22:30:02Z", "creator": "Ralph.Goers@dslextreme.com", "creation_time": "2011-04-25T22:30:02Z", "tags": [], "is_private": false}, {"count": 41, "tags": [], "bug_id": 41214, "attachment_id": null, "text": "(In reply to comment #35)\n> ####<Feb 16, 2011 7:05:25 PM GST> <Critical> <WebLogicServer> <camp1s>\n> <CAMAProd> <[ACTIVE] ExecuteThread: '6' for queue: 'weblogic.kernel.Default\n> (self-tuning)'> <<WLS Kernel>> <> <> <1297868725143> <BEA-000394> <\n> \n> DEADLOCK DETECTED:\n> ==================\n> \n> [deadlocked thread] [ACTIVE] ExecuteThread: '4' for queue:\n> 'weblogic.kernel.Default (self-tuning)':\n> -----------------------------------------------------------------------------\n> ---------------------\n> Thread '[ACTIVE] ExecuteThread: '4' for queue: 'weblogic.kernel.Default\n> (self-tuning)'' is waiting to acquire lock\n> 'org.apache.log4j.RollingFileAppender@18cceaf' that is held by thread\n> '[ACTIVE] ExecuteThread: '0' for queue: 'weblogic.kernel.Default\n> (self-tuning)''\n> \n> Stack trace:\n> ------------\n> \torg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:210)\n> \torg.apache.log4j.helpers.AppenderAttachableImpl.\n> appendLoopOnAppenders(AppenderAttachableImpl.java:65)\n> \torg.apache.log4j.Category.callAppenders(Category.java:203)\n> \torg.apache.log4j.Category.forcedLog(Category.java:388)\n> \torg.apache.log4j.Category.debug(Category.java:257)\n> \tcom.company.product.jdbc.MyJDBC.selectAllObjects(MyJDBC.java:294)\n> \tcom.company.product.business.MyBusinessMgrImpl.\n> selectAllObjects(MyBusinessMgrImpl.java:155)\n> \t...\n> \n> [deadlocked thread] [ACTIVE] ExecuteThread: '0' for queue:\n> 'weblogic.kernel.Default (self-tuning)':\n> -----------------------------------------------------------------------------\n> ---------------------\n> Thread '[ACTIVE] ExecuteThread: '0' for queue: 'weblogic.kernel.Default\n> (self-tuning)'' is waiting to acquire lock 'org.apache.log4j.Logger@155742b'\n> that is held by thread '[ACTIVE] ExecuteThread: '4' for queue:\n> 'weblogic.kernel.Default (self-tuning)''\n> \n> Stack trace:\n> ------------\n> \torg.apache.log4j.Category.callAppenders(Category.java:201)\n> \torg.apache.log4j.Category.forcedLog(Category.java:388)\n> \torg.apache.log4j.Category.warn(Category.java:1008)\n> \tcom.company.product.datatypes.BusinessException.toString(BusinessException.\n> java:757)\n> \torg.apache.log4j.spi.VectorWriter.println(ThrowableInformation.java:97)\n> \tjava.lang.Throwable.printStackTrace(Throwable.java:510)\n> \torg.apache.log4j.spi.ThrowableInformation.\n> getThrowableStrRep(ThrowableInformation.java:59)\n> \torg.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:342)\n> \torg.apache.log4j.WriterAppender.subAppend(WriterAppender.java:304)\n> \torg.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:234)\n> \torg.apache.log4j.WriterAppender.append(WriterAppender.java:159)\n> \torg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230)\n> \torg.apache.log4j.helpers.AppenderAttachableImpl.\n> appendLoopOnAppenders(AppenderAttachableImpl.java:65)\n> \torg.apache.log4j.Category.callAppenders(Category.java:203)\n> \torg.apache.log4j.Category.forcedLog(Category.java:388)\n> \torg.apache.log4j.Category.error(Category.java:319)\n> \tcom.company.framework.ejb.connectivity.BaseMessageDrivenBean.\n> doRollback(BaseMessageDrivenBean.java:111)\n> \t...\n> \n> \n> Before diving into the explanation, you need a few code samples to fully\n> understand:\n> The BusinessException extends Exception and implements the toString() method\n> with some logging inside.\n> public class BusinessException extends Exception {\n>     private static final Logger log =\n> Logger.getLogger(BusinessException.class);\n> \n>     @Override\n>     public String toString() {\n>         ...\n>         log.debug(\"some logging in the toString()\");\n>         ...\n>     }\n> }\n> \n> \n> The Category.callAppenders() method makes its way up the Logger hierarchy\n> (log4j Logger hierarchy knowledge needed). On each iteration the Logger is\n> locked with a synchronized block. Once in the synchronized block, if  an\n> AppenderAttachableImpl exists on the current Logger, it calls\n> AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent).\n> public class Category implements AppenderAttachable {\n>     public void callAppenders(LoggingEvent event) {\n>         int writes = 0;\n> \n>         for(Category c = this; c != null; c=c.parent) {\n>             // Protected against simultaneous call to addAppender,\n> removeAppender,...\n>             synchronized(c) {\n>                 if(c.aai != null) {\n>                     writes += c.aai.appendLoopOnAppenders(event);\n>                 }\n>                 if(!c.additive) {\n>                     break;\n>                 }\n>             }\n>         }\n> \n>         if(writes == 0) {\n>             repository.emitNoAppenderWarning(this);\n>         }\n>     }\n> }\n> \n> In AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), the method\n> Appender.doAppend(LoggingEvent) is called.\n> public class AppenderAttachableImpl implements AppenderAttachable {\n>     public int appendLoopOnAppenders(LoggingEvent event) {\n>         int size = 0;\n>         Appender appender;\n> \n>         if(appenderList != null) {\n>             size = appenderList.size();\n>             for(int i = 0; i < size; i++) {\n>                 appender = (Appender) appenderList.elementAt(i);\n>                 appender.doAppend(event);\n>             }\n>         }\n>         return size;\n>     }\n> }\n> \n> Finally, the RollingFileAppender indirectly implementes Appender via the\n> indirectly extended AppenderSkeleton class.\n> AppenderSkeleton.doAppend(LoggingEvent) is a synchonized method, so calling\n> RollingFileAppender.doAppend(LoggingEvent) will lock the current\n> RollingFileAppender object.\n> public abstract class AppenderSkeleton implements Appender, OptionHandler {\n>     public synchronized void doAppend(LoggingEvent event) {\n>         ...\n>     }\n> }\n> \n> \n> \n> What follows is what I found was happening.\n> We have the following Logger hierarchy:\n> \n> \n> RootLogger\n>  +- com.company     - RollingFileAppender attached\n>      +- com.company.product.jdbc.MyJDBC                        \n> (ExecuteThread: '4')\n>      +- com.company.product.datatypes.BusinessException         \n> (ExecuteThread: '0')\n>      +- framework.core.ejb.connectivity.BaseMessageDrivenBean  \n> (ExecuteThread: '0')        - RollingFileAppender attached\n> \n> \n> I think the sequence of event is something like this:\n> 1.\tExecuteThread: '0' \u2013 is logging on the BaseMessageDrivenBean logger with\n> a BusinessException (which implements toString())\n> 2.\tExecuteThread: '0' - in Category.callAppenders(), it starts to make its\n> way up in the Logger hierarchy (locking the current Logger on each\n> iteration).\n> 3.\tExecuteThread: '0' - it reaches and locks the\n> \"framework.core.ejb.connectivity.BaseMessageDrivenBean\" Logger, where it\n> finds a RollingFileAppender attached. It calls\n> AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), which ends up\n> calling synchronized method doAppend(LoggingEvent) on a RollingFileAppender\n> object, locking it at the same time.\n> 4.\tExecuteThread: '4' \u2013 is logging something on the MyJDBC logger\n> 5.\tExecuteThread: '4' - in Category.callAppenders(), it starts to make its\n> way up in the Logger hierarchy (locking the current Logger on each\n> iteration).\n> 6.\tExecuteThread: '4' - it reaches and locks the \"com.company\" Logger, where\n> it finds a RollingFileAppender attached. It calls\n> AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), which ends up\n> calling the synchronized method doAppend(LoggingEvent) on the _already\n> locked_ RollingFileAppender object. *ExecuteThread: '4' is blocked waiting\n> on ExecuteThread: '0'*\n> 7.\tExecuteThread: '0' \u2013 goes on with its execution inside the\n> RollingFileAppender. This ends up logging the BusinessException, which\n> indirectly calls BusinessException.toString(). Inside the toString() method\n> come logging is called on the BusinessException Logger.\n> 8.\tExecuteThread: '0' - in Category.callAppenders(), it starts to make its\n> way up in the Logger hierarchy (locking the current Logger on each\n> iteration).\n> 9.\tExecuteThread: '0' - it reaches \"com.company\" and tries to lock it, but\n> it is already locked by ExecuteThread: '4'. *ExecuteThread: '0' is blocked\n> waiting on ExecuteThread: '4'*\n> \n> THIS IS A DEADLOCK SITUATION.\n> \n> This is not really a problem with log4j's itself. The problem lies more in\n> how log4j is configured, and a complex sequence of method calls exhibiting a\n> deadlock situation with java locks on synchronized blocks. Using the\n> RollingFileAppender probably makes things worse: if a file rolling (very\n> slow compared to running java code) is happening in this sequence of events,\n> then it greatly increases the chances of reaching such problem.\n\nActually, I think the solution (or a mitigation) might be simple. I suggest the following \"workaround\" modifications, in the Log4J version 1.2.17 (the last version):\n\n1)In the org.apache.log4j.spi.LoggingEvent class, inside all the constructor's:\n\n  Replace:\n  this.message = message;\n  \n  By:\n  this.message = message != null ? message.toString() : null;\n  \n  Doing so, you can make the \"message\" instance field always of the type String.\n\n\n2)In the org.apache.log4j.spi.ThrowableInformation class:\n  2.1)Create the following instance field:\n  \n  private final String[] stackTraceArray;\n  \n  And, inside all the constructors, put the following piece of code:\n  \n    this.stackTraceArray = getThrowableStrRep();\n    \n  The intention here is using \"stackTraceArray\" instance field instead of calling getThrowableStrRep() later.\n  \n\n  2.2)Make getThrowableStrRep() private. Doing so, there is no need to maintain the synchronization of this method. Actually, I think there would be no need of synchronizing this method even if it were public, because I didn't see any place in Log4j source codes where an instance of ThrowableInformation is being shared by more than one thread. I do not understand what was the intention of the developer that created that method, maybe he/she predicted a future necessity of synchronization, as ThrowableInformation is public and has public constructors. We never know the sort of things the other programmers will do with our public classes and methods (the \"bug in Log4j\" we are discussing here shows exactly such a situation, doesn't it?)\n  \n\n  2.3)Create a public method called \"getStackTraceArray()\" that returns \"stackTraceArray\" instance field (or we can also return a clone of this array).\n  \n  \n3)In all the places where getThrowableStrRep() is being called, replace by getStackTraceArray() invocation. This replacement should be done in the following classes:\n  org.apache.log4j.pattern.ThrowableInformationPatternConverter\n  org.apache.log4j.spi.LoggingEvent\n  org.apache.log4j.lf5.Log4JLogRecord\n  \n\nI do not think it is related just to RollingFileAppender, as the title of this page suggests. That's just a synchronization among threads problem, not diretly related to any appender.\n\nTrying to understand who is the \"guilty\" of the problem (whether is Log4j or the application/framework that uses Log4j) is not so important. What is revelant is the fact that we have a big serious problem that has to be solved. Otherwise, we can never rely on an multithreaded application subject to a Log4j deadlock. If the solution I proposed doesn't solve the problem, I will really consider the alternative of removing Log4j from my application. I will sadly do that, although I like Log4j so much. But I tested my solution, and I think it works and covers all the critical cases. Of course, as I said, this solution might be considered just a mitigation, not a real solution. In my opinion, a real and elegant solution would be done through a more deep refactoring.", "id": 160380, "time": "2012-07-01T12:48:33Z", "creator": "marc_sm2003@yahoo.com.br", "creation_time": "2012-07-01T12:48:33Z", "is_private": false}, {"count": 42, "tags": [], "bug_id": 41214, "text": "It's not as simple as:\n\nthis.message = message != null ? message.toString() : null;\n\nbecause of the implementation of org.apache.log4j.spi.LoggingEvent.getRenderedMessage()\n\nI do like turning the Object into a String in the constructor because you get a snapshot of the message in one place, not at some later time; and calling toString() can give you different values when called repeatedly.\n\nWhat about submitting a patch?", "id": 160385, "time": "2012-07-01T15:33:46Z", "creator": "garydgregory@gmail.com", "creation_time": "2012-07-01T15:33:46Z", "is_private": false, "attachment_id": null}, {"count": 43, "tags": [], "bug_id": 41214, "text": "(In reply to comment #42)\n\nI didn't think exactly about getting a snapshot. I just thought defining \"message\" instance field as the type String would fix the bug merely because inside toString() method of the String class there isn't any Log4j call.\n\nThe same idea applies when calling org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep() method at the beginning, in the constructor.\n\nI believe I achieved the goal.\n\nThis bug can be easily reproduced. See below.\n\nCreate the following classes:\n\ncom.a.AnObject\ncom.b.AnException\ncom.c.AnObjectThread\ncom.d.AnExceptionThread\ncom.main.RootLoggerThread\ncom.main.Main\n\n==============\npackage com.a;\n\nimport org.apache.log4j.Logger;\n\npublic class AnObject {\n    private static final Logger LOGGER = Logger.getLogger(AnObject.class);\n    private final String name;\n    \n    public AnObject(String name) {\n        this.name = name;\n    }\n    \n    @Override\n    public String toString() {\n        try {\n            Thread.sleep(4000);\n        } catch (InterruptedException ex) {\n            ex.printStackTrace();\n        }\n        LOGGER.info(\"Logging DEBUG in AnObject [\" + name + \"]\");\n        return name;\n    }\n}\n==============\n\n==============\npackage com.b;\n\nimport org.apache.log4j.Logger;\n\npublic class AnException extends RuntimeException {\n    private static final Logger LOGGER = Logger.getLogger(AnException.class);\n    \n    public AnException() {\n        super();\n    }\n    \n    public AnException(String msg) {\n        super(msg);\n    }\n    \n    public AnException(Throwable t) {\n        super(t);\n    }\n    \n    public AnException(String msg, Throwable t) {\n        super(msg, t);\n    }\n    \n    @Override\n    public String getMessage() {\n        try {\n            Thread.sleep(4000);\n        } catch (InterruptedException ex) {\n            ex.printStackTrace();\n        }\n        LOGGER.error(\"Logging ERROR in AnException\");\n        return super.getMessage();\n    }\n}\n==============\n\n==============\npackage com.c;\n\nimport com.a.AnObject;\nimport org.apache.log4j.Logger;\n\npublic class AnObjectThread extends Thread {\n    private static final Logger LOGGER = Logger.getLogger(AnObjectThread.class);\n    \n    public AnObjectThread(String threadName) {\n        super(threadName);\n    }\n    \n    @Override\n    public void run() {\n        AnObject anObject = new AnObject(\"Object created in AnObjectThread\");\n        LOGGER.info(anObject);\n    }\n}\n==============\n\n==============\npackage com.d;\n\nimport com.b.AnException;\nimport org.apache.log4j.Logger;\n\npublic class AnExceptionThread extends Thread {\n    private static final Logger LOGGER = Logger.getLogger(AnExceptionThread.class);\n    \n    public AnExceptionThread(String threadName) {\n        super(threadName);\n    }\n    \n    @Override\n    public void run() {\n        LOGGER.info(\"Just logging INFO in AnExceptionThread\", new AnException(\"test exception\", new AnException(\"cause exception\")));\n    }\n}\n==============\n\n==============\npackage com.main;\n\nimport org.apache.log4j.Logger;\n\npublic class RootLoggerThread extends Thread {\n    private static final Logger LOGGER = Logger.getLogger(RootLoggerThread.class);\n    \n    public RootLoggerThread(String threadName) {\n        super(threadName);\n    }\n    \n    @Override\n    public void run() {\n        LOGGER.info(\"Just logging INFO in RootLoggerThread\");\n    }\n}\n==============\n\n==============\npackage com.main;\n\nimport com.c.AnObjectThread;\nimport com.d.AnExceptionThread;\n\n/**\n * Bug 41214 reproduction.\n *\n * https://issues.apache.org/bugzilla/show_bug.cgi?id=41214\n *\n * @author Marcelo S. Miashiro\n */\npublic class Main {\n    public static void main(String[] args) {\n        RootLoggerThread rootLoggerThread = new RootLoggerThread(\"RootLoggerThread\");\n        AnObjectThread anObjectThread = new AnObjectThread(\"AnObjectThread\");\n        AnExceptionThread anExceptionThread = new AnExceptionThread(\"AnExceptionThread\");\n\n        anExceptionThread.start();\n        anObjectThread.start();\n\n        try {\n            // To reproduce the bug, com.a.AnObject.toString() and com.b.AnException.getMessage()\n            // methods must be called before rootLogger\n            Thread.sleep(2000);\n        } catch (InterruptedException ex) {\n            ex.printStackTrace();\n        }\n\n        rootLoggerThread.start();\n    }\n}\n==============\n\n\nAnd, of course, create the log4j.properties file with the following content:\n\n==============\nlog4j.rootLogger=INFO, ConsoleAppender\nlog4j.logger.com.c=INFO, ConsoleAppender\nlog4j.logger.com.d=INFO, ConsoleAppender\n\nlog4j.appender.ConsoleAppender=org.apache.log4j.ConsoleAppender\nlog4j.appender.ConsoleAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.ConsoleAppender.layout.ConversionPattern=[%d{dd-MM-yyyy HH:mm:ss,SSS}][%-5p][%c] %M : %m%n\n==============\n\n\nTry running the Main class with log4j version 1.2.17 firstly without the fix I suggested. You see a deadlock happening. Try using jconsole to see what's going on. You'll see in jconsole something like this:\n\n==============\nName: AnExceptionThread\nState: BLOCKED on org.apache.log4j.ConsoleAppender@13e75a5 owned by: AnObjectThread\nTotal blocked: 2  Total waited: 0\n\nStack trace: \n org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:231)\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\norg.apache.log4j.Category.callAppenders(Category.java:206)\n   - locked org.apache.log4j.Logger@147e668\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:683)\ncom.d.AnExceptionThread.run(AnExceptionThread.java:19)\n==============\n\n==============\nName: AnObjectThread\nState: BLOCKED on org.apache.log4j.spi.RootLogger@157b46f owned by: RootLoggerThread\nTotal blocked: 1  Total waited: 1\n\nStack trace: \n org.apache.log4j.Category.callAppenders(Category.java:205)\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:666)\ncom.a.AnObject.toString(AnObject.java:24)\norg.apache.log4j.or.DefaultRenderer.doRender(DefaultRenderer.java:37)\norg.apache.log4j.or.RendererMap.findAndRender(RendererMap.java:80)\norg.apache.log4j.spi.LoggingEvent.getRenderedMessage(LoggingEvent.java:368)\norg.apache.log4j.helpers.PatternParser$BasicPatternConverter.convert(PatternParser.java:402)\norg.apache.log4j.helpers.PatternConverter.format(PatternConverter.java:65)\norg.apache.log4j.PatternLayout.format(PatternLayout.java:506)\norg.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310)\norg.apache.log4j.WriterAppender.append(WriterAppender.java:162)\norg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n   - locked org.apache.log4j.ConsoleAppender@13e75a5\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\norg.apache.log4j.Category.callAppenders(Category.java:206)\n   - locked org.apache.log4j.Logger@12c4c57\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:666)\ncom.c.AnObjectThread.run(AnObjectThread.java:20)\n==============\n\n==============\nName: RootLoggerThread\nState: BLOCKED on org.apache.log4j.ConsoleAppender@13e75a5 owned by: AnObjectThread\nTotal blocked: 1  Total waited: 0\n\nStack trace: \n org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:231)\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\norg.apache.log4j.Category.callAppenders(Category.java:206)\n   - locked org.apache.log4j.spi.RootLogger@157b46f\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:666)\ncom.main.RootLoggerThread.run(RootLoggerThread.java:18)\n==============\n\nAfter applying the fix I suggested, no more deadlock happens.\n\nI have already submitted the patch to Log4j team.", "id": 160471, "attachment_id": null, "creator": "marc_sm2003@yahoo.com.br", "creation_time": "2012-07-06T04:46:22Z", "time": "2012-07-06T04:46:22Z", "is_private": false}, {"count": 44, "tags": [], "bug_id": 41214, "text": "Created attachment 29064\nLog4j bug deadlock fix and reproduction (Bug 41214)\n\nHi Apache Log4j dev team,\n\n\nI am attaching the Bug41214.zip file. Inside this file there are two other zip files: Bug41214_Reproduction.zip and Log4j_1.2.17_Bug41214_Fix.zip.\n\nI am submitting a patch for bug 41214 (https://issues.apache.org/bugzilla/show_bug.cgi?id=41214), in the Log4j_1.2.17_Bug41214_Fix.zip attached file.\n\nPlease, read below my explanation:\n\nActually, I think the solution (or a mitigation) might be simple. I suggest the following \"workaround\" modifications, in the Log4J version 1.2.17 (the last version):\n\n1)In the org.apache.log4j.spi.LoggingEvent class, inside all the constructors:\n\n  Replace:\n  this.message = message;\n \n  By:\n  this.message = message != null ? message.toString() : null;\n \n  Doing so, you can make the \"message\" instance field always of the type String.\n\n\n2)In the org.apache.log4j.spi.ThrowableInformation class:\n  2.1)Create the following instance field:\n \n  private final String[] stackTraceArray;\n \n  And, inside all the constructors, put the following piece of code:\n \n    this.stackTraceArray = getThrowableStrRep();\n   \n  The intention here is using \"stackTraceArray\" instance field instead of calling getThrowableStrRep() later.\n \n\n  2.2)Make getThrowableStrRep() private. Doing so, there is no need to maintain the synchronization of this method. Actually, I think there would be no need of synchronizing this method even if it were public, because I didn't see any place in Log4j source codes where an instance of ThrowableInformation is being shared by more than one thread. I do not understand what was the intention of the developer that created that method, maybe he/she predicted a future necessity of synchronization, as ThrowableInformation is public and has public constructors. We never know the sort of things the other programmers will do with our public classes and methods (the \"bug in Log4j\" we are discussing here shows exactly such a situation, doesn't it?)\n \n\n  2.3)Create a public method called \"getStackTraceArray()\" that returns \"stackTraceArray\" instance field (or we can also return a clone of this array).\n \n \n3)In all the places where getThrowableStrRep() is being called, replace by getStackTraceArray() invocation. This replacement should be done in the following classes:\n  org.apache.log4j.pattern.ThrowableInformationPatternConverter\n  org.apache.log4j.pattern.LogEvent\n  org.apache.log4j.lf5.Log4JLogRecord\n \n\nI do not think it is related just to RollingFileAppender, as the title of this page suggests. That's just a synchronization among threads problem, not diretly related to any appender.\n\nThe idea behind this solution is based on the fact that inside toString() method of the String class there isn't any Log4j call.\n\nTrying to understand who is the \"guilty\" of the problem (whether is Log4j or the application/framework that uses Log4j) is not so important. What is revelant is the fact that we have a big serious problem that has to be solved. Otherwise, we can never rely on a multithreaded application subject to a Log4j deadlock. If the solution I proposed doesn't solve the problem, I will really consider the alternative of removing Log4j from my application. I will sadly do that, although I like Log4j so much. But I tested my solution, and I think it works and covers all the critical cases. Of course, as I said, this solution might be considered just a mitigation, not a real solution. In my opinion, a real and elegant solution would be done through a more deep refactoring.\n\nI also want to tell that this problem really happens, this is not just theoretical. I have a multithreaded application that uses Apache Mina framework for exchanging bytes with another multithreaded application. Sometimes the application simply stops working. I didn't understand why some threads remain BLOCKED until I have found the web page of the bug 41214. This web page was my salvation.\n\nIn the Bug41214_Reproduction.zip attached file, you have the source codes of an executable that reproduces the bug. First, try running this executable with log4j version 1.2.17 without the fix suggested by me. You see a deadlock happening. Try using jconsole to see what's going on. You'll see in jconsole something like this:\n\n\n==============\nName: AnExceptionThread\nState: BLOCKED on org.apache.log4j.ConsoleAppender@13e75a5 owned by: AnObjectThread\nTotal blocked: 2  Total waited: 0\n\nStack trace:\n org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:231)\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\norg.apache.log4j.Category.callAppenders(Category.java:206)\n   - locked org.apache.log4j.Logger@147e668\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:683)\ncom.d.AnExceptionThread.run(AnExceptionThread.java:19)\n==============\n\n==============\nName: AnObjectThread\nState: BLOCKED on org.apache.log4j.spi.RootLogger@157b46f owned by: RootLoggerThread\nTotal blocked: 1  Total waited: 1\n\nStack trace:\n org.apache.log4j.Category.callAppenders(Category.java:205)\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:666)\ncom.a.AnObject.toString(AnObject.java:24)\norg.apache.log4j.or.DefaultRenderer.doRender(DefaultRenderer.java:37)\norg.apache.log4j.or.RendererMap.findAndRender(RendererMap.java:80)\norg.apache.log4j.spi.LoggingEvent.getRenderedMessage(LoggingEvent.java:368)\norg.apache.log4j.helpers.PatternParser$BasicPatternConverter.convert(PatternParser.java:402)\norg.apache.log4j.helpers.PatternConverter.format(PatternConverter.java:65)\norg.apache.log4j.PatternLayout.format(PatternLayout.java:506)\norg.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310)\norg.apache.log4j.WriterAppender.append(WriterAppender.java:162)\norg.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n   - locked org.apache.log4j.ConsoleAppender@13e75a5\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\norg.apache.log4j.Category.callAppenders(Category.java:206)\n   - locked org.apache.log4j.Logger@12c4c57\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:666)\ncom.c.AnObjectThread.run(AnObjectThread.java:20)\n==============\n\n==============\nName: RootLoggerThread\nState: BLOCKED on org.apache.log4j.ConsoleAppender@13e75a5 owned by: AnObjectThread\nTotal blocked: 1  Total waited: 0\n\nStack trace:\n org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:231)\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\norg.apache.log4j.Category.callAppenders(Category.java:206)\n   - locked org.apache.log4j.spi.RootLogger@157b46f\norg.apache.log4j.Category.forcedLog(Category.java:391)\norg.apache.log4j.Category.info(Category.java:666)\ncom.main.RootLoggerThread.run(RootLoggerThread.java:18)\n\nAfter applying the fix I suggest, no more deadlock happens.\n==============\n\nThanks in advance\n\nMarcelo Sussumu Miashiro.", "id": 160651, "time": "2012-07-15T10:12:42Z", "creator": "marc_sm2003@yahoo.com.br", "creation_time": "2012-07-15T10:12:42Z", "is_private": false, "attachment_id": 29064}, {"count": 45, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 162818, "time": "2012-10-18T22:31:07Z", "creator": "jdmarshall@gmail.com", "creation_time": "2012-10-18T22:31:07Z", "is_private": false, "text": "I too like the snapshotting aspects of this solution, unfortunately some people use toString to implement expensive pretty printing calls.  You wouldn't want to call toString() eagerly on a debug() level call, only to have it be dropped on the floor.\n\n(In reply to comment #41)\n> (In reply to comment #35)\n> > The Category.callAppenders() method makes its way up the Logger hierarchy\n> > (log4j Logger hierarchy knowledge needed). On each iteration the Logger is\n> > locked with a synchronized block. Once in the synchronized block, if  an\n> > AppenderAttachableImpl exists on the current Logger, it calls\n> > AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent).\n> > public class Category implements AppenderAttachable {\n> >     public void callAppenders(LoggingEvent event) {\n> >         int writes = 0;\n> > \n> >         for(Category c = this; c != null; c=c.parent) {\n> >             // Protected against simultaneous call to addAppender,\n> > removeAppender,...\n> >             synchronized(c) {\n> >                 if(c.aai != null) {\n> >                     writes += c.aai.appendLoopOnAppenders(event);\n> >                 }\n> >                 if(!c.additive) {\n> >                     break;\n> >                 }\n> >             }\n> >         }\n> > \n> >         if(writes == 0) {\n> >             repository.emitNoAppenderWarning(this);\n> >         }\n> >     }\n> > }\n> > \n> > In AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), the method\n> > Appender.doAppend(LoggingEvent) is called.\n> > public class AppenderAttachableImpl implements AppenderAttachable {\n> >     public int appendLoopOnAppenders(LoggingEvent event) {\n> >         int size = 0;\n> >         Appender appender;\n> > \n> >         if(appenderList != null) {\n> >             size = appenderList.size();\n> >             for(int i = 0; i < size; i++) {\n> >                 appender = (Appender) appenderList.elementAt(i);\n> >                 appender.doAppend(event);\n> >             }\n> >         }\n> >         return size;\n> >     }\n> > }\n> > \n> > Finally, the RollingFileAppender indirectly implementes Appender via the\n> > indirectly extended AppenderSkeleton class.\n> > AppenderSkeleton.doAppend(LoggingEvent) is a synchonized method, so calling\n> > RollingFileAppender.doAppend(LoggingEvent) will lock the current\n> > RollingFileAppender object.\n> > public abstract class AppenderSkeleton implements Appender, OptionHandler {\n> >     public synchronized void doAppend(LoggingEvent event) {\n> >         ...\n> >     }\n> > }\n> > \n> > \n> > \n> > What follows is what I found was happening.\n> > We have the following Logger hierarchy:\n> > \n> > \n> > RootLogger\n> >  +- com.company     - RollingFileAppender attached\n> >      +- com.company.product.jdbc.MyJDBC                        \n> > (ExecuteThread: '4')\n> >      +- com.company.product.datatypes.BusinessException         \n> > (ExecuteThread: '0')\n> >      +- framework.core.ejb.connectivity.BaseMessageDrivenBean  \n> > (ExecuteThread: '0')        - RollingFileAppender attached\n> > \n> > \n> > I think the sequence of event is something like this:\n> > 1.\tExecuteThread: '0' \u2013 is logging on the BaseMessageDrivenBean logger with\n> > a BusinessException (which implements toString())\n> > 2.\tExecuteThread: '0' - in Category.callAppenders(), it starts to make its\n> > way up in the Logger hierarchy (locking the current Logger on each\n> > iteration).\n> > 3.\tExecuteThread: '0' - it reaches and locks the\n> > \"framework.core.ejb.connectivity.BaseMessageDrivenBean\" Logger, where it\n> > finds a RollingFileAppender attached. It calls\n> > AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), which ends up\n> > calling synchronized method doAppend(LoggingEvent) on a RollingFileAppender\n> > object, locking it at the same time.\n> > 4.\tExecuteThread: '4' \u2013 is logging something on the MyJDBC logger\n> > 5.\tExecuteThread: '4' - in Category.callAppenders(), it starts to make its\n> > way up in the Logger hierarchy (locking the current Logger on each\n> > iteration).\n> > 6.\tExecuteThread: '4' - it reaches and locks the \"com.company\" Logger, where\n> > it finds a RollingFileAppender attached. It calls\n> > AppenderAttachableImpl.appendLoopOnAppenders(LoggingEvent), which ends up\n> > calling the synchronized method doAppend(LoggingEvent) on the _already\n> > locked_ RollingFileAppender object. *ExecuteThread: '4' is blocked waiting\n> > on ExecuteThread: '0'*\n> > 7.\tExecuteThread: '0' \u2013 goes on with its execution inside the\n> > RollingFileAppender. This ends up logging the BusinessException, which\n> > indirectly calls BusinessException.toString(). Inside the toString() method\n> > come logging is called on the BusinessException Logger.\n> > 8.\tExecuteThread: '0' - in Category.callAppenders(), it starts to make its\n> > way up in the Logger hierarchy (locking the current Logger on each\n> > iteration).\n> > 9.\tExecuteThread: '0' - it reaches \"com.company\" and tries to lock it, but\n> > it is already locked by ExecuteThread: '4'. *ExecuteThread: '0' is blocked\n> > waiting on ExecuteThread: '4'*\n> > \n> > THIS IS A DEADLOCK SITUATION.\n> > \n> > This is not really a problem with log4j's itself. The problem lies more in\n> > how log4j is configured, and a complex sequence of method calls exhibiting a\n> > deadlock situation with java locks on synchronized blocks. Using the\n> > RollingFileAppender probably makes things worse: if a file rolling (very\n> > slow compared to running java code) is happening in this sequence of events,\n> > then it greatly increases the chances of reaching such problem.\n> \n> Actually, I think the solution (or a mitigation) might be simple. I suggest\n> the following \"workaround\" modifications, in the Log4J version 1.2.17 (the\n> last version):\n> \n> 1)In the org.apache.log4j.spi.LoggingEvent class, inside all the\n> constructor's:\n> \n>   Replace:\n>   this.message = message;\n>   \n>   By:\n>   this.message = message != null ? message.toString() : null;\n>   \n>   Doing so, you can make the \"message\" instance field always of the type\n> String.\n> \n>"}, {"count": 46, "tags": [], "creator": "jdmarshall@gmail.com", "text": "I can see the deadlock from here.\n\nWith nested Categories A->B->C\n\nWhile you're in B.callAppenders B is locked.  If toObject() attempts to log, then C.callAppenders is invoked again.  If someone else has already called C.callAppenders, you have a deadlock.  One has C waiting for B, another has B waiting for C.  \n\nThe classical solution to this problem is to sort your locks and grab them in the same predictable order.  The parent-child relationship -almost- solves the problem here, except for a couple problems.  One, being able to rearrange your loggers while logging is happening, and the other is that pesky toObject() method.\n\nWhat you probably should do here is call parent.callAppenders while still holding your own lock. With deeply nested Categories this will widen your bottleneck but eliminate the deadlock.  I won't be able to write to c.log while someone is still writing to b.log or a.log, and unfortunately it also breaks your emitNoAppenderWarning fallback, because callAppenders returns void.  \n\nIt's unclear to my why the parentage of a Category can be updated after it's created.  It seems like it's an artifact of the evolution of the library.  I'm not familiar with this aspect of Log4j but it seems that this is a case of premature publication.  Add on a sprinkle of too much delegation and a dash of concurrency and you've got quite a soup here.\n\nI thought I had a patch proposal (that didn't break the API) but it turned out on further inspection to have the exact same failure mode.  So instead of my help all I can offer is my sympathy.  I now understand why this has been open for so long.  \n\nI do think that if you pull some of the deprecated things out of this part of the API that it may be easier to fix.", "id": 162840, "time": "2012-10-19T22:46:59Z", "bug_id": 41214, "creation_time": "2012-10-19T22:46:59Z", "is_private": false, "attachment_id": null}, {"count": 47, "tags": [], "bug_id": 41214, "attachment_id": null, "text": "After discussion with some colleagues, I would like to propose the following fix:\n\nYou cannot protect AppenderAttachableImpl from here.  You can only protect the lazy instantiation.   All of the public methods on AppenderAttachableImpl should be marked synchronized if you're worried about add and remove calls causing problems.\n\n        Category c = this\n        while (c != null) {\n            Category parent;\n            boolean additive;\n            AppenderAttachableImpl aai;\n\n            // Protected against simultaneous call to addAppender,\nremoveAppender,...\n            synchronized(c) {\n                aai = c.aai;\n                additive = c.additive;\n                c = c.parent();\n            }\n\n            if(aai != null) {\n                writes += aai.appendLoopOnAppenders(event);\n            }\n\n            if(!additive) {\n                break;\n            } \n        }\n\n        if(writes == 0) {\n            repository.emitNoAppenderWarning(this);\n        }\n    }\n\nWith this code, no lock is held on the heirarchy while toString can be called, and no two locks are held at once, breaking the deadlock race condition.  In-flight log messages can still show up in the old log while structural changes are being made, but I believe that was always a danger anyway.\n\nThoughts?\n\n(In reply to comment #46)\n> I can see the deadlock from here.\n> \n> With nested Categories A->B->C\n> \n> While you're in B.callAppenders B is locked.  If toObject() attempts to log,\n> then C.callAppenders is invoked again.  If someone else has already called\n> C.callAppenders, you have a deadlock.  One has C waiting for B, another has\n> B waiting for C.  \n> \n> The classical solution to this problem is to sort your locks and grab them\n> in the same predictable order.  The parent-child relationship -almost-\n> solves the problem here, except for a couple problems.  One, being able to\n> rearrange your loggers while logging is happening, and the other is that\n> pesky toObject() method.\n> \n> What you probably should do here is call parent.callAppenders while still\n> holding your own lock. With deeply nested Categories this will widen your\n> bottleneck but eliminate the deadlock.  I won't be able to write to c.log\n> while someone is still writing to b.log or a.log, and unfortunately it also\n> breaks your emitNoAppenderWarning fallback, because callAppenders returns\n> void.  \n> \n> It's unclear to my why the parentage of a Category can be updated after it's\n> created.  It seems like it's an artifact of the evolution of the library. \n> I'm not familiar with this aspect of Log4j but it seems that this is a case\n> of premature publication.  Add on a sprinkle of too much delegation and a\n> dash of concurrency and you've got quite a soup here.\n> \n> I thought I had a patch proposal (that didn't break the API) but it turned\n> out on further inspection to have the exact same failure mode.  So instead\n> of my help all I can offer is my sympathy.  I now understand why this has\n> been open for so long.  \n> \n> I do think that if you pull some of the deprecated things out of this part\n> of the API that it may be easier to fix.", "id": 162881, "time": "2012-10-22T17:29:25Z", "creator": "jdmarshall@gmail.com", "creation_time": "2012-10-22T17:29:25Z", "is_private": false}, {"count": 48, "tags": [], "text": "Since you asked for thoughts....  Please test Log4j 2.", "attachment_id": null, "bug_id": 41214, "id": 162882, "time": "2012-10-22T17:41:33Z", "creator": "Ralph.Goers@dslextreme.com", "creation_time": "2012-10-22T17:41:33Z", "is_private": false}, {"count": 49, "tags": [], "text": "(In reply to comment #47)\n>             synchronized(c) {\n>                 aai = c.aai;\n>                 additive = c.additive;\n>                 c = c.parent();\n>             }\n> \n>             if(aai != null) {\n>                 writes += aai.appendLoopOnAppenders(event);\n>             }\n\nThe code above is risky. The reason for that synchronized block is lost. That code doesn\u00b4t protect against a simultaneous call of aai.appendLoopOnAppenders() from two or more threads. You are throwing away the main purpose of that synchronization.\n\nThe solution proposed in Comment 37 works pretty well, but unfortunately it depends on JRE 1.5 or later. The solution proposed by me also works, even using JRE 1.4.", "attachment_id": null, "bug_id": 41214, "id": 163149, "time": "2012-11-02T23:45:55Z", "creator": "marc_sm2003@yahoo.com.br", "creation_time": "2012-11-02T23:45:55Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "marc_sm2003@yahoo.com.br", "text": "(In reply to comment #48)\n> Since you asked for thoughts....  Please test Log4j 2.\n\nYes, the bug presented in this page is solved in log4j version 2. I tested using log4j-2.0-beta2 and no more deadlock happens. But, unfortunately (or, FORTUNATELY, it depends on the point of view), this version of log4j requires JDK/JRE 1.5 or later.", "count": 50, "id": 163152, "time": "2012-11-03T12:08:47Z", "bug_id": 41214, "creation_time": "2012-11-03T12:08:47Z", "is_private": false}, {"count": 51, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 168398, "time": "2013-07-09T08:18:49Z", "creator": "nishant.patralekh@astrazeneca.com", "creation_time": "2013-07-09T08:18:49Z", "is_private": false, "text": "We are getting Stuck threads on Windows 2003 environment. We are using version 1.2.14 of log4j.\n2013-07-08 15:35:35,098] INFO : Size: 0; Questions: 113; Hits: 19; Adaptive r/w ratio: 1%; Total hitrate: 17%\n<08-Jul-2013 15:35:36 o'clock BST> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '140' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for \"627\" seconds working on the request \"Http Request: /NewsBroker/news.rss\", which is more than the configured time (StuckThreadMaxTime) of \"600\" seconds. Stack trace:\n\torg.apache.log4j.Category.callAppenders(Category.java:201)\n\torg.apache.log4j.Category.forcedLog(Category.java:388)\n\torg.apache.log4j.Category.error(Category.java:319)\n\tcom.astrazeneca.portal.rss.TransformerUtils.getChannels(TransformerUtils.java:154)\n\tsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tjava.lang.reflect.Method.invoke(Method.java:592)\n\torg.apache.xalan.extensions.ExtensionHandlerJavaPackage.callFunction(ExtensionHandlerJavaPackage.java:332)\n\torg.apache.xalan.extensions.ExtensionHandlerJavaPackage.callFunction(ExtensionHandlerJavaPackage.java:426)\n\torg.apache.xalan.extensions.ExtensionsTable.extFunction(ExtensionsTable.java:220)\n\nWe get hundreds of similar stuck threads daily.\nIs there any workaround for this issue?\n\nThanks,\nNishant"}, {"count": 52, "tags": [], "bug_id": 41214, "attachment_id": null, "text": "(In reply to nishant.patralekh from comment #51)\n> We are getting Stuck threads on Windows 2003 environment. We are using\n> version 1.2.14 of log4j.\n> ...\n> We get hundreds of similar stuck threads daily.\n> Is there any workaround for this issue?\n\nThe best solution for this problem is to upgrade to log4j 2.0.\n1.2.14 is extremely old and log4j 2.0 does address a lot of the multi-threading issues.", "id": 168400, "time": "2013-07-09T08:49:02Z", "creator": "grobmeier@gmail.com", "creation_time": "2013-07-09T08:49:02Z", "is_private": false}, {"count": 53, "tags": [], "bug_id": 41214, "attachment_id": null, "id": 168403, "time": "2013-07-09T09:07:31Z", "creator": "nishant.patralekh@astrazeneca.com", "creation_time": "2013-07-09T09:07:31Z", "is_private": false, "text": "Thanks for such a quick response.\nWhere can I check if there is any compatitibility issue in log4j 2.0 ?\nCurrently we are ussing jdk150_04.\nAlso, from where can we get the jar from? \n\nThanks,\nNishant"}, {"count": 54, "attachment_id": null, "bug_id": 41214, "text": "I upgraded to log4j2 and still it was jamming up for ten to twenty seconds, directly after midnight when rollover and compression happens.\n\nOne thread is\n\n   java.lang.Thread.State: RUNNABLE\n        at java.io.RandomAccessFile.writeBytes(Native Method)\n        at java.io.RandomAccessFile.write(RandomAccessFile.java:499)\n        at org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.flush(FastRollingFileManager.java:105)\n        at org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.write(FastRollingFileManager.java:89)\n        - locked <0x00000000e81ae1e0> (a org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager)\n        at org.apache.logging.log4j.core.appender.OutputStreamManager.write(OutputStreamManager.java:129)\n        at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:115)\n        at org.apache.logging.log4j.core.appender.FastRollingFileAppender.append(FastRollingFileAppender.java:97)\n        at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:102)\n        at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:426)\n        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:407)\n        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:410)\n        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:368)\n        at org.apache.logging.log4j.core.Logger.log(Logger.java:110)\n        at org.apache.logging.log4j.spi.AbstractLoggerWrapper.log(AbstractLoggerWrapper.java:55)\n        at org.apache.logging.log4j.spi.AbstractLogger.info(AbstractLogger.java:984)\n\n\nand then scores of other threads at\n\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.checkRollover(RollingFileManager.java:108)\n        - waiting to lock <0x00000000e81ae1e0> (a org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager)\n        at org.apache.logging.log4j.core.appender.FastRollingFileAppender.append(FastRollingFileAppender.java:88)\n        at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:102)\n        at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:426)\n        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:407)\n        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:410)\n        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:368)\n        at org.apache.logging.log4j.core.Logger.log(Logger.java:110)\n        at org.apache.logging.log4j.spi.AbstractLoggerWrapper.log(AbstractLoggerWrapper.java:55)\n        at org.apache.logging.log4j.spi.AbstractLogger.info(AbstractLogger.java:984)\n\n\nChanging to disruptor fixed the problem (async appenders might have fixed the problem too i suspect).", "id": 168968, "time": "2013-07-31T08:25:52Z", "creator": "mck@apache.org", "creation_time": "2013-07-31T08:25:52Z", "tags": [], "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 41214, "text": "@mck@apache.org:\n\nPlease note: the LOG4j2 issues are stored here:\nhttps://issues.apache.org/jira/browse/LOG4J2\nI highly encourage you to open an issue there.", "count": 55, "id": 168969, "time": "2013-07-31T08:32:03Z", "creator": "grobmeier@gmail.com", "creation_time": "2013-07-31T08:32:03Z", "is_private": false}, {"count": 56, "tags": [], "bug_id": 41214, "attachment_id": 31193, "id": 172254, "time": "2014-01-10T03:55:14Z", "creator": "eyang@apache.org", "creation_time": "2014-01-10T03:55:14Z", "is_private": false, "text": "Created attachment 31193\nUpdated version of log4j 1.2.6 with reentry lock\n\nAn updated version for Log4j 1.2.6, this version covers additional spot missed in earlier patch.  This is stress tested with lots jvm threads to ensure log4j 1.x series doesn't deadlock on high stressed environment."}, {"count": 57, "tags": [], "text": "Hi people. I have the same issue with log4j 1.2.17. Any update on this? Is there any ETA\n\nThanks!", "attachment_id": null, "id": 182097, "creator": "rodriguezgustavoandres@gmail.com", "time": "2015-03-25T20:13:41Z", "bug_id": 41214, "creation_time": "2015-03-25T20:13:41Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "garydgregory@gmail.com", "text": "I'm afraid you will not get much more than our stock answer here: Our energy is focused on Log4j 2, and in particular into getting our next release, 2.3, out the door, hopefully in a week or two. For v1, there are no current plans to release a version 1.2.18.", "count": 58, "id": 182108, "time": "2015-03-26T03:56:45Z", "bug_id": 41214, "creation_time": "2015-03-26T03:56:45Z", "is_private": false}]