[{"count": 0, "tags": [], "bug_id": 51318, "attachment_id": null, "text": "Related to 51317 - Need ability to stream and chunk data out of MS Publisher documents.\n\nI attempted to implement streaming and chunking of data out of pub files and got errors as below.\n\nBasically I attempted to read from DocumentInputStream in chunks, in succession, rather than read in the whole stream into a large preallocated byte array.\n\n    byte[] filler = new byte[25]; \n    \n    byte[] bytes = new byte[8];\n    int read = dis.read(bytes, 0, 8);\n    \n    if (read <= 0) {\n      // \n    } else {\n      String f8 = new String(bytes);\n      if (!f8.equals(\"CHNKINK \")) {\n        throw new IllegalArgumentException(\"Expecting 'CHNKINK ' but was '\" + f8 + \"'\");\n      }\n      // Ignore the next 24, for now at least\n    \n      dis.read(filler, 8, 24);\n      \n      for (int i = 0; i < 20; i++) {\n        int offset = 0x20 + i * 24;\n        \n        bytes = new byte[25];\n        read = dis.read(bytes, offset, bytes.length);\n\nNote the line which attempts to read the filler 24 bytes so we can get to the bits. I had to try it there because was getting error simply trying to do read(bytes, offset, bytes.length).\n\nErrors are all like this first:\nException in thread \"main\" java.lang.IndexOutOfBoundsException: can't read past buffer boundaries\n\tat org.apache.poi.poifs.filesystem.NDocumentInputStream.read(NDocumentInputStream.java:142)\n\tat org.apache.poi.poifs.filesystem.DocumentInputStream.read(DocumentInputStream.java:118)\n\nNow, if we examine NDocumentInputStream.read(byte[], int, int), there is a conditional there:\nif (off < 0 || len < 0 || b.length < off + len) {\n\nThis assumes that the byte array is large and you're going in sequence. If you want to jump around you'd presumably want to check b.length < len.\n\nTried that. Got the next error as follows:\nException in thread \"main\" java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkBounds(Buffer.java:530)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:125)\n\tat org.apache.poi.poifs.filesystem.NDocumentInputStream.readFully(NDocumentInputStream.java:250)\n\tat org.apache.poi.poifs.filesystem.NDocumentInputStream.read(NDocumentInputStream.java:151)\n\tat org.apache.poi.poifs.filesystem.DocumentInputStream.read(DocumentInputStream.java:118)", "id": 146837, "time": "2011-06-03T17:44:01Z", "creator": "dgoldenberg@attivio.com", "creation_time": "2011-06-03T17:44:01Z", "is_private": false}, {"count": 1, "tags": [], "bug_id": 51318, "attachment_id": null, "text": "Attachment is too big to attached even in a zip. Please let me know if you want the file. I suspect this will happen on many or most pub files.", "id": 146838, "time": "2011-06-03T17:48:47Z", "creator": "dgoldenberg@attivio.com", "creation_time": "2011-06-03T17:48:47Z", "is_private": false}, {"count": 2, "tags": [], "text": "Does this happen even on small publisher files? I'm guessing it may affect anything where an entry in a POIFS is more than one big block. If you can reproduce it with one of the small sample files we already have, that'd mean we could use them and make life easy :)", "is_private": false, "id": 146842, "creation_time": "2011-06-03T19:35:49Z", "time": "2011-06-03T19:35:49Z", "creator": "apache@gagravarr.org", "bug_id": 51318, "attachment_id": null}, {"count": 3, "text": "Created attachment 27107\nSmaller file to repro on", "bug_id": 51318, "attachment_id": 27107, "id": 146852, "time": "2011-06-03T23:22:41Z", "creator": "dgoldenberg@attivio.com", "creation_time": "2011-06-03T23:22:41Z", "tags": [], "is_private": false}, {"count": 4, "tags": [], "creator": "dgoldenberg@attivio.com", "text": "Nick,\n\nYes, it's reproducible on smaller files (please see attached).\n\nThanks", "id": 146853, "time": "2011-06-03T23:23:52Z", "bug_id": 51318, "creation_time": "2011-06-03T23:23:52Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 51318, "attachment_id": null, "is_private": false, "id": 146892, "time": "2011-06-06T14:36:52Z", "creator": "apache@gagravarr.org", "creation_time": "2011-06-06T14:36:52Z", "text": "Can you try with svn trunk, and see if it helps? I fixed a few bits on the weekend, and I updated most of the DocumentInputStream tests to check NPOIFS too\n\nThere is a mark/reset issue though, need to fix that before I can write a test for your specific case."}, {"count": 6, "tags": [], "bug_id": 51318, "attachment_id": null, "id": 146894, "time": "2011-06-06T14:40:49Z", "creator": "dgoldenberg@attivio.com", "creation_time": "2011-06-06T14:40:49Z", "is_private": false, "text": "I can certainly try. So is all this stuff going into trunk?\nWe were actually on NIO 3.2 branch...  Also would ideally like your other fix too :)\n\nDo you think you'll be looking into streaming API's for HPBF? Then perhaps we could write the chunking on top of that..."}, {"count": 7, "attachment_id": null, "bug_id": 51318, "is_private": false, "id": 179889, "time": "2014-12-20T07:25:23Z", "creator": "apache@gagravarr.org", "creation_time": "2014-12-20T07:25:23Z", "tags": [], "text": "The NIO 3.2 branch hasn't been worked on for quite some time, and isn't likely to receive any new work. As such, I'm closing this as \"In a Later Version\", sorry\n\nIf you can reproduce this problem still on 3.11, please let us know. As it stands, a very similar unit test passes on trunk, and has done for some time, so I think your problem is solved!"}]