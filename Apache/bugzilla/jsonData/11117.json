[{"count": 0, "tags": [], "text": "I've built a small test servlet (included below) that shows this behaviour.\n\nIf I upload data to tomcat (configured to use the HTTP/1.1 Coyote connector)\nusing a chunked transfer-encoding on a PUT, then the result (i.e. what I read\nusing the servlet's request.getInputStream() is corrupted.\n\nThis _only_ happens on large PUTs (it happens every time I tried it (about 10)\non a large PUT - I was testing with something around 700 kB, and another thing\nof about 1 MB, but does NOT always corrupt it in the same way. I did not see any\ncorruption on small files (~50 kB and smaller), though I didn't test this\nextensively.\n\nThe total length of the data read is exactly correct (712080 bytes in my first\ntest). At the first point of corruption in the file, a chunk header appears in\nthe output (\"\\r\\n1000\\r\\n\") followed by correct data from about 700 bytes later\non, then a second copy (in the correct place, I think) of the corrupt data (i.e.\ncorrect data, followed by the chunk header, followed by some data repeated, the\nsecond instance of which is in the correct place - so the first instance is\nbeing produced _instead_ of the correct data for that point in the input).\n\nTest servlet follows:\n\nimport java.io.*;\n\nimport javax.servlet.*;\nimport javax.servlet.http.*;\n\npublic class Test extends HttpServlet\n{\n\n    public void doPut(HttpServletRequest req, HttpServletResponse res)\n                throws ServletException, IOException\n    {\n        FileOutputStream fos = new FileOutputStream(\"/tmp/servlet-out\");\n\n        InputStream is = req.getInputStream();\n\n        byte buf[] = new byte[2000];\n        int ret;\n\n        while((ret = is.read(buf)) > 0) {\n            fos.write(buf, 0, ret);\n        }\n\n        fos.close();\n        is.close();\n\n        PrintWriter pw = res.getWriter();\n        pw.println(\"Done\");\n        pw.flush();\n        pw.close();\n    }\n}", "is_private": false, "bug_id": 11117, "id": 20108, "time": "2002-07-24T07:44:57Z", "creator": "msmith@ns.xn.com.au", "creation_time": "2002-07-24T07:44:57Z", "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 11117, "is_private": false, "text": "Assuming that there's indeed a bug, this will be triggered by how the client\ngenerates the chunks (I can translate by: thanks for attaching a test servlet,\nbut what I actually need is the client).", "id": 20109, "time": "2002-07-24T08:02:35Z", "creator": "remm@apache.org", "creation_time": "2002-07-24T08:02:35Z", "attachment_id": null}, {"count": 2, "text": "Ok.\nI originally found this bug when using a program based on slide's webdav client\nlibraries. It does not appear to be specific to this, though (i.e. I had no\ndifficulty triggering it in other ways). \nFollowing is a (very) minimal test program that seems to trigger this, at least\nin combination with the test servlet I gave previously (i.e. the output file\nfrom the servlet is NOT the same as the input file for the program here):\n\nimport java.net.*;\nimport java.io.*;\n\npublic class TestChunkedPut\n{\n    private static final String HOST = \"localhost\";\n    private static final int PORT = 8080;\n    private static final String PATH = \"/testcontext/test\";\n    private static final String FILE = \"/tmp/testfile\";\n\n    public static void main(String[] args) {\n        try {\n            Socket sock = new Socket(HOST, PORT);\n\n            OutputStream os = sock.getOutputStream();\n\n            os.write((\"PUT \"+PATH+\" HTTP/1.1\\r\\n\").getBytes());\n            os.write((\"Host: \"+HOST+\":\"+PORT+\"\\r\\n\").getBytes());\n            os.write(\"Transfer-Encoding: chunked\\r\\n\".getBytes());\n            os.write(\"\\r\\n\".getBytes());\n\n            InputStream is = new FileInputStream(FILE);\n\n            byte buf[] = new byte[4096];\n            int bytes;\n\n            while((bytes = is.read(buf)) > 0) {\n                os.write((Integer.toString(bytes, 16) + \"\\r\\n\").getBytes());\n                os.write(buf, 0, bytes);\n                os.write(\"\\r\\n\".getBytes());\n            }\n\n            os.write(\"0\\r\\n\\r\\n\".getBytes());\n\n            os.close();\n            sock.close();\n            is.close();\n        } catch(Exception e) {\n            e.printStackTrace();\n        }\n\n        System.out.println(\"Done\\n\");\n    }\n}\n", "bug_id": 11117, "is_private": false, "id": 20218, "time": "2002-07-26T00:33:47Z", "creator": "msmith@ns.xn.com.au", "creation_time": "2002-07-26T00:33:47Z", "tags": [], "attachment_id": null}, {"count": 3, "tags": [], "text": "Problem diagnosed, patch attached.\nThe detailed cause:\n\nThis code sets a particular buffer in a ByteChunk, and sets a specific start/end\nto this buffer. This allows sharing of the buffers, rather than copying them\naround.\n\nIn this code, the buffer was set (in chunk - the output ByteChunk) as pointing\nto some part of the buffer in readChunk (the input ByteChunk), which is fine.\nHowever, in one particular code path, this was immediately followed by a call to\nparseCRLF() - which simply swallows a CRLF pair. In the very unfortunate event\n(which happened on sufficiently large inputs with some regularity) of this CRLF\nbeing over the end of the buffer, more content would need to be read in order to\ndo this. This refilled readChunk, but because this was shared with the (output)\nchunk, the output buffer was _also_ overwritten. Output is then corrupted.\n\nThis is fixed here by deferring the parseCRLF() until later (the next call to\ndoRead())\n\nThis fix is critical for PUT (and presumably things like POST, or anything else\nwith a request-body) to work reliably with chunked transfer-encoding. \n\nIndex: filters/ChunkedInputFilter.java\n===================================================================\nRCS file:\n/home/cvspublic/jakarta-tomcat-connectors/http11/src/java/org/apache/coyote/http11/filters/ChunkedInputFilter.java,v\nretrieving revision 1.5\ndiff -u -r1.5 ChunkedInputFilter.java\n--- filters/ChunkedInputFilter.java     20 Jun 2002 11:14:30 -0000      1.5\n+++ filters/ChunkedInputFilter.java     1 Aug 2002 07:13:44 -0000\n@@ -136,6 +136,12 @@\n      */\n     protected boolean endChunk = false;\n \n+    /**\n+     * Flag set to true if the next call to doRead() must parse a CRLF pair\n+     * before doing anything else.\n+     */\n+    protected boolean needCRLFParse = false;\n+\n \n     // ------------------------------------------------------------- Properties\n \n@@ -158,6 +164,11 @@\n         if (endChunk)\n             return -1;\n \n+        if(needCRLFParse) {\n+            needCRLFParse = false;\n+            parseCRLF();\n+        }\n+\n         if (remaining <= 0) {\n             if (!parseChunkHeader()) {\n                 throw new IOException(\"Invalid chunk\");\n@@ -184,7 +195,7 @@\n             chunk.setBytes(buf, pos, remaining);\n             pos = pos + remaining;\n             remaining = 0;\n-            parseCRLF();\n+            needCRLFParse = true;\n         }\n \n         return result;\n\n", "is_private": false, "id": 20516, "creator": "msmith@ns.xn.com.au", "time": "2002-08-01T07:14:41Z", "bug_id": 11117, "creation_time": "2002-08-01T07:14:41Z", "attachment_id": null}, {"count": 4, "tags": [], "creator": "msmith@ns.xn.com.au", "is_private": false, "id": 20648, "attachment_id": null, "bug_id": 11117, "creation_time": "2002-08-05T06:02:46Z", "time": "2002-08-05T06:02:46Z", "text": "Any chance of getting this fix committed? This is the only thing holding us from\nmoving to using 4.1.x (which we want to do for all the other nice things it\nadds), and this patch is straightforward and well tested.\n"}, {"attachment_id": null, "tags": [], "creator": "william.barker@wilshire.com", "is_private": false, "count": 5, "id": 20768, "time": "2002-08-08T02:56:44Z", "bug_id": 11117, "creation_time": "2002-08-08T02:56:44Z", "text": "Your patch has been applied.\n\nThanks much!"}, {"count": 6, "tags": [], "text": "Oh dear. Turns out there's another very closely related case which can be\ntriggered (but on my development machine, it never is - others have reported\nthis to me) in very rare circumstances. I changed the logic to compute the\nlength of the chunk as we read the chunk header, rather than first determining\nthe chunk header length, then computing it after the entire header has been\nread. This is neccesary because it's possible (if unlikely - so it gets\ntriggered rarely) for the buffer to be re-filled (with new data) in between\nthis, so the length computation fails.\n\nHere's the patch:\n\nIndex: src/java/org/apache/coyote/http11/filters/ChunkedInputFilter.java\n===================================================================\nRCS file:\n/home/cvspublic/jakarta-tomcat-connectors/http11/src/java/org/apache/coyote/http11/filters/ChunkedInputFilter.java,v\nretrieving revision 1.6\ndiff -u -r1.6 ChunkedInputFilter.java\n--- src/java/org/apache/coyote/http11/filters/ChunkedInputFilter.java   8 Aug\n2002 02:55:35 -0000       1.6\n+++ src/java/org/apache/coyote/http11/filters/ChunkedInputFilter.java   12 Aug\n2002 03:12:24 -0000\n@@ -283,8 +283,6 @@\n \n         int result = 0;\n         boolean eol = false;\n-        int begin = pos;\n-        int end = begin;\n         boolean readDigit = false;\n \n         while (!eol) {\n@@ -299,11 +297,9 @@\n                 eol = true;\n             } else {\n                 if (HexUtils.DEC[buf[pos]] != -1) {\n-                    if (!readDigit) {\n-                        readDigit = true;\n-                        begin = pos;\n-                    }\n-                    end = pos;\n+                    readDigit = true;\n+                    result *=16;\n+                    result += HexUtils.DEC[buf[pos]];\n                 }\n             }\n \n@@ -313,15 +309,6 @@\n \n         if (!readDigit)\n             return false;\n-\n-        int offset = 1;\n-        for (int i = end; i >= begin; i--) {\n-            int val = HexUtils.DEC[buf[i]];\n-            if (val == -1)\n-                return false;\n-            result = result + val * offset;\n-            offset = offset * 16;\n-        }\n \n         if (result == 0)\n             endChunk = true;\n", "is_private": false, "bug_id": 11117, "id": 20899, "time": "2002-08-12T07:34:58Z", "creator": "msmith@ns.xn.com.au", "creation_time": "2002-08-12T07:34:58Z", "attachment_id": null}, {"count": 7, "tags": [], "bug_id": 11117, "text": "I don't see any reason not to compute the chunk length on the fly, and that \nmakes the algorithm simpler. Sorry for not doing that in the first place ... \nBill, can you apply the patch if it looks good to you ? (I would only be able \nto commit it tomorrow)", "id": 20907, "time": "2002-08-12T11:02:02Z", "creator": "remm@apache.org", "creation_time": "2002-08-12T11:02:02Z", "is_private": false, "attachment_id": null}, {"count": 8, "tags": [], "creator": "william.barker@wilshire.com", "attachment_id": null, "text": "It seems that Remy found time to commit your patch before I got to it.  It's \nnow in the CVS HEAD.\n\nThanks again!", "id": 20930, "time": "2002-08-12T20:02:29Z", "bug_id": 11117, "creation_time": "2002-08-12T20:02:29Z", "is_private": false}]