[{"count": 0, "tags": [], "creator": "midenok@gmail.com", "attachment_id": null, "id": 141994, "time": "2010-11-25T06:57:28Z", "bug_id": 50335, "creation_time": "2010-11-25T06:57:28Z", "is_private": false, "text": "stack.1198:\n(most frequent core)\nThread 122 (Thread 1280):\n#0  0x00002b3b47a846f7 in kill () from /lib64/libc.so.6\n#1  <signal handler called>\n#2  0x00002b3b46d97acf in apr_brigade_cleanup (data=<value optimized out>) at buckets/apr_brigade.c:44\n#3  0x00002b3b4742088d in run_cleanups (cref=0x2aab280c3158) at memory/unix/apr_pools.c:2314\n#4  0x00002b3b4742130b in apr_pool_destroy (pool=0x2aab280c3138) at memory/unix/apr_pools.c:782\n#5  0x00002b3b474214c5 in apr_pool_clear (pool=0x2aab140834b8) at memory/unix/apr_pools.c:736\n#6  0x00002b3b4629ea94 in ?? ()\n#7  0x00002b3b4763a73d in start_thread () from /lib64/libpthread.so.0\n#8  0x00002b3b47b27d1d in clone () from /lib64/libc.so.6\n\n\nstack.5566:\nThread 20 (Thread 5808):\n#0  0x00002b3b47a846f7 in kill () from /lib64/libc.so.6\n#1  <signal handler called>\n#2  0x00002b3b4628b456 in ap_core_input_filter ()\n#3  0x00002b3b492fe298 in ap_proxy_http_process_response (p=0x2aaaf01ed998, r=0x2aaaf01eda18, backend=0x2b3b5c4022b8, origin=0x2b3b5c97a618, conf=0x2b3b5c03e250, server_portstr=0x2aaaddefbd50 \":3128\") at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:1767\n#4  0x00002b3b492fffeb in proxy_http_handler (r=0x2aaaf01eda18, worker=<value optimized out>, conf=0x2b3b5c03e250, url=0x2b3b5cdbae18 \"/xml.aspx?AppID=8DCA1510B4F71546E54034F13C5F41A837522444&Version=2.2&Sources=web&adult=off&web.count=50&Query=\\\"Leave+a+Reply\\\"+\\\"Name+(required)\\\"+\\\"Mail+(will+not+be+published)\\\"+\\\"Website\\\"+-site:wordpress\"..., proxyname=0x0, proxyport=0) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:2033\n#5  0x00002b3b490eb111 in proxy_run_scheme_handler (r=0x2aaaf01eda18, worker=0x2b3b5c000fb0, conf=0x2b3b5c03e250, url=0x2b3b5cdba9f6 \"http://api.bing.net/xml.aspx?AppID=8DCA1510B4F71546E54034F13C5F41A837522444&Version=2.2&Sources=web&adult=off&web.count=50&Query=\\\"Leave+a+Reply\\\"+\\\"Name+(required)\\\"+\\\"Mail+(will+not+be+published)\\\"+\\\"Websi\"..., proxyhost=0x0, proxyport=<value optimized out>) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:2563\n#6  0x00002b3b490ef1c0 in proxy_handler (r=0x2aaaf01eda18) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:1065\n#7  0x00002b3b4628bc9a in ap_run_handler ()\n#8  0x00002b3b4628f122 in ap_invoke_handler ()\n#9  0x00002b3b46299da8 in ap_process_request ()\n#10 0x00002b3b46296eb0 in ?? ()\n#11 0x00002b3b462931b2 in ap_run_process_connection ()\n#12 0x00002b3b4629ea65 in ?? ()\n#13 0x00002b3b4763a73d in start_thread () from /lib64/libpthread.so.0\n#14 0x00002b3b47b27d1d in clone () from /lib64/libc.so.6\n\n\nstack.5580:\nThread 131 (Thread 7291):\n#0  0x00002b3b47a846f7 in kill () from /lib64/libc.so.6\n#1  <signal handler called>\n#2  0x00002b3b46d99226 in apr_bucket_free (mem=0x2b3b5cd6f978) at buckets/apr_buckets_alloc.c:191\n#3  0x00002b3b46d97aea in apr_brigade_cleanup (data=<value optimized out>) at buckets/apr_brigade.c:44\n#4  0x00002b3b4742088d in run_cleanups (cref=0x2aaaec1cd768) at memory/unix/apr_pools.c:2314\n#5  0x00002b3b4742130b in apr_pool_destroy (pool=0x2aaaec1cd748) at memory/unix/apr_pools.c:782\n#6  0x00002b3b474214c5 in apr_pool_clear (pool=0x2b3b5c668378) at memory/unix/apr_pools.c:736\n#7  0x00002b3b4629ea94 in ?? ()\n#8  0x00002b3b4763a73d in start_thread () from /lib64/libpthread.so.0\n#9  0x00002b3b47b27d1d in clone () from /lib64/libc.so.6\n\n\nstack.5987:\nThread 117 (Thread 11007):\n#0  0x00002b3b47a846f7 in kill () from /lib64/libc.so.6\n#1  <signal handler called>\n#2  0x00002b3b490f1498 in ap_proxy_release_connection (proxy_function=0x2b3b49301777 \"HTTP\", conn=0x2b3b5ca8f218, s=0x2b3b5bffd590) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/proxy_util.c:2041\n#3  0x00002b3b492feed3 in ap_proxy_http_cleanup (scheme=0x2b3b49301777 \"HTTP\", r=<value optimized out>, backend=0x63203b6c6d74682f) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:1906\n#4  0x00002b3b492ff192 in proxy_http_handler (r=0x2b3b5cb5b878, worker=0x0, conf=0x2b3b5c03e250, url=0x2b3b5cb5d558 \"/squid-internal-dynamic/netdb\", proxyname=0x0, proxyport=0) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:2044\n#5  0x00002b3b490eb111 in proxy_run_scheme_handler (r=0x2b3b5cb5b878, worker=0x2b3b5c000fb0, conf=0x2b3b5c03e250, url=0x2b3b5cb5d426 \"http://174.142.104.57:3128/squid-internal-dynamic/netdb\", proxyhost=0x0, proxyport=<value optimized out>) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:2563\n#6  0x00002b3b490ef1c0 in proxy_handler (r=0x2b3b5cb5b878) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:1065\n#7  0x00002b3b4628bc9a in ap_run_handler ()\n#8  0x00002b3b4628f122 in ap_invoke_handler ()\n#9  0x00002b3b46299da8 in ap_process_request ()\n#10 0x00002b3b46296eb0 in ?? ()\n#11 0x00002b3b462931b2 in ap_run_process_connection ()\n#12 0x00002b3b4629ea65 in ?? ()\n#13 0x00002b3b4763a73d in start_thread () from /lib64/libpthread.so.0\n#14 0x00002b3b47b27d1d in clone () from /lib64/libc.so.6\n\n\nstack.6427:\nThread 148 (Thread 6923):\n#0  0x00002b3b47a846f7 in kill () from /lib64/libc.so.6\n#1  <signal handler called>\n#2  apr_brigade_split_line (bbOut=0x2aaaec2cc8b8, bbIn=0x2b3b5d0b18a0, block=APR_BLOCK_READ, maxbytes=8192) at buckets/apr_brigade.c:319\n#3  0x00002b3b4628b58a in ap_core_input_filter ()\n#4  0x00002b3b4627fc3a in ap_rgetline_core ()\n#5  0x00002b3b492fda8b in ap_proxygetline (bb=0x2aaaec2cc8b8, s=<value optimized out>, n=8192, r=0x2b3b5d09ce18, fold=0, writen=0x64310b7c) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:1329\n#6  0x00002b3b492fdc1f in ap_proxy_http_process_response (p=0x2aaaec5f39c8, r=0x2aaaec5f3a48, backend=0x2b3b5c7f23a8, origin=0x2b3b5d0b1158, conf=0x2b3b5c03e250, server_portstr=0x64314d50 \":3128\") at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:1400\n#7  0x00002b3b492fffeb in proxy_http_handler (r=0x2aaaec5f3a48, worker=<value optimized out>, conf=0x2b3b5c03e250, url=0x2aaaec4973f0 \"/login?u=951898728&p=08121B6D645B7277AFB3B87099F45C68&verifycode=&aid=15002005&u1=http%3A%2F%2Fqz.qq.com&h=1&ptredirect=1&ptlang=2052&from_ui=1&dumy=&fp=loginerroralert\", proxyname=0x0, proxyport=0) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:2033\n#8  0x00002b3b490eb111 in proxy_run_scheme_handler (r=0x2aaaec5f3a48, worker=0x2b3b5c000fb0, conf=0x2b3b5c03e250, url=0x2aaaec49712e \"http://ptlogin2.qq.com/login?u=951898728&p=08121B6D645B7277AFB3B87099F45C68&verifycode=&aid=15002005&u1=http%3A%2F%2Fqz.qq.com&h=1&ptredirect=1&ptlang=2052&from_ui=1&dumy=&fp=loginerroralert\", proxyhost=0x0, proxyport=<value optimized out>) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:2563\n#9  0x00002b3b490ef1c0 in proxy_handler (r=0x2aaaec5f3a48) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:1065\n#10 0x00002b3b4628bc9a in ap_run_handler ()\n#11 0x00002b3b4628f122 in ap_invoke_handler ()\n#12 0x00002b3b46299da8 in ap_process_request ()\n#13 0x00002b3b46296eb0 in ?? ()\n#14 0x00002b3b462931b2 in ap_run_process_connection ()\n#15 0x00002b3b4629ea65 in ?? ()\n#16 0x00002b3b4763a73d in start_thread () from /lib64/libpthread.so.0\n#17 0x00002b3b47b27d1d in clone () from /lib64/libc.so.6\n\n\nstack.8937:\nThread 8 (Thread 9841):\n#0  0x00002b3b47a846f7 in kill () from /lib64/libc.so.6\n#1  <signal handler called>\n#2  allocator_free (allocator=0x2b3b5bf855a0, node=0xa0) at memory/unix/apr_pools.c:358\n#3  apr_allocator_free (allocator=0x2b3b5bf855a0, node=0xa0) at memory/unix/apr_pools.c:416\n#4  0x00002b3b46d98b8b in socket_bucket_read (a=0x2aaaf0068ea8, str=0x2aaae61048e0, len=0x2aaae61048d8, block=<value optimized out>) at buckets/apr_buckets_socket.c:43\n#5  0x00002b3b46d9869b in apr_brigade_split_line (bbOut=0x2aab28028928, bbIn=0x2aab1c077af0, block=APR_BLOCK_READ, maxbytes=8192) at buckets/apr_brigade.c:319\n#6  0x00002b3b4628b58a in ap_core_input_filter ()\n#7  0x00002b3b4627fc3a in ap_rgetline_core ()\n#8  0x00002b3b492fda8b in ap_proxygetline (bb=0x2aab28028928, s=<value optimized out>, n=8192, r=0x0, fold=160, writen=0x2aaae6104b7c) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:1329\n#9  0x00002b3b492fdc1f in ap_proxy_http_process_response (p=0x2aab2810f638, r=0x2aab2810f6b8, backend=0x2aab1c0d3478, origin=0x2aab1c0773a8, conf=0x2b3b5c03e250, server_portstr=0x2aaae6108d50 \":3128\") at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:1400\n#10 0x00002b3b492fffeb in proxy_http_handler (r=0x2aab2810f6b8, worker=<value optimized out>, conf=0x2b3b5c03e250, url=0x2aab281113e0 \"/\", proxyname=0x0, proxyport=0) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy_http.c:2033\n#11 0x00002b3b490eb111 in proxy_run_scheme_handler (r=0x2aab2810f6b8, worker=0x2b3b5c000fb0, conf=0x2b3b5c03e250, url=0x2aab281112f6 \"http://174.142.104.57:3128/\", proxyhost=0x0, proxyport=<value optimized out>) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:2563\n#12 0x00002b3b490ef1c0 in proxy_handler (r=0x2aab2810f6b8) at /usr/src/debug/httpd-2.2.17.1/phorm/modules/proxy/mod_proxy.c:1065\n#13 0x00002b3b4628bc9a in ap_run_handler ()\n#14 0x00002b3b4628f122 in ap_invoke_handler ()\n#15 0x00002b3b46299da8 in ap_process_request ()\n#16 0x00002b3b46296eb0 in ?? ()\n#17 0x00002b3b462931b2 in ap_run_process_connection ()\n#18 0x00002b3b4629ea65 in ?? ()\n#19 0x00002b3b4763a73d in start_thread () from /lib64/libpthread.so.0\n#20 0x00002b3b47b27d1d in clone () from /lib64/libc.so.6"}, {"count": 1, "tags": [], "creator": "jadhavdeepakm@yahoo.co.in", "attachment_id": null, "id": 161816, "time": "2012-08-29T13:07:19Z", "bug_id": 50335, "creation_time": "2012-08-29T13:07:19Z", "is_private": false, "text": "Hi,\n\nI am also getting similer cores.\nIs there any solution for this?\n\nThanks,\nDeepak"}, {"count": 2, "tags": [], "creator": "max.romanov@gmail.com", "attachment_id": 29300, "id": 161818, "creation_time": "2012-08-29T14:01:17Z", "time": "2012-08-29T14:01:17Z", "bug_id": 50335, "text": "Created attachment 29300\nproposed changes in httpd 2.2.17", "is_private": false}, {"count": 3, "attachment_id": null, "bug_id": 50335, "text": "Hello,\n\nMostly, I am getting 1st and 3rd above type of core dump:\n\ni.e.core in #4 0x0812a58f in apr_brigade_cleanup (data=0xf656e780) at\nbuckets/apr_brigade.c:44\n\nand core in #4 0x0812acdc in apr_bucket_free (mem=0xe7aaaa28) at\nbuckets/apr_buckets_alloc.c:191\n\nIs above core dump issue fixed in this patch? Also are there any particular test cases to test this patch/fix?\n\nThanks in advance.\n\nRegards,\nDeepak", "id": 161833, "time": "2012-08-30T10:09:51Z", "creator": "jadhavdeepakm@yahoo.co.in", "creation_time": "2012-08-30T10:09:51Z", "tags": [], "is_private": false}, {"count": 4, "tags": [], "creator": "max.romanov@gmail.com", "attachment_id": null, "id": 161835, "time": "2012-08-30T10:38:05Z", "bug_id": 50335, "creation_time": "2012-08-30T10:38:05Z", "is_private": false, "text": "No specific test cases. Just free proxy under high load. I've noticed no crashes after patch.\nThis is a race-condition issue caused by using request pool for buckets send to back-end."}, {"count": 5, "tags": [], "creator": "stimmins@wiley.co.uk", "attachment_id": null, "text": "We are seeing this on our production servers running httpd v2.2.22 and 2.2.21.\n\nBefore a recent upgrade from Oracle/Sun SPARC T2 servers (1 processor, 8 core, 64 thread, 1.45Ghz clock speed, 64GB RAM) the rate was tiny with less than 1 crash per day on average.\n\nAfter replacing these servers with Oracle/Sun SPARC T4 servers (1 processor, 8 core, 64 thread, 2.85GHz clock speed, 128GB RAM) we get far more ranging from 0 to 5 per hour on any of the servers.\n\nI am going to try the attached patch to see if this makes the system more stable.", "id": 162157, "time": "2012-09-13T14:05:45Z", "bug_id": 50335, "creation_time": "2012-09-13T14:05:45Z", "is_private": false}, {"count": 6, "tags": [], "creator": "jim@apache.org", "attachment_id": null, "id": 162163, "creation_time": "2012-09-13T15:36:27Z", "time": "2012-09-13T15:36:27Z", "bug_id": 50335, "text": "If you JUST use the diff for the actual proxy module (mod_proxy_http.c and proxy_util.c), does the problem go away?\n\nThe concern is that all the other changes just don't look right and \"hide\" the real problem. For example, the use of f->c->bucket_alloc should be correct and the  change to b->bucket_alloc is \"safe\" but unneeded.", "is_private": false}, {"count": 7, "tags": [], "bug_id": 50335, "text": "Jim,\n\nIf the bucket is going to be inserted to brigade 'b' what is correct to allocate the bucket using other object?\nI agree, in some places equal object used, but I think it is _correct_ to use 'b->bucket_alloc', and it is _sometimes safe_ use connection to allocate it.", "id": 162175, "time": "2012-09-14T10:00:07Z", "creator": "max.romanov@gmail.com", "creation_time": "2012-09-14T10:00:07Z", "is_private": false, "attachment_id": null}, {"count": 8, "tags": [], "bug_id": 50335, "attachment_id": null, "text": "The reference to \"httpd-2.2.17.1/phorm\" in the original backtrace implies this is a non-standard, possibly patched httpd.  It would be useful to see actual backtraces from unpatched 2.2.22/2.2.23.\n\nWhat Jim said, otherwise.  That patch looks like an attempt to fix the problem fixed in r713146 doesn't it?", "id": 162180, "time": "2012-09-14T14:02:49Z", "creator": "jorton@redhat.com", "creation_time": "2012-09-14T14:02:49Z", "is_private": false}, {"count": 9, "tags": [], "creator": "stimmins@wiley.co.uk", "attachment_id": null, "id": 163075, "time": "2012-10-31T08:50:35Z", "bug_id": 50335, "creation_time": "2012-10-31T08:50:35Z", "is_private": false, "text": "Here are two different stack traces from an unpatched httpd v2.2.22 that seems to be exhibiting the same issue as per my previous comment.\n\nI am also running one instance of httpd v2.2.22 with the attached proposed patch applied.\n\nSystems are Oracle T4s running Solaris 10 (Generic_147330-09)\n\n#0  0xff297500 in apr_brigade_cleanup (data=0xdf8828)\n    at buckets/apr_brigade.c:44\n44              apr_bucket_delete(e);\n(gdb) where\n#0  0xff297500 in apr_brigade_cleanup (data=0xdf8828)\n    at buckets/apr_brigade.c:44\n#1  0xff125fb0 in run_cleanups (cref=0xdf49f0) at memory/unix/apr_pools.c:2346\n#2  0xff1270a0 in apr_pool_destroy (pool=0xdf49e0)\n    at memory/unix/apr_pools.c:809\n#3  0xff1272fc in apr_pool_clear (pool=0xdec9c0) at memory/unix/apr_pools.c:764\n#4  0x00050158 in worker_thread (thd=0xcc9c0, dummy=0x3) at worker.c:897\n#5  0xff131f1c in dummy_worker (opaque=0xcc9c0) at threadproc/unix/thread.c:142\n#6  0xfef3a9d0 in _lwp_start () from /lib/libc.so.1\n#7  0xfef3a9d0 in _lwp_start () from /lib/libc.so.1\nBacktrace stopped: previous frame identical to this frame (corrupt stack?)\n(gdb)\n\n\n#0  0xfef3e3ac in _read () from /lib/libc.so.1\n(gdb) where\n#0  0xfef3e3ac in _read () from /lib/libc.so.1\n#1  0xfef2ca24 in read () from /lib/libc.so.1\n#2  0x0005296c in ap_mpm_pod_check (pod=0xacf50) at pod.c:54\n#3  0x00050a0c in child_main (child_num_arg=17) at worker.c:1258\n#4  0x00050b88 in make_child (s=0x7b800, slot=17) at worker.c:1341\n#5  0x000513b8 in ap_mpm_run (_pconf=0xfe710158, plog=0x14, s=0x2693)\n    at worker.c:1560\n#6  0x0002afec in main (argc=5, argv=0xffbffb6c) at main.c:753"}, {"count": 10, "attachment_id": null, "bug_id": 50335, "text": "I have been running the patched httpd v2.2.22 binary for over 24 hours now and there have been no new core dumps on it, while my other three unpatched instances of httpd v2.2.22 have produced a total of 18 core dumps over the same time period (All 4 instances are serving the same site).\n\nI will continue to run with this configuration until tomorrow morning when I will switch the patched instance to a version of httpd 2.2.22 that has been patched with the proposed changes, but excluding the patch to \"srclib/apr-util/buckets/apr_brigade.c\" as request by Covener. I will then report back after it has been running for a day or so.\n\nIf there are any other things I can test please let me know.", "id": 163100, "time": "2012-10-31T21:21:36Z", "creator": "stimmins@wiley.co.uk", "creation_time": "2012-10-31T21:21:36Z", "tags": [], "is_private": false}, {"count": 11, "attachment_id": null, "bug_id": 50335, "text": "We have now been running the modified patch as described in Comment 10 for about 4 days and have had no core dumps on the patched server at all compared to over 50 cores between the three unpatched server over the same time period.\n\nI'll be rolling out the patched httpd v2.2.22 to the other three servers tomorrow.", "id": 163164, "time": "2012-11-05T00:18:57Z", "creator": "stimmins@wiley.co.uk", "creation_time": "2012-11-05T00:18:57Z", "tags": [], "is_private": false}, {"count": 12, "attachment_id": null, "bug_id": 50335, "text": "We got another core! The frequencey has dropped dramatically (only 1 in over a week). This is with the partial patch as detailed in Comment 10.\n\nHere is the stack trace.\n\n#0  0xfef3e3ac in _read () from /lib/libc.so.1\n(gdb) where\n#0  0xfef3e3ac in _read () from /lib/libc.so.1\n#1  0xfef2ca24 in read () from /lib/libc.so.1\n#2  0x00052950 in ap_mpm_pod_check (pod=0x126240) at pod.c:54\n#3  0x000509f0 in child_main (child_num_arg=4) at worker.c:1258\n#4  0x00050b6c in make_child (s=0x7b800, slot=4) at worker.c:1341\n#5  0x0005139c in ap_mpm_run (_pconf=0xfe8d0158, plog=0x14, s=0x0)\n    at worker.c:1560\n#6  0x0002afec in main (argc=5, argv=0xffbffa0c) at main.c:753", "id": 163292, "time": "2012-11-08T17:25:10Z", "creator": "stimmins@wiley.co.uk", "creation_time": "2012-11-08T17:25:10Z", "tags": [], "is_private": false}, {"count": 13, "attachment_id": null, "bug_id": 50335, "text": "From the (gdb) prompt you need to run\n\nthread apply all backtrace full\n\nand attach that file.\n\nThe thread you captured is not doing anything interesting.", "id": 163295, "time": "2012-11-08T21:36:14Z", "creator": "trawick@apache.org", "creation_time": "2012-11-08T21:36:14Z", "tags": [], "is_private": false}, {"count": 14, "tags": [], "bug_id": 50335, "text": "Created attachment 29576\noutput from: gdb thread apply all backtrace full\n\ngdb output as requested", "id": 163307, "time": "2012-11-09T11:06:10Z", "creator": "stimmins@wiley.co.uk", "creation_time": "2012-11-09T11:06:10Z", "is_private": false, "attachment_id": 29576}, {"count": 15, "tags": [], "creator": "trawick@apache.org", "attachment_id": null, "id": 163309, "time": "2012-11-09T11:31:18Z", "bug_id": 50335, "creation_time": "2012-11-09T11:31:18Z", "is_private": false, "text": "Here's the crasher in the last backtrace posted.  (I filtered out any threads that were blocked in syscalls.)\n\nThread 39 (process 2641292    ):\n#0  PyObject_Malloc (nbytes=47) at Objects/obmalloc.c:758\n        bp = (block *) 0xffffffff <Address 0xffffffff out of bounds>\n        pool = (poolp) 0xa72000\n        next = (poolp) 0xff3aa270\n        size = 5\n#1  0xfe6d5f98 in PyString_FromString (str=0xad86d0 \"onlinelibrary.wiley.com\")\n    at Objects/stringobject.c:138\n        op = (PyStringObject *) 0x2f\n#2  0xfe6c6f14 in PyDict_GetItemString (v=0xad86d0, \n    key=0xad86d0 \"onlinelibrary.wiley.com\") at Objects/dictobject.c:2284\n        kv = (PyObject *) 0xad86d0\n        rv = (PyObject *) 0x26b8a0\n#3  0xfe68f0fc in get_interpreter (name=0xad86d0 \"onlinelibrary.wiley.com\")\n    at mod_python.c:255\n        tstate = (PyThreadState *) 0xd22730\n        idata = (interpreterdata *) 0x0\n#4  0xfe68f620 in python_cleanup (data=0x7d7798) at mod_python.c:353\nNo locals.\n#5  0xff125fb0 in run_cleanups (cref=0xafd488) at memory/unix/apr_pools.c:2346\n        c = (cleanup_t *) 0x613790\n#6  0xff1270a0 in apr_pool_destroy (pool=0xafd478)\n    at memory/unix/apr_pools.c:809\n        active = (apr_memnode_t *) 0x0\n        allocator = (apr_allocator_t *) 0xafd478\n...\n\nAlthough this is also on a pool cleanup, it may be completely unrelated.  See if any subsequent crashes have different backtraces (suggestive of there still being a more general issue with pool use) or if they now are consistently in this code (suggestive of this particular code needing to be reviewed carefully)."}, {"count": 16, "attachment_id": null, "bug_id": 50335, "text": "I'd suggest to apply patch for apr_brigade.c because it fixes potential memory corruption - it is unsafe to put the bucket allocated in one pool to the brigade allocated from another one.", "id": 163313, "time": "2012-11-09T14:22:50Z", "creator": "max.romanov@gmail.com", "creation_time": "2012-11-09T14:22:50Z", "tags": [], "is_private": false}, {"count": 17, "tags": [], "creator": "trawick@apache.org", "attachment_id": null, "id": 163316, "creation_time": "2012-11-09T16:06:45Z", "time": "2012-11-09T16:06:45Z", "bug_id": 50335, "text": "(In reply to comment #16)\n> I'd suggest to apply patch for apr_brigade.c because it fixes potential\n> memory corruption - it is unsafe to put the bucket allocated in one pool to\n> the brigade allocated from another one.\n\nAs I understand it, the patch was already applied (good), the frequency of core dumps reduced significantly (awesome!), and what remains so far is potentially very different.", "is_private": false}, {"count": 18, "tags": [], "creator": "stimmins@wiley.co.uk", "attachment_id": null, "id": 163348, "time": "2012-11-10T18:39:27Z", "bug_id": 50335, "creation_time": "2012-11-10T18:39:27Z", "is_private": false, "text": "Currently I have 4 servers running with the attached patch applied *apart* from the patch to srclib/apr-util/buckets/apr_brigade.c\n\nI can switch between this httpd build and a build with srclib/apr-util/buckets/apr_brigade.c patched as well as they are both installed on the servers.\n\nJust let me know which you want me to run.\n\nThanks for your help on this, it's very much appreciated."}, {"count": 19, "tags": [], "bug_id": 50335, "attachment_id": 29585, "id": 163356, "creation_time": "2012-11-11T23:14:45Z", "time": "2012-11-11T23:14:45Z", "creator": "stimmins@wiley.co.uk", "text": "Created attachment 29585\ngdb output from core 2012-11-11\n\nWe had another core so here is the output from running the same gdb command as before.\n\nI'm afraid I don't have the requisite knowledge to identify the relevant thread(s)", "is_private": false}, {"count": 20, "attachment_id": null, "bug_id": 50335, "text": "One additional piece of information. Althouth I realise rare event can happen, both cores were dumped at exactly 03:00, the first on Nov 5th and the second on Nov 11th.\n\nLooking at the system scheduled jobs, the only one I have found that runs at 03:00 is a root cron syncs the system time with an internal time server.  This is run just once per day. Unfortunately we are not currently logging the amount by which the time changes.", "id": 163357, "time": "2012-11-11T23:30:32Z", "creator": "stimmins@wiley.co.uk", "creation_time": "2012-11-11T23:30:32Z", "tags": [], "is_private": false}, {"count": 21, "tags": [], "creator": "trawick@apache.org", "attachment_id": null, "text": "(In reply to comment #19)\n> Created attachment 29585 [details]\n> gdb output from core 2012-11-11\n> \n> We had another core so here is the output from running the same gdb command\n> as before.\n> \n> I'm afraid I don't have the requisite knowledge to identify the relevant\n> thread(s)\n\nThe only thread I see that could have crashed is\n\nThread 1 (process 2370996    ):\n#0  PyObject_Malloc (nbytes=41) at Objects/obmalloc.c:758\n        bp = (block *) 0x144679f \"\"\n        pool = (poolp) 0x1446000\n        next = (poolp) 0xfc4aac10\n        size = 5\n#1  0xfe73626c in PyString_FromStringAndSize (str=0x143ed79 \"olbannerright.gif\", size=17) at Objects/stringobject.c:83\n        op = (PyStringObject *) 0x29\n#2  0xfe6f8464 in PySequence_GetSlice (s=0x143ed40, i1=37, i2=54)\n    at Objects/abstract.c:1979\n        m = (PySequenceMethods *) 0xfe85ef9c\n        mp = (PyMappingMethods *) 0xfe85ef9c\n#3  0xfe7bfc74 in match_group (self=0x1437cc8, args=0x14439d0)\n    at Modules/_sre.c:3228\n        result = (PyObject *) 0x800000\n        i = 3\n        size = -24778856\n#4  0xfe727934 in PyCFunction_Call (func=0xd467d8, arg=0x14439d0, kw=0x0)\n    at Objects/methodobject.c:116\n        meth = (PyCFunction) 0xfe7bfa2c <match_group>\n        self = (PyObject *) 0x1437cc8\n        size = 1\n#5  0xfe7733d0 in PyEval_EvalFrameEx (f=0x108a090, throwflag=5889119)\n    at Python/ceval.c:3706\n...\n\nAs with your previous crash, this crash is in PyObject_Malloc.  Unlike the previous one, it is not in a pool cleanup.\n\nAny idea if this infrequent crash could have been going on \"forever\"?  Is the crash frequently still much lower than before you applied the patch to this bug?", "id": 163358, "time": "2012-11-11T23:36:00Z", "bug_id": 50335, "creation_time": "2012-11-11T23:36:00Z", "is_private": false}, {"count": 22, "tags": [], "bug_id": 50335, "text": "Yes it is possible that the crash has been going on for some time. As I mentioned in comment 5 the crashes became far more noticeable after upgrading our hardware from SPARC T2 to SPARC T4 machines.\n\nWith the old hardware we were only getting around 1 core per day across all 4 servers and unfortunately there were always more pressing things to deal with. After the hardware upgrade the frequencey of crashes was much higher (multiple cores per day per server) and so it received more attention.\n\nWith the patch version of httpd, the crash frequency appears to be quite a bit less than it was even before we updated the hardware (which is great!) but I'll be more sure of this after I've run it for a while longer.", "id": 163362, "time": "2012-11-11T23:48:45Z", "creator": "stimmins@wiley.co.uk", "creation_time": "2012-11-11T23:48:45Z", "is_private": false, "attachment_id": null}, {"count": 23, "tags": [], "bug_id": 50335, "text": "Created attachment 29596\ngdb output from core 2012-11-13\n\nAnother core today, again at exactly 03:00.", "id": 163422, "time": "2012-11-13T13:18:22Z", "creator": "stimmins@wiley.co.uk", "creation_time": "2012-11-13T13:18:22Z", "is_private": false, "attachment_id": 29596}, {"count": 24, "tags": [], "creator": "trawick@apache.org", "attachment_id": null, "id": 163424, "time": "2012-11-13T13:42:36Z", "bug_id": 50335, "creation_time": "2012-11-13T13:42:36Z", "is_private": false, "text": "(In reply to comment #23)\n> Created attachment 29596 [details]\n> gdb output from core 2012-11-13\n> \n> Another core today, again at exactly 03:00.\n\nHere's the backtrace of the crashing thread:\n\n1 * [1] MPM child worker thread (running request handler)\n  PyObject_Malloc, conn_alloc, CS_CONTEXT_ct_con_alloc, PyCFunction_Call, PyEval_EvalFrameEx, PyEval_EvalCodeEx, function_call, PyObject_Call, instancemethod_call, PyObject_Call, PyEval_CallObjectWithKeywords, PyInstance_New, PyObject_Call, PyEval_EvalFrameEx, PyEval_EvalCodeEx, PyEval_EvalFrameEx, PyEval_EvalFrameEx, PyEval_EvalCodeEx, function_call, PyObject_Call, instancemethod_call, PyObject_Call, PyEval_CallObjectWithKeywords, PyInstance_New, PyObject_Call, PyEval_EvalFrameEx, PyEval_EvalFrameEx, PyEval_EvalFrameEx, PyEval_EvalCodeEx, function_call, PyObject_Call, PyEval_EvalFrameEx, PyEval_EvalCodeEx, PyEval_EvalFrameEx, PyEval_EvalCodeEx, PyEval_EvalFrameEx, PyEval_EvalFrameEx, PyEval_EvalFrameEx, PyEval_EvalCodeEx, PyEval_EvalFrameEx, PyEval_EvalCodeEx, function_call, PyObject_Call, instancemethod_call, PyObject_Call, PyObject_CallMethod, python_handler, ap_run_handler, ap_invoke_handler, ap_process_request, ap_process_http_connection, ap_run_process_connection, worker_thread, dummy_worker\n\nFor context, here are the previous two in the same format:\nNov 11:\n1 * [1] MPM child worker thread (running request handler) \n  PyObject_Malloc, PyString_FromStringAndSize, PySequence_GetSlice, match_group, PyCFunction_Call, PyEval_EvalFrameEx, PyEval_EvalCodeEx, PyEval_EvalFrameEx, PyEval_EvalFrameEx, PyEval_EvalFrameEx, PyEval_EvalCodeEx, PyEval_EvalFrameEx, PyEval_EvalCodeEx, function_call, PyObject_Call, instancemethod_call, PyObject_Call, PyObject_CallMethod, python_handler, ap_run_handler, ap_invoke_handler, ap_process_request, ap_process_http_connection, ap_run_process_connection, worker_thread, dummy_worker\nNov 9:\n1 * [39] MPM child worker thread \n  PyObject_Malloc, PyString_FromString, PyDict_GetItemString, get_interpreter, python_cleanup, run_cleanups, apr_pool_destroy, apr_pool_clear, worker_thread, dummy_worker\n\nThe 3:00 times suggest that *something* happens consistently, while the occurrences in PyObject_Malloc suggest that the problem is very possibly limited to the Python domain.  (I wouldn't say it is definitely caused by something in the Python space, but it needs to be investigated from that direction.  E.g., is the corruption caused by a memory overlay and is the data recognizable?)\n\nI don't see evidence in the backtraces of any web server-wide activity such as graceful restart.  I guess you've checked the error and access logs for anything interesting around that time?  Potentially you could wire up mod_whatkilledus (http://emptyhammock.com/projects/httpd/diag/quick-start.html) to see if there is something in common about the requests which trigger the issue.\n\nUnfortunately:\n. This should be investigated separately from the mod_proxy issue (bug 50335) as there's no reason to think they are related, and continuing here may be confusing to others trying to understand the proxy issue.\n. mod_python is officially dead for about 2.5 years, and I don't see any discussion on the mod_python mailing list for over a year, so I don't know where people that might have hints for debugging corruption in Python/mod_python memory management would hang out."}, {"count": 25, "tags": [], "creator": "stimmins@wiley.co.uk", "attachment_id": null, "id": 163427, "time": "2012-11-13T14:35:34Z", "bug_id": 50335, "creation_time": "2012-11-13T14:35:34Z", "is_private": false, "text": "Ok, lets close this part of the discussion as far as these newer core dumps go. (I'll work on getting rid of mod_python instead) Things are far better for me with the patch as it is.\n\nJust for my information, is there any chance the patch will make it's way into an official release?"}, {"count": 26, "tags": [], "text": "bump -- this ticket came up on IRC -- to recap, the httpd patch alone was sufficient in an environment where crashes were pretty regular, but it's all way over my head.    Does anyone grok it enough to declare it's not working around some other issue, or what the sacrifice is?", "attachment_id": null, "bug_id": 50335, "id": 164569, "time": "2013-01-10T15:53:27Z", "creator": "covener@gmail.com", "creation_time": "2013-01-10T15:53:27Z", "is_private": false}, {"count": 27, "tags": [], "creator": "alex@alex.org.uk", "attachment_id": null, "id": 168481, "time": "2013-07-10T21:14:06Z", "bug_id": 50335, "creation_time": "2013-07-10T21:14:06Z", "is_private": false, "text": "I'm not sure this is just a mod_proxy bug (and the fact more than just mod_proxy is patched confirms this). I am seeing this in an entirely different proxy ( https://github.com/abligh/apache-websocket ).\n\nIt would be really useful to understand what the non-mod_proxy changes are doing in the patch."}, {"count": 28, "tags": [], "bug_id": 50335, "text": "I have not managed to reproduce the segfaults, but I think I now understand the problem expressed in comment 7 with bucket allocator mismatches.\n\nSpecifically, I think this is possible:\n\na) ap_proxy_http_request allocates header brigade in the proxy worker pool \"scpool\"\nb) buckets are inserted into that brigade, allocated from the bucket allocator from \"ptrans\"\nc) <something happens>\nd) header brigade is NOT CLEANED UP\ne) ptrans allocator gets destroyed, bucket memory are invalidated\nf) later... scpool is cleared/destroyed, header brigade gets cleaned up but has a corrupt bucket list -> boom\n\nI confirmed (a) and (b) by hacking ap_pass_brigade() to compare e->list against bb->bucket_alloc for every bucket in the passed-in brigade.  I don't know precisely how to trigger (c) thru (f) but it is consistent with reported symptoms, of a crash in running the brigade pool cleanup.\n\n*IF* this is all correct then a simple fix is just to avoid (a) & (b).  Any other thoughts?\n\n-    header_brigade = apr_brigade_create(p, origin->bucket_alloc);\n+    header_brigade = apr_brigade_create(p, bucket_alloc);", "id": 169577, "time": "2013-08-19T18:58:58Z", "creator": "jorton@redhat.com", "creation_time": "2013-08-19T18:58:58Z", "is_private": false, "attachment_id": null}, {"count": 29, "tags": [], "bug_id": 50335, "attachment_id": 30743, "text": "Created attachment 30743\npossible fix for PR 50335\n\nPossible fix.", "id": 169578, "time": "2013-08-19T19:00:03Z", "creator": "jorton@redhat.com", "creation_time": "2013-08-19T19:00:03Z", "is_private": false}, {"count": 30, "tags": [], "text": "(In reply to Joe Orton from comment #28)\n> I have not managed to reproduce the segfaults, but I think I now understand\n> the problem expressed in comment 7 with bucket allocator mismatches.\n> \n> Specifically, I think this is possible:\n> \n> a) ap_proxy_http_request allocates header brigade in the proxy worker pool\n> \"scpool\"\n> b) buckets are inserted into that brigade, allocated from the bucket\n> allocator from \"ptrans\"\n> c) <something happens>\n> d) header brigade is NOT CLEANED UP\n\nCould happen if we return early in ap_proxy_pass_brigade.\n\n> e) ptrans allocator gets destroyed, bucket memory are invalidated\n> f) later... scpool is cleared/destroyed, header brigade gets cleaned up but\n> has a corrupt bucket list -> boom\n\nRight: scpool and ptrans have completly different lifecycles and scpool could live longer than ptrans.\n\n> \n> I confirmed (a) and (b) by hacking ap_pass_brigade() to compare e->list\n> against bb->bucket_alloc for every bucket in the passed-in brigade.  I don't\n> know precisely how to trigger (c) thru (f) but it is consistent with\n> reported symptoms, of a crash in running the brigade pool cleanup.\n> \n> *IF* this is all correct then a simple fix is just to avoid (a) & (b).  Any\n> other thoughts?\n> \n> -    header_brigade = apr_brigade_create(p, origin->bucket_alloc);\n> +    header_brigade = apr_brigade_create(p, bucket_alloc);\n\nThis looks like the correct thing to do. Can someone of the original reporters please check?\n\nIn addition I propose the following patch to have the brigade cleaned up in any case in ap_proxy_pass_brigade:\n\nIndex: proxy_util.c\n===================================================================\n--- proxy_util.c        (revision 1515728)\n+++ proxy_util.c        (working copy)\n@@ -3305,6 +3305,7 @@\n     if (transferred != -1)\n         p_conn->worker->s->transferred += transferred;\n     status = ap_pass_brigade(origin->output_filters, bb);\n+    apr_brigade_cleanup(bb);\n     if (status != APR_SUCCESS) {\n         ap_log_rerror(APLOG_MARK, APLOG_ERR, status, r, APLOGNO(01084)\n                       \"pass request body failed to %pI (%s)\",\n@@ -3324,7 +3325,6 @@\n             return HTTP_BAD_REQUEST;\n         }\n     }\n-    apr_brigade_cleanup(bb);\n     return OK;\n }", "attachment_id": null, "bug_id": 50335, "id": 169592, "time": "2013-08-20T08:02:19Z", "creator": "rpluem@apache.org", "creation_time": "2013-08-20T08:02:19Z", "is_private": false}, {"count": 31, "tags": [], "bug_id": 50335, "attachment_id": null, "text": "Hmmm, I am struggling to repro still.\n\nAn error return from ap_proxy_http_request() should always trigger destruction of scpool.  So there is still a missing piece to the puzzle.", "id": 169605, "time": "2013-08-20T16:33:08Z", "creator": "jorton@redhat.com", "creation_time": "2013-08-20T16:33:08Z", "is_private": false}, {"count": 32, "tags": [], "bug_id": 50335, "attachment_id": null, "text": "Unfortunately I'm unable to reproduce the issue again because we have no longer this installation.\n\nAbout other changes in my patch, I can comment I did it after small code instrumentation to macros which inserts the bucket to brigade - it checks whether bucket is allocated with same or parent pool as brigade and assert otherwise.", "id": 169621, "time": "2013-08-20T21:07:07Z", "creator": "max.romanov@gmail.com", "creation_time": "2013-08-20T21:07:07Z", "is_private": false}, {"count": 33, "tags": [], "bug_id": 50335, "text": "I committed my patch in r1534321 - I have a user seeing similar crashes trying this out.", "id": 170739, "time": "2013-10-22T08:28:47Z", "creator": "jorton@redhat.com", "creation_time": "2013-10-22T08:28:47Z", "is_private": false, "attachment_id": null}, {"count": 34, "attachment_id": null, "bug_id": 50335, "text": "The problem is that make_fake_req uses scpool. Fake request generated by this method is passed to ap_get_brigade by following code:\n\nrv = ap_get_brigade(backend->r->input_filters, bb,\n                    AP_MODE_READBYTES, mode,\n                    conf->io_buffer_size);\n\n\"bb\" is brigade created with \"request\" pool. Input_filters later adds new buckets created from \"scpool\" into brigade created with \"request\" pool. The problem here is when scpool gets destroyed before request pool. I'm not able to reproduce it now, but I'm pretty sure that's what we see here.\n\nI think the problem is in some \"break;\" in the code block right after ap_get_brigade, because in this case \"bb\" brigade is not cleaned up. I think to trigger this, input_filter has to return some strange error and add something to brigade in the same time, so it's not clean, but the mod_proxy presumes it is clean when this return value is returned.\n\nNote that this is also particular thing which is changed in the patch by Max. The patch changes pool used by input_filters.\n\nI'm proposing another patch which cleans \"bb\" brigade in the end of ap_proxy_http_process_response.", "id": 171678, "time": "2013-12-10T08:45:38Z", "creator": "jkaluza@redhat.com", "creation_time": "2013-12-10T08:45:38Z", "tags": [], "is_private": false}, {"count": 35, "tags": [], "bug_id": 50335, "attachment_id": 31103, "text": "Created attachment 31103\nAnother possible fix for PR 50335", "id": 171679, "time": "2013-12-10T08:47:06Z", "creator": "jkaluza@redhat.com", "creation_time": "2013-12-10T08:47:06Z", "is_private": false}, {"count": 36, "tags": [], "bug_id": 50335, "attachment_id": null, "text": "I have no idea why other people ignores two major changes from the first patch:\n1. allocate request to backend from backend connection pool\n2. avoid moving of buckets between brigades allocated from the non-relational pools\n\nIt is hard to reproduce and analyse such issues. I prefer to perform redundant copying than have once-at-week core file.", "id": 171680, "time": "2013-12-10T12:34:54Z", "creator": "max.romanov@gmail.com", "creation_time": "2013-12-10T12:34:54Z", "is_private": false}, {"count": 37, "attachment_id": null, "bug_id": 50335, "text": "(In reply to Max Romanov from comment #36)\n> I have no idea why other people ignores two major changes from the first\n> patch:\n> 1. allocate request to backend from backend connection pool\n\nSince the backend's connection can handle multiple requests, they would leak on its pool.\nYour patch creates a subpool for the fake request, you have to explicitly destroy (or clear/recycle) it somewhere, and that's more work/cycles (per request).", "id": 171681, "time": "2013-12-10T13:06:57Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2013-12-10T13:06:57Z", "tags": [], "is_private": false}, {"count": 38, "attachment_id": null, "bug_id": 50335, "text": "Agree, this is extra work.\nConnection will be closed early or later and destroy all subpools.", "id": 171683, "time": "2013-12-10T13:35:46Z", "creator": "max.romanov@gmail.com", "creation_time": "2013-12-10T13:35:46Z", "tags": [], "is_private": false}, {"count": 39, "tags": [], "text": "I have committed my patch as r1550061.", "attachment_id": null, "bug_id": 50335, "id": 171690, "time": "2013-12-11T07:30:55Z", "creator": "jkaluza@redhat.com", "creation_time": "2013-12-11T07:30:55Z", "is_private": false}, {"count": 40, "tags": [], "creator": "jorton@redhat.com", "attachment_id": null, "id": 171691, "time": "2013-12-11T11:06:47Z", "bug_id": 50335, "creation_time": "2013-12-11T11:06:47Z", "is_private": false, "text": "Using a subpool for the fake request sounds like it is not actually fixing the difficult problem here of getting the lifetimes right.\n\nBut Max is right that basically everything after that ap_get_brigade() call is potentially dangerous as-is because of dangers with bucket lifetime mismatches.\n\nComparing trunk with 2.2.x I notice there is a further complication in r1035605\n\nIn the EOF case, the proxy now reads HEAP buckets from the backend socket, converts them into TRANSIENT buckets so the bucket *structure* lifetime is correct... and then converts those buckets back into HEAP buckets so the bucket structure the bucket *data* lifetime are correct.  That seems over-complicated."}, {"count": 41, "tags": [], "bug_id": 50335, "attachment_id": null, "text": "I get similar segmentation faults with Apache 2.4.6, apr 1.4.8, apr-util 1.5.3 and mod_security 2.7.5 (from Debian unstable).\n\nI configured a simple reverse proxy and enabled mod_security for a location:\n\n<Location />\n    SecRuleEngine On\n    SecRequestBodyAccess On\n\n    ProxyPass http://backend:8080/\n    ProxyPassReverse http://backend:8080/\n</Location>\n\nOn the backend I run faucet to simulate a request-dropping backend server:\n\nfaucet 8080 --out echo \"\"\n\nNow I send POST requests in parallel by starting this loop on multiple shells, the more the better (data_file is 25k):\n\nwhile true ; do curl -d @data_file http://frontend/ ; done\n\nEvery once in a while I get a segmentation fault.\n\nAfter applying the following patches (attachments 30743 and 31103) I get no more segmentation faults:\n\nhttp://svn.apache.org/viewvc?view=revision&revision=1534321\nhttp://svn.apache.org/viewvc?view=revision&revision=1550061", "id": 171711, "time": "2013-12-12T15:05:22Z", "creator": "ewald_dieterich@t-online.de", "creation_time": "2013-12-12T15:05:22Z", "is_private": false}, {"count": 42, "tags": [], "creator": "jkaluza@redhat.com", "attachment_id": null, "id": 171766, "time": "2013-12-16T09:21:39Z", "bug_id": 50335, "creation_time": "2013-12-16T09:21:39Z", "is_private": false, "text": "Thanks for the info, that looks promising :)."}, {"count": 43, "tags": [], "text": "Backported to v2.4.8.", "attachment_id": null, "bug_id": 50335, "id": 171942, "time": "2013-12-26T18:28:30Z", "creator": "minfrin@sharp.fm", "creation_time": "2013-12-26T18:28:30Z", "is_private": false}, {"count": 44, "tags": [], "text": "*** Bug 55979 has been marked as a duplicate of this bug. ***", "attachment_id": null, "bug_id": 50335, "id": 172203, "time": "2014-01-09T09:01:44Z", "creator": "avf@eldamar.org.uk", "creation_time": "2014-01-09T09:01:44Z", "is_private": false}, {"count": 45, "attachment_id": null, "bug_id": 50335, "text": "*** Bug 50834 has been marked as a duplicate of this bug. ***", "id": 172499, "time": "2014-01-19T19:28:33Z", "creator": "covener@gmail.com", "creation_time": "2014-01-19T19:28:33Z", "tags": [], "is_private": false}]