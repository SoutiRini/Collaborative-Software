[{"count": 0, "tags": [], "bug_id": 47865, "text": "Created attachment 24286\nTwo new listeners (similar to AgreggateReport but much faster) and optimized version of DistributionGraph\n\nPackage contains two listener with functionality similar to Aggeregate Raport\nbut designed to work perfectly with huge amount of data (millions samples) and\nhight throughput (hundreds per seconds) and optimized DistributionGraph. Code\nrequires at least java 5.0.\n\n\nDetails:\n\n- UpgradedStatVizualizer and associated classes are similar to StatVisualizer\nbut faster. Those classes doesn't hold all samples but only some part. Main\ngoal is to calculate avg, standard deviation, avg 90% line, avg 99% line and\navg 99.9% line without holding all data. No more than MAX_COUNT samples are\nhold. When number of samples are great than MAX_COUNT then 90% line, 99% line\nand 99.9% are calculated on those samples. Those values are used to update avg\n90% line, avg 99% line and avg 99.9% Then samples are removed and collected\nfrom beginning. The lower clearLimit the better performance but the higher\nclearLimit the more accurate avg lines (90%, 99% and 99.9%).Partial results of\nthe test can be saved in csv files, one file for each sampler in case of\nsystem/machine failure/restart those results can be used to calculate overall\nstatistics. To synchronize threads ReentranstLocks are used instead of\nsynchronized methods.\n\n- BacketStatVisualizer annd associated classes are also similar to\nStatVisualizer and UpgradedStatVizualizer. Designed to work with huge amount of\nsamples with high throughput. Rather not use if the maximum value of the\nsamples is high (more than 1-2 min if it time). Much faster than default\nStatCalculator, can be a little slower than UpgradedStatCalculator but instead\nof UpgradedStatCalculator gives exactly value of median, 90% line, 99% line and\n99.9% line (not some approximation), due to performance those value are\nrefreshed every REFRESH_RATE samples. Work perfectly with DistributionGraph.\n\n- AbstractSamplingStatCalculator super class for all sampling statistic\ncalculators\n\n- SamplingStatContainer interface for all sampling statistic calculators\n\n- DistributionGraph refactored to use SamplingStatContainer interface instead\nof SamplingStatCalculator and as a model BacketStatCalculator so now when\nnumber of samples become huge distribution graph doesn't slow down\n\n- AbstractStatCalculator super class for all statistics calculators\n\n- updated messages and messages_pl properties files\n\n- refactored class in org.apache.jmeter.testelement package to use interface\nSamplingStatContainer instead of class SamplingStatCalculator", "id": 130501, "time": "2009-09-17T23:35:59Z", "creator": "ragnor84@gmail.com", "creation_time": "2009-09-17T23:35:59Z", "is_private": false, "attachment_id": 24286}, {"count": 1, "attachment_id": 24287, "creator": "ragnor84@gmail.com", "is_private": false, "id": 130502, "time": "2009-09-17T23:42:16Z", "bug_id": 47865, "creation_time": "2009-09-17T23:42:16Z", "tags": [], "text": "Created attachment 24287\nJUnits test for statistics calculators\n\nJUnit tests for StatCalculator, UpgradedStatCalculator and BacketStatCalculator"}, {"count": 2, "tags": [], "bug_id": 47865, "is_private": false, "text": "As far as I can tell, the BacketStatCalculator assumes that all Numbers are positive and no larger than Integer.MAX_VALUE - is that correct?", "id": 130656, "time": "2009-09-24T10:35:49Z", "creator": "sebb@apache.org", "creation_time": "2009-09-24T10:35:49Z", "attachment_id": null}, {"count": 3, "tags": [], "bug_id": 47865, "text": "Yes it's true. If this is problem HashMap<Number, Long> can be used instead of array as a internal container for data but performance will be lower so I chose array because performance was my target. \nIf you think that this listener (or DistributionGraph) should be used in long (even days) stress test with high throughput then you can assume that response time is very low probably less than 1s so this listener will work correctly and efficiently.\nBacketStatCalculator is optimized to work as a data model for DistributionGraph and doesn't slow down the test because it gives information about number of occurrences of each sample (value) without any extra calculation.\n\n\n\nBTW. I created all those listeners because when I was testing my application after 2 minutes when jmeter collected about 100 000 samples throughput was started fall from 800 req/s to 300 req/s after several minutes. At the beginning I'd thought it was a problem with my application but then I realized that when I cleared collected samples in jmeter throughput was again about 800 req/s (for about 2 min ;)). Aggregate report was the bottle neck but 90% line is very useful and I need it (extra I need 99% and 99.9% line) so I created UpgradedAggregateReport that can calculate some approximation of 90, 99 and 99.9% line. I realized that I can calculate exactly  value of those lines and I created BacketStatCalculator (performance can be a bit lower than UpgradedStatCalculator).\n\nJust to summarize before I made any optimization, jmeter with 30 thread could generate about 2000 req/s to my application (after few minutes throughput was dropping) after I made some optimization in listeners, integration with javascript, used collections and thread synchronization in jmeter code, jmeter can generate about 7500 req/s. Nowadays I can make tests that take 48h or more and collect 600 000 000 samples and I can save 3 server (currently 1 machine can generate load equals to 4 machine previously).", "id": 130658, "time": "2009-09-24T12:06:20Z", "creator": "ragnor84@gmail.com", "creation_time": "2009-09-24T12:06:20Z", "is_private": false, "attachment_id": null}, {"count": 4, "tags": [], "creator": "sebb@apache.org", "attachment_id": null, "text": "Thanks. Given that these calculations are all done on response time (milliseconds), I agree that the limitation is not likely to cause a problem.\n\nBTW, I just realised that one could save memory at the expense of some accuracy by using centiseconds instead of milliseconds (i.e. divide the response time by 10 before storing in the bucket). One could even scale the divisor to keep the rounding roughly proportional to the value, but there would be the extra cost of checking the value.", "id": 130659, "time": "2009-09-24T12:42:28Z", "bug_id": 47865, "creation_time": "2009-09-24T12:42:28Z", "is_private": false}, {"count": 5, "tags": [], "creator": "ragnor84@gmail.com", "text": "I think memory is not a problem and divide the response time by 10 will decrease performance (better use 8 or 16 then divide will be faster). In my case it is important to know if the response time was 1ms or 9ms, 10ms or 19ms - when centiseconds will be use all response from 0-9ms go to first bucket and 10-19ms go to second, details information will be lost, so I prefer to have 1ms backets. \nIn regard to the memory lets assume that the longest response time is 1 min (I don't know who will be wait so long for response but maybe there are patient people;)), so the backet array will occupied:\n1 * 60 * 1000 * 8 = 480 000 = 468 kb\nso less than 0.5 MB.\nOne important thing to noticed is that this array will not grow during the test no matter how many samples are collected, so there is no different if there are 1 000 or 10 000 000 samples collected if all are below 1 min. Another important thing is that because this is an array there is fast access to each element, so counters increment in backets is fast.\n\nBut maybe it will be useful to have an option in gui to set the backet size, one will choose 1ms (like me) and others maybe 8 or 16ms.", "id": 130663, "attachment_id": null, "bug_id": 47865, "creation_time": "2009-09-24T14:13:31Z", "time": "2009-09-24T14:13:31Z", "is_private": false}, {"count": 6, "tags": [], "creator": "sebb@apache.org", "is_private": false, "id": 132847, "attachment_id": null, "bug_id": 47865, "creation_time": "2009-12-15T17:04:13Z", "time": "2009-12-15T17:04:13Z", "text": "See Bug 48259 - Improve StatCalculator performance\nand the changes in http://svn.apache.org/viewvc?rev=891076&view=rev\n\nThese have vastly improved the performance of Aggregate Report and many other Visualizers.\n\nHaving made those changes, I now realise that using TreeMap to store aggregated response times is a similar solution to the bucket approach. It has the advantage that the number of buckets does not have to be specified in advance.\n\nSo I think the changes proposed here are probably no longer necessary."}]