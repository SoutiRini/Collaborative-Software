[{"count": 0, "tags": [], "bug_id": 33374, "text": "Hi,\n\nThere is a problem with mod_jk and Tomcat under high load that causes slots to\nbe kept by Apache even though the backend Tomcat links has gone down. This is\ncaused by too eager user aborts, ie. where you have web clients that abort the\nconnection by just closing it. You get a broken pipe on the Tomcat side and\neventually that is cleared out - since under less traffic this is not an issue.\nNow consider that you get hammered with requests and many are aborted, ie. out\nof 30 per seconds 10 are aborted. \n\nThere are two issues with that, both I will list as separate reports:\n\n1. Reporting errors to user code\n2. Closing connections\n\nWe use Apache/2.0.52 (Unix) mod_ssl/2.0.52 OpenSSL/0.9.7d mod_jk/1.2.6 and\nTomcat 5.0.28\n\nThis report is for problem #2 above. \n\nFor some reason, when we get those broken pipe exceptions as reported in bug\n#33371 we are having big problems with Apache. What happens is that the\nscoreboard of Apache fills up with connections (see attached file), all slots\nshow status \"W\" which means it the request is in work by the handler, in this\ncase mod_jk. This only happens if we go over a certain amount of requests per\nsecond, otherwise it does not cause any problems. But when we go over the limit\nand the slots fill up, what we notice is that we get heaps of \"Broken pipe\"\nexceptions on the Tomcat side, where we have 9-10 app servers all running\nTomcat, load balanced by mod_jk. All of these servers start to show these Broken\npipe exceptions. It is not that we never have those exceptions, of course when a\nuser aborts, then we get that exception in the logs. But at peak time and when\nthe slots fill up in Apache, then we have heaps of them, ie. a couple of them\nper second.\n\nThose errors may be caused because of the too eager calling system closing\nconnection all the time because we respond a bit to slow, but shouldn't those\nconnections be closed all the way, ie. also on the Apache side, in other words\nthe connections between Apache and the user? \n\nThat is the part that seem to fail. Furthermore we get heaps of stray\nconnections on the Tomcat machines. If you do a \"netstat -p -n\" you get heaps of\nconnections to local port 8009 (AJP13) but no process is connected to it. I\ntried to fix this by applying a this patch to Tomcat 5.0.28, file:\n/jakarta-tomcat-5.0.28-src/jakarta-tomcat-connectors/jk/java/org/apache/jk/common/ChannelSocket.java:\n\n267d266\n<     final int error=8;\n507,513c506\n<         try {\n<             os.write( buf, 0, len );\n<         } catch (IOException ex) {\n<             ep.setNote( error, Boolean.TRUE ); \n<             log.warn(\"Flagging endpoint as in error. \" + ep.getNote(\nsocketNote ) + \"\\nError was: \" + ex.getMessage() );\n<             throw ex;\n<         }\n---\n>         os.write( buf, 0, len );\n522,528c515\n<             try {\n<                 os.flush();\n<             } catch (IOException ex) {\n<                 ep.setNote( error, Boolean.TRUE );\n<                 log.warn(\"Flagging endpoint as in error. \" + ep.getNote(\nsocketNote ) + \"\\nError was: \" + ex.getMessage() );\n<                 throw ex;\n<             }\n---\n>             os.flush();\n681c668\n<                         log.warn(\"Closing ajp connection \" + status + \" - \" +\nep.getNote( socketNote ));\n---\n>                         log.warn(\"Closing ajp connection \" + status );\n691,695d677\n<                     break;\n<                 }\n<                 if ( ep.getNote ( error ) == Boolean.TRUE ) {\n<                     log.warn(\"processConnection: Error reported, closing\nconnection. \" + ep.getNote( socketNote ));\n<                     ep.setNote( error, null );\n\n\nWhat it does is remembering when there was an error during IO and when control\ncomes back all the way through the call stack it eventually closes this connection. \n\nThis is a problem in itself and as reported in bug #33371, the user code does\nnot get the error, but even if it does it can swallow the exception just like\nthe current code does and therefore the low-level code does not handle the\nproblem at all. My patch makes sure that connections that have caused IO errors\nto be thrown to close the connection.\n\nThat took care of our stray connections, and things worked better for a while,\nbut we are getting the problem again. The bottomline seems to be that my patch\nis useful and should in some way or another be part of the Coyote code, but\nunder heavy load the problem is not solved yet. \n\nIf you investigate the attached scoreboard dump, you will see how long some of\nthose connections are sitting there, ie. some of the values in the \"Seconds\nSince\" (SS) column are greater than 800! And we set all timeouts we could find\nto a lower value:\n\nhttpd.conf:           Timeout 60\nworker.properties:    worker.tc1.reply_timeout=120000\nworker.properties:    worker.tc1.socket_timeout=60\n\nWhy do they get stuck? I am pretty sure they are all those many Broken pipe\nexception connections we get, but since applying my patch I do close them all,\nwhy does that not flow back to mod_jk?\n\nThe biggest problem of all is that mod_jk creates a new connection to the app\nservers and does not clear out the previous connection to the user. What happens\nis that when we have those 1000 slots on 3 web servers full, and I do a thread\ndump on the 9-10 app servers, all 150 configured TP-Processor threads are idle!\nThey all show this state:\n\n\"TP-Processor183\" daemon prio=1 tid=0x5af00018 nid=0x626a runnable\n[8bfff000..8bfffb58]\n    at java.net.SocketInputStream.socketRead0(Native Method)\n    at java.net.SocketInputStream.read(SocketInputStream.java:129)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:183)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:222)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:277)\n    - locked <0x4e030028> (a java.io.BufferedInputStream)\n    at org.apache.jk.common.ChannelSocket.read(ChannelSocket.java:611)\n    at org.apache.jk.common.ChannelSocket.receive(ChannelSocket.java:548)\n    at org.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:676)\n    at org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:890)\n    at\norg.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:683)\n    at java.lang.Thread.run(Thread.java:534)\n\nwhich means they wait for a new request to come in, but this does not happens\nsince all Apache slots are full. \n\nTo recap:\n\n- Connections (User<>Apache) do not get reset or released (quick enough) under\nheavy load when they should (eg. user closes connection)\n- Closing connection in Coyote code does only partially help the problem.\n- mod_jk creates new connection to Tomcat, but old connection to user is not\ndropped.\n\nI am sorry that I cannot assist any further yet, but I keep reading the mod_jk\nand Tomcat code and see if I can find anything. Until then i wanted to get the\nissue out in hope that someone else may have an idea.\n\nRegards,\n\nLars George, CTO\nWorldLingo", "id": 70420, "time": "2005-02-02T22:51:42Z", "creator": "lars@worldlingo.com", "creation_time": "2005-02-02T22:51:42Z", "is_private": false, "attachment_id": null}, {"count": 1, "tags": [], "creator": "lars@worldlingo.com", "attachment_id": 14161, "text": "Created attachment 14161\nScoreboard dump showing full slots.", "id": 70422, "time": "2005-02-02T22:52:23Z", "bug_id": 33374, "creation_time": "2005-02-02T22:52:23Z", "is_private": false}, {"count": 2, "tags": [], "creator": "lars@worldlingo.com", "attachment_id": 14162, "text": "Created attachment 14162\nPatches ChannelSocket to close broken connections.", "id": 70423, "time": "2005-02-02T22:53:12Z", "bug_id": 33374, "creation_time": "2005-02-02T22:53:12Z", "is_private": false}, {"count": 3, "tags": [], "creator": "william.barker@wilshire.com", "attachment_id": null, "text": "This is fixed in the CVS, and will appear in the next version of TC 5.5, 4.1, \nand 3.3.", "id": 70441, "time": "2005-02-03T04:38:28Z", "bug_id": 33374, "creation_time": "2005-02-03T04:38:28Z", "is_private": false}, {"count": 4, "tags": [], "bug_id": 33374, "text": "William,\n\nWasn't the change meant to fix bug #33371? \n\nLars", "id": 70442, "time": "2005-02-03T06:01:54Z", "creator": "lars@worldlingo.com", "creation_time": "2005-02-03T06:01:54Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 33374, "text": "(In reply to comment #4)\n> William,\n> Wasn't the change meant to fix bug #33371? \n> Lars\n\nNope.  This is the socket not getting closed fast enough.  Tomcat still \ndoesn't throw an exception from action.  As I stated in 33371, I can't see why \nthe servlet wouldn't get an IOE in this case, so I can't really fix it ;-).", "id": 70443, "time": "2005-02-03T06:32:15Z", "creator": "william.barker@wilshire.com", "creation_time": "2005-02-03T06:32:15Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 33374, "text": "Hi Bill (or anyone else who can help here),\n\nI just checked and neither 5.5.20 nor 6.0 of Tomcat does have that patch applied\nto it. Could you please advise why?\n\nAlso, if you are involved in 6.0, do you know if anything discussed has been\nimproved in 6.0? I mean there were a few design issue with the ChannelSocket and\nrelated classes. Is there a better alternative?\n\nPlease note, that we still have the same problem as back then.\n\nThanks and best regards,\nLars\n", "id": 103892, "time": "2007-05-31T11:11:44Z", "creator": "lars@worldlingo.com", "creation_time": "2007-05-31T11:11:44Z", "is_private": false, "attachment_id": null}]