[{"count": 0, "tags": [], "bug_id": 46808, "attachment_id": 23337, "is_private": false, "id": 125347, "time": "2009-03-05T18:03:30Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-05T18:03:30Z", "text": "Created attachment 23337\npatch for jk_lb_worker.c\n\nFor instance, assume to use one lb worker with two sub workers.\n\nWhen the network cable of one AP server is pulled out,\najp_send_request() returns JK_FATAL_ERROR.\n\nBecause the state of lbworker doesn't become JK_LB_STATE_ERROR when\n\"rec->s->busy\" is larger than 0, there is a possibility that the next \nrequest is transmitted to the server to which the network cable has \nbeen pulled out.\nIn this case, the request wasted additional time for connecting.\nThis additional time depends on \"socket_timeout\"( or \"socket_connect_timeout\") and \"retries\".\n\nActually, when the network cable was pulled out by intention in the load test, \nsuch a situation was generated. \nAnd the waste of time continued until the network cable was reconnected.\n\nIf the request has stickyness, it seems that the waste of time can be reduced \nby using JvmRouteBinderValve on the Tomcat.\n\nIn case of mod_jk 1.2.22, the situation like 1.2.27 was not generated\nbecause it made an error of the state of the lbworker if the connection\nfailed.\n\nIn the following cases, ajp_service() returns JK_FATAL_ERROR.\nIn these cases, it seems that there might be no problem even if the\nstate of lbworker becomes an error state.\n\n * JK_FATAL_ERROR      JK_FALSE              ajp_connection_tcp_send_message() returns JK_FATAL_ERROR\n *           Endpoint belongs to unknown protocol.\n * JK_FATAL_ERROR                            JK_TRUE ajp_connection_tcp_send_message() returns JK_FALSE\n *           Sending request or request body in jk_tcp_socket_sendfull() returns with error.\n * JK_FATAL_ERROR      JK_TRUE               Could not connect to backend\n * JK_FATAL_ERROR         ?                  ajp_process_callback() returns JK_AJP13_ERROR\n *           JK_AJP13_ERROR: protocol error, or JK_INTERNAL_ERROR: chunk size to large\n\nIndex: mod_jk-head/native/common/jk_lb_worker.c\n===================================================================\n--- mod_jk-head/native/common/jk_lb_worker.c\t(revision 750704)\n+++ mod_jk-head/native/common/jk_lb_worker.c\t(working copy)\n@@ -1339,12 +1339,8 @@\n                      * Time for fault tolerance (if possible)...\n                      */\n                     rec->s->errors++;\n-                    if (rec->s->busy) {\n-                        rec->s->state = JK_LB_STATE_OK;\n-                    }\n-                    else {\n-                        rec->s->state = JK_LB_STATE_ERROR;\n-                    }\n+                    rec->s->state = JK_LB_STATE_ERROR;\n+\n                     p->states[rec->i] = JK_LB_STATE_ERROR;\n                     rec->s->error_time = time(NULL);\n                     rc = JK_FALSE;\n\nBest regards."}, {"count": 1, "tags": [], "text": "Your patch breaks other things that are more important.\nThe check fof busy is deliberate because athough one connection can be\nrejected others might still work. This would mean that\nnext request on keep-alive connection will fail.\n\nSo, -1 for the patch", "is_private": false, "id": 125368, "creator": "mturk@apache.org", "time": "2009-03-05T23:38:00Z", "bug_id": 46808, "creation_time": "2009-03-05T23:38:00Z", "attachment_id": null}, {"count": 2, "tags": [], "creator": "rainer.jung@kippdata.de", "attachment_id": null, "text": "I'm also investigating this and try to fully understand all implications of r647085.\n\nOne first question (to Mladen): In this revision you introduced the local states and an additional busy counter. From your changelog entry I assume you want to be able to behave differently depending on whether the process handling the request has a concurrent request running on the problematic backend, or not.\n\nMy question is: the freshly introduced busy for the lb sub worker is located in shared memory. Shouldn't it be in the process local sub worker data?", "id": 125369, "time": "2009-03-05T23:51:05Z", "bug_id": 46808, "creation_time": "2009-03-05T23:51:05Z", "is_private": false}, {"count": 3, "tags": [], "text": "Right the problem here is that we don't know weather\nthe failed connection to backend was caused by Tomcat\nrejecting the connection because too busy or someone pulled\nthe cable.\nWhat we did before was mark the entire worker in error if\none of the connection failed, and that cause the exiting\nconnections failed as well on next request.\nEg. 100 connections to Tomcat is operational. 101st client\ncomes in and it is rejected by tomcat (maxThreads=100).\nBefore we would kill all 100 connections on next request\nbecause the 101st reported the Tomcat is 'in-error'\n\nNow, the drawback with the s->busy is that each client request\nwill observe the connection delay (and that is much beter then\npresuming all connections are down)\n\nIf we could reliably distinguish the return codes from\njk_connect (reject or network totally down) we could then\nship the s->busy check and mark the entire worker in error\n\nAnyhow, the proposed patch sets thing like there were before,\nand that will break more things cause the amount of problems\ncaused by rejecting connections is far far far (did I say far)\nmore common that someone pulling out the calble from a\nproduction server :)", "is_private": false, "id": 125370, "creator": "mturk@apache.org", "time": "2009-03-06T00:06:07Z", "bug_id": 46808, "creation_time": "2009-03-06T00:06:07Z", "attachment_id": null}, {"count": 4, "tags": [], "text": "So having the \"new\" busy in shm is on purpose, and we have three of those:\n\na) one for the lb in total\nb) one for each lb sub\nc) one for each ajp worker\n\nb) and c) are very likely, but not the same, e.g. when the ajp worker is used in more than one lb, or is used in an lb and also as a direct worker.\n\nBut from your explanation I would think, that using the c) busy would be even better than using the b) busy?", "attachment_id": null, "bug_id": 46808, "id": 125371, "time": "2009-03-06T00:13:55Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2009-03-06T00:13:55Z", "is_private": false}, {"count": 5, "tags": [], "text": "Did you actually use prepose cping/cpong and socket_connect_timeout to keep latency of error detection low for the two cases \"already connected\" and \"new connection\"? If so, what kind of real problems did you experience:\n\n- requests took unacceptably long\n- requests got lost\n- resources usage was unacceptably higher\n- something else\n\nI understand your formal reasoning, but as Mladen pointed out, there are also situations were the negative impact of a global failover are much worse (all sessions are lost).\n\nSo the question here is, what is the observed problem. I do understand your \"problem\" view from a code perspective, but let's first understand it from a functional point of view.\n\nRegards,\n\nRainer", "is_private": false, "bug_id": 46808, "id": 125379, "time": "2009-03-06T03:56:54Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2009-03-06T03:56:54Z", "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 46808, "attachment_id": null, "id": 125405, "time": "2009-03-07T07:28:41Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-07T07:28:41Z", "is_private": false, "text": "Hi.\n\nI'm sorry that the explanation is insufficient. \n\nI used the roughly following settings. \n-----\nworker.list=wlb\n\n# not use socket_connect_timeout\nworker.ap00.socket_timeout=5\nworker.ap00.prepost_timeout=3000\nworker.ap00.connect_timeout=3000\n\n# not use max_reply_timeouts\nworker.ap00.reply_timeout=310000\n\nworker.ap00.recovery_options=3\n\nworker.ap01.reference=ap00\nworker.ap02.reference=ap00\n\nworker.wlb.sticky_session=True\nworker.wlb.sticky_session_force=False\nworker.wlb.balance_workers=ap01, ap02\nworker.wlb.method=B\n---\n\nPlease look at the attached log about an actual situation. \n - The NW cable has been pulled out around 13:35:47.\n - The request that has ap01 as route tried to connect to ap01, and failed to connect. And ap01 was in local error state (not error state).\n - Ap01 had become in error state (not local error state) at 13:50:06. It seems that it is because all sessionid with ap01 as a route ended. \n\nWhen the NW cable on the ap01 side is pulled out, the request that has been transmitted to Tomcat returns error to the client because of recovery_options is set to 3. I think that this is not a problem. \n\nHowever, when the state of lbworker doesn't become JK_LB_STATE_ERROR, the request that has ap01 as route in the sessionid tries to connect to ap01. And keeps trying to connect to ap01 until sessionid become invalid. I think that this is a problem, because \"socket_timeout * retries\" ( equals 10secs in my case ) is added to the request processing time without fail. \n\nWhen the request that don't have sessionid connects to ap01, the time for the connection is wasted.  I think this is a problem, too. However, because the failover occurs, the request is forwarded to ap02 and sessionid is given in ap02. As a result, the next time of this request is not transmitted to ap01.\n\nBest regards."}, {"count": 7, "tags": [], "creator": "mashmk02@gmail.com", "attachment_id": 23349, "text": "Created attachment 23349\nlog file 1 of 4", "id": 125407, "time": "2009-03-07T07:43:57Z", "bug_id": 46808, "creation_time": "2009-03-07T07:43:57Z", "is_private": false}, {"count": 8, "tags": [], "bug_id": 46808, "attachment_id": 23350, "is_private": false, "id": 125408, "time": "2009-03-07T07:44:52Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-07T07:44:52Z", "text": "Created attachment 23350\nlog file 2 of 4"}, {"count": 9, "tags": [], "text": "Created attachment 23351\nlog file 3 of 4", "attachment_id": 23351, "bug_id": 46808, "id": 125409, "time": "2009-03-07T07:45:38Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-07T07:45:38Z", "is_private": false}, {"count": 10, "tags": [], "text": "Created attachment 23352\nlog file 4 of 4", "is_private": false, "bug_id": 46808, "id": 125410, "time": "2009-03-07T07:47:10Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-07T07:47:10Z", "attachment_id": 23352}, {"count": 11, "tags": [], "text": "Oops! I'm sorry. The log file was too large. I divided the log file.", "is_private": false, "bug_id": 46808, "id": 125411, "time": "2009-03-07T07:49:16Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-07T07:49:16Z", "attachment_id": null}, {"count": 12, "tags": [], "creator": "mturk@apache.org", "attachment_id": null, "text": "> However, when the state of lbworker doesn't become JK_LB_STATE_ERROR, the\n> request that has ap01 as route in the sessionid tries to connect to ap01. And\n> keeps trying to connect to ap01 until sessionid become invalid.\n\nNope this is completely valid. The node is retried in a conservative maner.\nBefore pulling out the cable you should mark the node a Disabled, and\nwhen all sessions times out, then pull the cable.\n\nWe cannot simply kill all sessions if one of them fails.\nRainer is working on a solution that will allow to make that\ndelay not larger then recover_wait_timeout.\n\nAgain, this is edge case and you should really not pull out\nthe cables out of production servers :)", "id": 125415, "time": "2009-03-07T10:22:54Z", "bug_id": 46808, "creation_time": "2009-03-07T10:22:54Z", "is_private": false}, {"count": 13, "tags": [], "creator": "mashmk02@gmail.com", "attachment_id": null, "text": "> Nope this is completely valid. The node is retried in a conservative maner.\n> Before pulling out the cable you should mark the node a Disabled, and\n> when all sessions times out, then pull the cable.\nI think that it is correct to mark the node a Disabled if it is a scheduled down.\nBecause I was investigating how mod_jk behaved when the NW problem occurred suddenly, I couldn't mark the node a Disabled.\n\n> \n> We cannot simply kill all sessions if one of them fails.\n> Rainer is working on a solution that will allow to make that\n> delay not larger then recover_wait_timeout.\nI see. I am waiting for the solution.\n\n> \n> Again, this is edge case and you should really not pull out\n> the cables out of production servers :)\nYes. Of course!", "id": 125430, "time": "2009-03-09T00:35:03Z", "bug_id": 46808, "creation_time": "2009-03-09T00:35:03Z", "is_private": false}, {"count": 14, "tags": [], "bug_id": 46808, "attachment_id": null, "is_private": false, "id": 125455, "time": "2009-03-09T20:01:13Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2009-03-09T20:01:13Z", "text": "I discussed with Mladen and we both committed a few changes.\n\nBut let's first express our expectation to what should happen.\n\nWe don't want the system to behave overly nervous. If the load balancer marks a node as in ERROR, no more requests will be send there for some time. Most applications need stickyness, and in many cases people do not use session replication. So marking a node as being in ERROR has serious implications for all people whose sessions live on the node and who try to access it as long as it is in error.\n\nOn the other hand sending traffic to a node that really is broken obviously also has serious implications.\n\nNow what everyone needs to do is using the socket_connect_timeout and CPing/CPong to check that the node has some basic connectivity available. With these fatures each request can make sure for itself, that it will fail over to another node in a relatively timely and robust manner. I think that worked in your situation.\n\nNow what we don't want to do is as soon as a node doesn't react on one CPing or one connection attempt, taking it out of service (marking as in ERROR).\n\nInstead we implemented now, that we are looking at the timestamp of when we last had such bad behaviour and if this is longer ago than recover_time/2. In that case we will mark the node as ERROR.\n\nIn your case this should mean: directly after the cable breaks (let's assume you didn't plug the cable but it magically broke), the system will behave like it does for 1.2.27. All requests for that node will take a little longer, because they first have to go through their connect or Cping/Cpong timeouts. But after (default) 30 seconds, the node should be put into the global ERROR state, so no more requests will be sent there. Unless every now and then a request suceeds, which means the node isn't totally broken.\n\nI think that should be a good compromise. It limits the amount of time an initial problem, like the broken cable, negatively influences the system, but it also limits the negative influence a temporarily overloaded node could have, if we immediately would put it into ERROR.\n\nThe magical limit (recover_wait/2) is configurable independently, so you can get close to the 1.2.22 behaviour by setting error_escalation_time to 0, but I don't recommend doing that.\n\nThis is new code, and I hope you find a chance of testing it. I will put it into\n\nhttp://people.apache.org/~rjung/mod_jk-dev/source/jk-1.2.28-dev/\n\nin a minute.\n\nIt might not yet be the latest revision for 1.2.28, but I think it's close :)"}, {"count": 15, "tags": [], "bug_id": 46808, "attachment_id": 23364, "id": 125456, "time": "2009-03-09T23:59:56Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-09T23:59:56Z", "is_private": false, "text": "Created attachment 23364\npatch for jk_util.c\n\nThank you for dealing with this problem.\nI will examine it in some patterns.\n- error_escalation_time = 30(default)/20/10/0(!)\n\nI made the patch because I was not able to set error_escalation_time. \n\nBest regards."}, {"count": 16, "tags": [], "bug_id": 46808, "attachment_id": null, "id": 125458, "time": "2009-03-10T01:13:06Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2009-03-10T01:13:06Z", "is_private": false, "text": "Doh!\n\nFixed (r752016)."}, {"count": 17, "tags": [], "text": "Hi.\n\nThe result of the test was excellent.\nThanks a lot!", "is_private": false, "bug_id": 46808, "id": 125501, "time": "2009-03-11T17:49:43Z", "creator": "mashmk02@gmail.com", "creation_time": "2009-03-11T17:49:43Z", "attachment_id": null}]