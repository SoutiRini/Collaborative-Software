[{"count": 0, "tags": [], "creator": "carsten@wolffcarsten.de", "attachment_id": 35421, "id": 201459, "time": "2017-10-14T10:09:36Z", "bug_id": 61616, "creation_time": "2017-10-14T10:09:36Z", "is_private": false, "text": "Created attachment 35421\ndefault vhost with proxy_connect conf\n\nWhen proxy_connect is used for taffic that is less of a request-response nature, it can happen, that the send buffer on the backend socket gets full. In that case, in libapr, when the writev() comes back with EAGAIN, it will block in poll() on the backend socket with a 300-second timeout. No read from any of the two sockets can happen anymore in that situation. Thus, depending on the application protocol that is tunneled through CONNECT, the backend application will then fillup the receive buffer at proxy_connect's backend socket and then stall trying to send indefinitely. Now proxy_connect and the backend application are in a deadlock.\n\nA concrete setup to reproduce it (using debian stretch):\n\nrsync -------------------------> apache ---------------> rsync --daemon\n        CONNECT localhost:873\n\nI will attach configuration and a script to this report and can also provide a qemu image, if desired.\n\nIn this setup the stall looks like this:\n\n----------------------------- rsync ----------------------------------------\n\n# RSYNC_PROXY=localhost:80 rsync -rcP rsync://localhost:873/testsrc/ /testdst/\n[..]\n5269\n              0   0%    0.00kB/s    0:00:00  \n[ .. 5 minute delay .. ]\nrsync: connection unexpectedly closed (403374822 bytes received so far) [receiver]\nrsync error: error in rsync protocol data stream (code 12) at io.c(235) [receiver=3.1.2]\nrsync: connection unexpectedly closed (242925 bytes received so far) [generator]\nrsync error: error in rsync protocol data stream (code 12) at io.c(235) [generator=3.1.2]\n\n----------------------------- netstat --------------------------------------\n\n# netstat -pnt | grep -v :22\nActive Internet connections (w/o servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \n\ntcp6       0         0 ::1:43642               ::1:80                  ESTABLISHED 21638/rsync\ntcp6  169814         0 ::1:80                  ::1:43642               ESTABLISHED 20628/apache2\ntcp6  5974823  2608576 ::1:43798               ::1:873                 ESTABLISHED 20628/apache2\ntcp6  809935   2760413 ::1:873                 ::1:43798               ESTABLISHED 21639/rsync\n\n----------------------------- strace --------------------------------------\n\n[..]\nread(10, \"\\24\\372\\342\\1\\16\\255\\223\\375G\\213k\\251\\215\\360\\357}\\311\\275\\202\\5jQ>\\341>\\370 jr\\357,\\4\"..., 8000) = 8000    <---- fd 10 is client socket\nwritev(11, [{iov_base= [..] ., iov_len=7424}], 16) = -1 EAGAIN (Resource temporarily unavailable)                      <---- fd 11 is backend socket\npoll([{fd=11, events=POLLOUT}], 1, 300000)\n <detached ...>\n\n----------------------------- gdb -----------------------------------------\n\n(gdb) bt\n#0  0x00007f37a88c4690 in __poll_nocancel () at ../sysdeps/unix/syscall-template.S:84\n#1  0x00007f37a8dc5088 in apr_poll () from target:/usr/lib/x86_64-linux-gnu/libapr-1.so.0\n#2  0x00005572f5f781cd in send_brigade_blocking (c=0x7f37a95ad9b0, bytes_written=0x7f37a95ade68, bb=0x7f37a95a1068, s=0x7f37a95a6c28) at core_filters.c:747\n#3  ap_core_output_filter (f=0x7f37a95add88, new_bb=0x7f37a95a1068) at core_filters.c:542\n#4  0x00007f37a58fac77 in ap_proxy_transfer_between_connections (r=r@entry=0x7f37a95a50a0, c_i=0x7f37a95ad290, c_o=0x7f37a95ad9b0, bb_i=<optimized out>, bb_o=0x7f37a95a1068, name=0x7f37a56e39dd \"client\", sent=0x0, \n    bsize=8192, after=1) at proxy_util.c:3836\n#5  0x00007f37a56e31c3 in proxy_connect_handler (r=0x7f37a95a50a0, worker=<optimized out>, conf=<optimized out>, url=<optimized out>, proxyname=<optimized out>, proxyport=<optimized out>) at mod_proxy_connect.c:421"}, {"count": 1, "tags": [], "bug_id": 61616, "attachment_id": 35422, "text": "Created attachment 35422\nrsync daemon config", "id": 201460, "time": "2017-10-14T10:10:43Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-14T10:10:43Z", "is_private": false}, {"count": 2, "tags": [], "bug_id": 61616, "attachment_id": 35423, "text": "Created attachment 35423\nscript for reproducing the bug", "id": 201461, "time": "2017-10-14T10:11:36Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-14T10:11:36Z", "is_private": false}, {"count": 3, "tags": [], "creator": "carsten@wolffcarsten.de", "attachment_id": null, "id": 201462, "time": "2017-10-14T17:49:16Z", "bug_id": 61616, "creation_time": "2017-10-14T17:49:16Z", "is_private": false, "text": "One more remark: I selected version 2.4.25 in the bug header, because I used that (debian) version for the reproducer. Effected versions are actually all versions, including 2.2 branch, 2.4 branch as well as trunk."}, {"count": 4, "tags": [], "creator": "ylavic.dev@gmail.com", "attachment_id": 35424, "id": 201463, "time": "2017-10-14T20:06:13Z", "bug_id": 61616, "creation_time": "2017-10-14T20:06:13Z", "is_private": false, "text": "Created attachment 35424\nHandle POLLOUT in proxy_connect\n\nCould you please try this patch on latest 2.4.x?"}, {"count": 5, "tags": [], "bug_id": 61616, "attachment_id": 35425, "text": "Created attachment 35425\nHandle POLLOUT in proxy_connect (v2)\n\nSmall update to the patch to keep POLLOUT until all data are out.\nPlease test this one instead.", "id": 201464, "time": "2017-10-14T20:31:49Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-10-14T20:31:49Z", "is_private": false}, {"count": 6, "tags": [], "bug_id": 61616, "attachment_id": null, "text": "First of all, thanks for the very quick turnaround!\n\nUnfortunately, the patch doesn't fix the problem. Actually, I have played with similar approaches for changing proxy_connect_handler() before, like doing a poll() for POLLOUT on the backend upon every POLLIN event from the client before actually calling ap_proxy_transfer_between_connections(), or like prefering the direction \"backend to client\", when both sockets issue POLLIN events.\nBut, no change to the poll() logic in mod_proxy_connect.c worked, because it's not there, where the poll() blocks. The whole situation can build up during one call of ap_proxy_transfer_between_connections(), when rsync daemon keeps sending long enough without reading, while rsync client is also sending. In that case apache will always block in apr/network_io/unix/sendrecv.c without a chance to return to mod_proxy again for another poll on the other socket.\nIs there a chance to parallelize the two data directions through threading or something?", "id": 201467, "time": "2017-10-15T06:17:44Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-15T06:17:44Z", "is_private": false}, {"count": 7, "attachment_id": 35426, "creator": "ylavic.dev@gmail.com", "is_private": false, "id": 201468, "time": "2017-10-15T08:30:55Z", "bug_id": 61616, "creation_time": "2017-10-15T08:30:55Z", "tags": [], "text": "Created attachment 35426\nHandle POLLOUT in proxy_connect (v3)\n\nWe have to stop reading on the other side when writing blocks on one side.\nHow does this new version work?"}, {"count": 8, "tags": [], "bug_id": 61616, "text": "(In reply to Yann Ylavic from comment #7)\n> We have to stop reading on the other side when writing blocks on one side.\n> How does this new version work?\n\nNot better, I'm afraid:\n\nThread 2 (Thread 0x7f534bfff700 (LWP 31402)):\n#0  0x00007f5354d316ad in poll () at ../sysdeps/unix/syscall-template.S:84\n#1  0x00007f5355232088 in apr_poll () from target:/usr/lib/x86_64-linux-gnu/libapr-1.so.0\n#2  0x000055df6ab6c17d in send_brigade_blocking (c=0x7f53559b6a98, bytes_written=0x7f53559b6fe0, bb=0x7f53559b3ee0, s=0x7f53559b3c48) at core_filters.c:747\n#3  ap_core_output_filter (f=0x7f53559b6e70, new_bb=0x7f53559b3ee0) at core_filters.c:542\n#4  0x00007f5351d60da7 in ap_proxy_transfer_between_connections (r=r@entry=0x7f53559b20a0, c_i=0x7f53559b6338, c_o=0x7f53559b6a98, bb_i=<optimized out>, bb_o=0x7f53559b3ee0, name=name@entry=0x7f5351b49cee \"client\", \n    sent=0x0, bsize=8192, after=1) at proxy_util.c:3839\n#5  0x00007f5351b495d9 in proxy_connect_handler (r=0x7f53559b20a0, worker=<optimized out>, conf=<optimized out>, url=<optimized out>, proxyname=<optimized out>, proxyport=<optimized out>) at mod_proxy_connect.c:468", "id": 201469, "time": "2017-10-15T10:04:52Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-15T10:04:52Z", "is_private": false, "attachment_id": null}, {"count": 9, "tags": [], "bug_id": 61616, "text": "Created attachment 35427\nFile read buffer and flush threshold tuning\n\nThanks for testing Carsten.\n\nI think I see what happens now, the issue is that whenever send_brigade_blocking() is entered it won't return before all the data are flushed, regardless of the remaining threshold.\n\nThis new patch is mostly unrelated (was mainly written some times ago to allow for better control of the outgoing traffic), but the changes in the core output filtering could address this case (see should_block() and how it's (re)used to bail out without blocking while we remain under the flush max threshold).\n\nI didn't committed this patch to trunk yet because it was made for 2.4.x, and some parts quite differ in trunk which makes the frontport non-trivial.\n\nCould you please test it and see if it helps for this case too?\nIt should be applied in addition to attachment 35426 (v3) because proxy_connect needs to do the right thing regarding POLLOUT still.", "id": 201471, "time": "2017-10-15T12:46:47Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-10-15T12:46:47Z", "is_private": false, "attachment_id": 35427}, {"count": 10, "attachment_id": null, "creator": "carsten@wolffcarsten.de", "is_private": false, "id": 201481, "time": "2017-10-16T11:14:32Z", "bug_id": 61616, "creation_time": "2017-10-16T11:14:32Z", "tags": [], "text": "(In reply to Yann Ylavic from comment #9)\n> Could you please test it and see if it helps for this case too?\n> It should be applied in addition to attachment 35426 [details] (v3) because\n> proxy_connect needs to do the right thing regarding POLLOUT still.\n\nI did now test these two patches together. The result is unchanged. strace and backtrace look the same, send_brigade_blocking() calls apr_poll() with a timeout of 300 seconds. After timeout it comes back with APR_TIMEUP, which results in a socket close.\nSo it seems, that should_flush() sets flush_upto to a true value and thus has no influence on the situation."}, {"count": 11, "attachment_id": null, "creator": "ylavic.dev@gmail.com", "is_private": false, "id": 201482, "time": "2017-10-16T11:29:21Z", "bug_id": 61616, "creation_time": "2017-10-16T11:29:21Z", "tags": [], "text": "Could you run the test with LogLevel trace6 and provide the error_log?"}, {"count": 12, "tags": [], "bug_id": 61616, "attachment_id": 35429, "id": 201484, "time": "2017-10-16T13:49:35Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-16T13:49:35Z", "is_private": false, "text": "Created attachment 35429\ntrace6 error log of a single test\n\n(In reply to Yann Ylavic from comment #11)\n> Could you run the test with LogLevel trace6 and provide the error_log?\n\nhere you are. It should be a clean log of just one test case"}, {"count": 13, "tags": [], "bug_id": 61616, "text": "Created attachment 35430\nHandle POLLOUT in proxy_connect (v4)\n\nThanks for the traces, I missed that proxy_connect was FLUSHing data.\n\nHow about this v4 (to replace attachment 35426 (v3), in addition to attachment 35427 still)?", "id": 201488, "time": "2017-10-17T08:42:06Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-10-17T08:42:06Z", "is_private": false, "attachment_id": 35430}, {"count": 14, "tags": [], "text": "Created attachment 35431\nshortening the poll timeout for send_brigade_blocking\n\nI tried this patch on top of your patches, but unfortunately it just leads to the connection loss happening more quickly:\n\n[Tue Oct 17 09:08:16.947301 2017] [core:trace6] [pid 13571:tid 140023972112128] core_filters.c(878): (11)Resource temporarily unavailable: [client ::1:44128] writev_nonblocking: 62656/65536 bytes\n[Tue Oct 17 09:08:16.947332 2017] [core:trace6] [pid 13571:tid 140023972112128] core_filters.c(407): [client ::1:44128] core_output_filter: flushing because of max threshold\n[Tue Oct 17 09:08:16.947339 2017] [core:trace6] [pid 13571:tid 140023972112128] core_filters.c(878): (11)Resource temporarily unavailable: [client ::1:44128] writev_nonblocking: 0/68416 bytes\n[Tue Oct 17 09:08:16.947344 2017] [core:trace6] [pid 13571:tid 140023972112128] core_filters.c(407): [client ::1:44128] core_output_filter: flushing because of max threshold\n[Tue Oct 17 09:08:16.947350 2017] [core:trace1] [pid 13571:tid 140023972112128] core_filters.c(560): (70007)The timeout specified has expired: [client ::1:44128] core_output_filter: writing data to the network\n[Tue Oct 17 09:08:16.947356 2017] [proxy:error] [pid 13571:tid 140023972112128] (70007)The timeout specified has expired: [client ::1:44128] AH03307: ap_proxy_transfer_between_connections: error on backend - ap_pass_brigade\n[Tue Oct 17 09:08:16.947362 2017] [proxy:trace2] [pid 13571:tid 140023972112128] proxy_util.c(3843): (70007)The timeout specified has expired: [client ::1:44128] ap_proxy_transfer_between_connections complete\n[Tue Oct 17 09:08:16.947368 2017] [proxy_connect:trace2] [pid 13571:tid 140023972112128] mod_proxy_connect.c(532): [client ::1:44128] finished with poll() - cleaning up\n[Tue Oct 17 09:08:16.947390 2017] [core:trace6] [pid 13571:tid 140023972112128] core_filters.c(418): [client ::1:873] core_output_filter: flushing because of FLUSH bucket", "attachment_id": 35431, "id": 201490, "creator": "carsten@wolffcarsten.de", "time": "2017-10-17T08:46:03Z", "bug_id": 61616, "creation_time": "2017-10-17T08:46:03Z", "is_private": false}, {"count": 15, "tags": [], "bug_id": 61616, "attachment_id": 35432, "text": "Created attachment 35432\nHandle POLLOUT in proxy_connect (v5)\n\nPlease use v5 instead, v4 can't work before it does not play well with EAGAIN.", "id": 201491, "time": "2017-10-17T09:07:40Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-10-17T09:07:40Z", "is_private": false}, {"count": 16, "attachment_id": null, "creator": "carsten@wolffcarsten.de", "is_private": false, "id": 201492, "time": "2017-10-17T11:06:33Z", "bug_id": 61616, "creation_time": "2017-10-17T11:06:33Z", "tags": [], "text": "(In reply to Yann Ylavic from comment #15)\n> Created attachment 35432 [details]\n> Handle POLLOUT in proxy_connect (v5)\n> \n> Please use v5 instead, v4 can't work before it does not play well with\n> EAGAIN.\n\nWith v5 + \"File read buffer and flush threshold tuning\", proxy_connect gets stuck very early on:\n\n\n\n[Tue Oct 17 12:55:53.597014 2017] [core:trace5] [pid 9813:tid 140438873556736] protocol.c(645): [client ::1:44136] Request received from client: CONNECT localhost:873 HTTP/1.0\n[Tue Oct 17 12:55:53.597118 2017] [http:trace4] [pid 9813:tid 140438873556736] http_request.c(433): [client ::1:44136] Headers received from client:\n[Tue Oct 17 12:55:53.597268 2017] [authz_core:debug] [pid 9813:tid 140438873556736] mod_authz_core.c(809): [client ::1:44136] AH01626: authorization result of Require host localhost: granted\n[Tue Oct 17 12:55:53.597278 2017] [authz_core:debug] [pid 9813:tid 140438873556736] mod_authz_core.c(809): [client ::1:44136] AH01626: authorization result of <RequireAny>: granted\n[Tue Oct 17 12:55:53.597282 2017] [core:trace3] [pid 9813:tid 140438873556736] request.c(304): [client ::1:44136] request authorized without authentication by access_checker_ex hook: localhost:873\n[Tue Oct 17 12:55:53.597291 2017] [proxy_connect:trace1] [pid 9813:tid 140438873556736] mod_proxy_connect.c(141): [client ::1:44136] canonicalising URL localhost:873\n[Tue Oct 17 12:55:53.597312 2017] [proxy:trace2] [pid 9813:tid 140438873556736] proxy_util.c(1970): [client ::1:44136] *: found forward proxy worker for localhost:873\n[Tue Oct 17 12:55:53.597318 2017] [proxy:debug] [pid 9813:tid 140438873556736] mod_proxy.c(1227): [client ::1:44136] AH01143: Running scheme localhost handler (attempt 0)\n[Tue Oct 17 12:55:53.597322 2017] [proxy_connect:trace1] [pid 9813:tid 140438873556736] mod_proxy_connect.c(190): [client ::1:44136] serving URL localhost:873\n[Tue Oct 17 12:55:53.597329 2017] [proxy_connect:debug] [pid 9813:tid 140438873556736] mod_proxy_connect.c(207): [client ::1:44136] AH01019: connecting localhost:873 to localhost:873\n[Tue Oct 17 12:55:53.597372 2017] [proxy_connect:trace1] [pid 9813:tid 140438873556736] mod_proxy_connect.c(233): [client ::1:44136] connecting to remote proxy localhost on port 873\n[Tue Oct 17 12:55:53.597384 2017] [proxy:trace2] [pid 9813:tid 140438873556736] proxy_util.c(2078): [client ::1:44136] CONNECT: fam 10 socket created to connect to localhost\n[Tue Oct 17 12:55:53.597466 2017] [proxy_connect:trace3] [pid 9813:tid 140438873556736] mod_proxy_connect.c(311): [client ::1:44136] connection complete to [::1]:873 (localhost)\n[Tue Oct 17 12:55:53.597483 2017] [proxy_connect:trace1] [pid 9813:tid 140438873556736] mod_proxy_connect.c(333): [client ::1:44136] Returning 200 OK\n[Tue Oct 17 12:55:53.597493 2017] [core:trace6] [pid 9813:tid 140438873556736] core_filters.c(418): [client ::1:44136] core_output_filter: flushing because of FLUSH bucket\n[Tue Oct 17 12:55:53.597522 2017] [core:trace6] [pid 9813:tid 140438873556736] core_filters.c(878): [client ::1:44136] writev_nonblocking: 76/76 bytes\n[Tue Oct 17 12:55:53.597522 2017] [proxy_connect:trace2] [pid 9813:tid 140438873556736] mod_proxy_connect.c(356): [client ::1:44136] setting up poll()\n[Tue Oct 17 12:55:53.597651 2017] [proxy:trace2] [pid 9813:tid 140438873556736] proxy_util.c(3844): [client ::1:44136] ap_proxy_transfer_between_connections complete\n[Tue Oct 17 12:55:53.980879 2017] [proxy:debug] [pid 13857:tid 140438968014016] proxy_util.c(1779): AH00925: initializing worker proxy:forward shared\n[Tue Oct 17 12:55:53.980922 2017] [proxy:debug] [pid 13857:tid 140438968014016] proxy_util.c(1821): AH00927: initializing worker proxy:forward local\n[Tue Oct 17 12:55:53.980933 2017] [proxy:debug] [pid 13857:tid 140438968014016] proxy_util.c(1856): AH00930: initialized pool in child 13857 for (*) min=0 max=25 smax=25\n\n\n\nThread 2 (Thread 0x7fba792bc700 (LWP 9815)):\n#0  0x00007fba7df1b0f3 in epoll_wait () at ../sysdeps/unix/syscall-template.S:84\n#1  0x00007fba7e411201 in impl_pollset_poll (pollset=0x7fba7eb93f78, timeout=<optimized out>, num=0x7fba792b9aa4, descriptors=0x7fba792b9ab0) at ./poll/unix/epoll.c:266\n#2  0x00007fba7ad29198 in proxy_connect_handler (r=0x7fba7eb920a0, worker=<optimized out>, conf=<optimized out>, url=<optimized out>, proxyname=<optimized out>, proxyport=<optimized out>) at mod_proxy_connect.c:401\n\n\n\n# netstat -pnt | grep -v :22\nActive Internet connections (w/o servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp6      14      0 ::1:35806               ::1:873                 ESTABLISHED 9813/apache2        \ntcp6       0      0 ::1:44136               ::1:80                  ESTABLISHED 13855/rsync         \ntcp6       0      0 ::1:873                 ::1:35806               ESTABLISHED 13856/rsync         \ntcp6       0      0 ::1:80                  ::1:44136               ESTABLISHED 9813/apache2"}, {"count": 17, "tags": [], "bug_id": 61616, "text": "Created attachment 35442\nsuppress poll with long timeout in filter code\n\nHi again,\n\nthis patch, ontop of https://bz.apache.org/bugzilla/attachment.cgi?id=35427 and https://bz.apache.org/bugzilla/attachment.cgi?id=35432 fixes it for me (tm).\n\nI'm not sure, if this is an acceptable way of doing it for you and I am sure the patch needs cleanup, but I wanted to let you know immediately, since I have no time for the rest of the day.", "id": 201542, "time": "2017-10-19T12:35:48Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-19T12:35:48Z", "is_private": false, "attachment_id": 35442}, {"count": 18, "tags": [], "bug_id": 61616, "text": "Created attachment 35443\nsuppress poll with long timeout in filter code (v2)\n\nhere is a cleanup of the patch, I hope this is better to read", "id": 201544, "time": "2017-10-19T15:28:18Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-19T15:28:18Z", "is_private": false, "attachment_id": 35443}, {"count": 19, "tags": [], "bug_id": 61616, "text": "Thanks for the patch Carsten, but there can't be an exception for send_brigade_blocking() to block when the conditions are met, the core output filter must control the memory buckets in flight (set aside).\n\nPlease see new proposal below.", "id": 201557, "time": "2017-10-21T12:47:39Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-10-21T12:47:39Z", "is_private": false, "attachment_id": null}, {"count": 20, "tags": [], "bug_id": 61616, "text": "Created attachment 35446\nHandle POLLOUT in proxy_connect (v6)\n\nThis new patch gives more control (flags) to the caller of ap_proxy_transfer_between_connections(), which can now stop its loop when the output filter chain starts buffering data.\n\nIt also refactors proxy_connect_handler() to have a symetric loop because the logic had to be duplicated on both sides (the more we add logic, the less it becomes readable).\n\nI made more testing with it and it seems to work as expected here..\n\nI also have an update for attachment 35427 but this patch is actually barely related (the above patch should work on its own for the proxy connect case).\n\nI can provide it here too if you wish because it allows to transfer more than 8K bytes at a time (a current limitation in mod_proxy_connect).", "id": 201558, "time": "2017-10-21T13:00:35Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-10-21T13:00:35Z", "is_private": false, "attachment_id": 35446}, {"count": 21, "attachment_id": null, "creator": "carsten@wolffcarsten.de", "is_private": false, "id": 201559, "time": "2017-10-21T14:39:45Z", "bug_id": 61616, "creation_time": "2017-10-21T14:39:45Z", "tags": [], "text": "(In reply to Yann Ylavic from comment #19)\n> Thanks for the patch Carsten, but there can't be an exception for\n> send_brigade_blocking() to block when the conditions are met, the core\n> output filter must control the memory buckets in flight (set aside).\n\nSure, using the timeout value as a criterion in send_brigade_blocking is ugly. I see there's a APR_NONBLOCK_READ flag one can pass to ap_get_brigade(). Maybe there should be a APR_NONBLOCK_WRITE flag one could pass to ap_pass_brigade() (and further down to send_brigade_blocking) to absolutely make sure, the poll() there is never hit?\n\n(In reply to Yann Ylavic from comment #20)\n> Created attachment 35446 [details]\n> Handle POLLOUT in proxy_connect (v6)\n> \n> This new patch gives more control (flags) to the caller of\n> ap_proxy_transfer_between_connections(), which can now stop its loop when\n> the output filter chain starts buffering data.\n>\n> It also refactors proxy_connect_handler() to have a symetric loop because\n> the logic had to be duplicated on both sides (the more we add logic, the\n> less it becomes readable).\n\nMuch nicer indeed, you also choose the right socket on POLLOUT now, which was wrong in v5.\n\n> I made more testing with it and it seems to work as expected here..\n\nIt gives me a hard time to reproduce the original issue. :-) But, it now blocks at the end of the transfer in mod_proxy_connect's poll() for 300 seconds before finishing successfully. See backtrace and log below.\n\n> I also have an update for attachment 35427 [details] but this patch is\n> actually barely related (the above patch should work on its own for the\n> proxy connect case).\n> \n> I can provide it here too if you wish because it allows to transfer more\n> than 8K bytes at a time (a current limitation in mod_proxy_connect).\n\nThis might be interesting for performance of connections on loopback, where the segment size is 64k.\n\nCurrent backtrace:\nThread 2 (Thread 0x7ffa5f0b9700 (LWP 5797)):\n#0  0x00007ffa63d180f3 in epoll_wait () at ../sysdeps/unix/syscall-template.S:84\n#1  0x00007ffa6420e201 in impl_pollset_poll (pollset=0x7ffa64990f78, timeout=<optimized out>, num=0x7ffa5f0b6a94, descriptors=0x7ffa5f0b6aa0) at ./poll/unix/epoll.c:266\n#2  0x00007ffa60b263b7 in proxy_connect_handler (r=0x7ffa6498f0a0, worker=<optimized out>, conf=<optimized out>, url=<optimized out>, proxyname=<optimized out>, \n    proxyport=<optimized out>) at mod_proxy_connect.c:442\n\n\nLog tail of the end of the transfer:\n[Sat Oct 21 16:28:29.475904 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:28:29.476140 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [client ::1:33340] writev_nonblocking: 8192/8192 bytes\n[Sat Oct 21 16:28:29.476241 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:28:29.476510 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [client ::1:33340] writev_nonblocking: 8192/8192 bytes\n[Sat Oct 21 16:28:29.476685 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:28:29.476985 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [client ::1:33340] writev_nonblocking: 8192/8192 bytes\n[Sat Oct 21 16:33:46.176810 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.178932 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.179110 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [remote ::1:873] writev_nonblocking: 4/4 bytes\n[Sat Oct 21 16:33:46.186064 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [client ::1:33340] writev_nonblocking: 4272/4272 bytes\n[Sat Oct 21 16:33:46.199852 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.200032 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [remote ::1:873] writev_nonblocking: 5/5 bytes\n[Sat Oct 21 16:33:46.200317 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.200365 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [client ::1:33340] writev_nonblocking: 5/5 bytes\n[Sat Oct 21 16:33:46.200793 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.200898 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [remote ::1:873] writev_nonblocking: 7/7 bytes\n[Sat Oct 21 16:33:46.200920 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.200952 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [client ::1:33340] writev_nonblocking: 6/6 bytes\n[Sat Oct 21 16:33:46.242005 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.242141 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [client ::1:33340] writev_nonblocking: 25/25 bytes\n[Sat Oct 21 16:33:46.242524 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.244044 2017] [core:trace6] [pid 5795:tid 140713313146624] core_filters.c(878): [remote ::1:873] writev_nonblocking: 5/5 bytes\n[Sat Oct 21 16:33:46.256132 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.256195 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): (70014)End of file found: [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.256236 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.256245 2017] [proxy:trace2] [pid 5795:tid 140713313146624] proxy_util.c(3845): (70014)End of file found: [client ::1:33340] ap_proxy_transfer_between_connections complete\n[Sat Oct 21 16:33:46.256368 2017] [proxy_connect:trace2] [pid 5795:tid 140713313146624] mod_proxy_connect.c(558): [client ::1:33340] finished with poll() - cleaning up"}, {"count": 22, "attachment_id": null, "creator": "ylavic.dev@gmail.com", "text": "(In reply to Carsten Wolff from comment #21)\n> \n> Maybe there should be a APR_NONBLOCK_WRITE flag one could\n> pass to ap_pass_brigade() (and further down to send_brigade_blocking) to\n> absolutely make sure, the poll() there is never hit?\n\nWhen the poll() is hit in send_brigade_blocking() it means the core output filter ought to block, because we don't want unbounded buffering.\n\nSuppose a handler keeps sending data out, when the socket becomes full we must stop buffering soon or later (i.e. above THRESHOLD_MAX_BUFFER or something configurable which is the point of attachment 35427, among other parameters).\n\nOtherwise it's unbounded memory usage.", "id": 201562, "time": "2017-10-22T12:22:57Z", "bug_id": 61616, "creation_time": "2017-10-22T12:22:57Z", "tags": [], "is_private": false}, {"count": 23, "tags": [], "bug_id": 61616, "attachment_id": null, "text": "(In reply to Carsten Wolff from comment #21)\n> \n> It gives me a hard time to reproduce the original issue. :-) But, it now\n> blocks at the end of the transfer in mod_proxy_connect's poll() for 300\n> seconds before finishing successfully. See backtrace and log below.\n\nIt looks like the socket is not really closed (before timeout) on the one or other end (proxy_connect won't close on its own).\n\nWhat's the output if you force \"#define DEBUGGING\" somewhere before proxy_connect_handler() in mod_proxy_connect.c?", "id": 201563, "time": "2017-10-22T12:27:36Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-10-22T12:27:36Z", "is_private": false}, {"count": 24, "tags": [], "creator": "ylavic.dev@gmail.com", "text": "Created attachment 35447\nRead buffer size and flush threshold tuning\n\nAllows to configure ReadBufferSize, FlushMinThreshold, FlushMaxThreshold and FlushMaxPipelined (instead of hard coded values).\n\nFor your proxy_connect case where large files may be rsync-ed, something like the below looks not absurd:\n\n# Read more at once than the default 8K\nReadBufferSize 65536\n# Don't block below this (i.e. more than ReadBufferSize)\nFlushMaxThreshold 131072\n# Always flush (non-blocking) when asked to write (i.e. don't retain data)\nFlushMinThreshold 1", "id": 201564, "time": "2017-10-22T12:52:02Z", "bug_id": 61616, "creation_time": "2017-10-22T12:52:02Z", "is_private": false, "attachment_id": 35447}, {"count": 25, "attachment_id": null, "creator": "ylavic.dev@gmail.com", "is_private": false, "id": 201565, "time": "2017-10-22T13:18:44Z", "bug_id": 61616, "creation_time": "2017-10-22T13:18:44Z", "tags": [], "text": "(In reply to Yann Ylavic from comment #24)\n> Created attachment 35447 [details]\n> Read buffer size and flush threshold tuning\n\nAlso not note that send_brigade_blocking() is now a bit misnamed, it's rather send_brigade_maybe_blocking() since it first attemps to write non-blocking in any case and that might prevent the poll() if !should_block() afterward..."}, {"count": 26, "tags": [], "bug_id": 61616, "text": "I added this just at the end of the poll() loop:\n        ap_log_rerror(APLOG_MARK, APLOG_TRACE2, 0, r,\n            \"@END: \"\n            \"[%s: POLLIN: %d POLLOUT: %d POLLHUP: %d data_in_filters: %d] \"\n            \"[%s: POLLIN: %d POLLOUT: %d POLLHUP: %d data_in_filters: %d]\",\n            conns[0].name,\n            conns[0].pfd.reqevents & APR_POLLIN && 1,\n            conns[0].pfd.reqevents & APR_POLLOUT && 1,\n            conns[0].pfd.reqevents & APR_POLLHUP && 1,\n            conns[0].c->data_in_output_filters,\n            conns[1].name,\n            conns[1].pfd.reqevents & APR_POLLIN && 1,\n            conns[1].pfd.reqevents & APR_POLLOUT && 1,\n            conns[1].pfd.reqevents & APR_POLLHUP && 1,\n            conns[1].c->data_in_output_filters\n        );\n\n\nThe whole long log looks like this sample:\n\n[Sun Oct 22 21:31:00.885835 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(452): [client ::1:33400] AH01024: woke from poll(), i=1\n[Sun Oct 22 21:31:00.885891 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(483): [client ::1:33400] AH01025: client was readable\n[Sun Oct 22 21:31:00.885946 2017] [proxy:trace2] [pid 26472:tid 140206004721408] proxy_util.c(3845): [client ::1:33400] ap_proxy_transfer_between_connections complete\n[Sun Oct 22 21:31:00.885960 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(517): [client ::1:33400] backend wait writable\n[Sun Oct 22 21:31:00.885978 2017] [proxy_connect:trace2] [pid 26472:tid 140206004721408] mod_proxy_connect.c(569): [client ::1:33400] @END: [client: POLLIN: 0 POLLOUT: 0 POLLHUP: 0 data_in_filters: 0] [backend: POLLIN: 1 POLLOUT: 1 POLLHUP: 0 data_in_filters: 1]\n[Sun Oct 22 21:31:00.885992 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(452): [client ::1:33400] AH01024: woke from poll(), i=1\n[Sun Oct 22 21:31:00.886002 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(528): [client ::1:33400] backend was writable\n[Sun Oct 22 21:31:00.886068 2017] [core:trace6] [pid 26472:tid 140206004721408] core_filters.c(878): [remote ::1:873] writev_nonblocking: 8/8 bytes\n[Sun Oct 22 21:31:00.886108 2017] [proxy_connect:trace2] [pid 26472:tid 140206004721408] mod_proxy_connect.c(569): [client ::1:33400] @END: [client: POLLIN: 1 POLLOUT: 0 POLLHUP: 0 data_in_filters: 0] [backend: POLLIN: 1 POLLOUT: 0 POLLHUP: 0 data_in_filters: 0]\n[Sun Oct 22 21:31:00.888170 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(452): [client ::1:33400] AH01024: woke from poll(), i=1\n[Sun Oct 22 21:31:00.888229 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(483): [client ::1:33400] AH01025: backend was readable\n[Sun Oct 22 21:31:00.888271 2017] [proxy:trace2] [pid 26472:tid 140206004721408] proxy_util.c(3845): [client ::1:33400] ap_proxy_transfer_between_connections complete\n[Sun Oct 22 21:31:00.888307 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(517): [client ::1:33400] client wait writable\n[Sun Oct 22 21:31:00.888329 2017] [proxy_connect:trace2] [pid 26472:tid 140206004721408] mod_proxy_connect.c(569): [client ::1:33400] @END: [client: POLLIN: 1 POLLOUT: 1 POLLHUP: 0 data_in_filters: 1] [backend: POLLIN: 0 POLLOUT: 0 POLLHUP: 0 data_in_filters: 0]\n[Sun Oct 22 21:31:00.888344 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(452): [client ::1:33400] AH01024: woke from poll(), i=1\n[Sun Oct 22 21:31:00.888354 2017] [proxy_connect:debug] [pid 26472:tid 140206004721408] mod_proxy_connect.c(528): [client ::1:33400] client was writable\n[Sun Oct 22 21:31:00.888450 2017] [core:trace6] [pid 26472:tid 140206004721408] core_filters.c(878): [client ::1:33400] writev_nonblocking: 12/12 bytes\n[Sun Oct 22 21:31:00.888492 2017] [proxy_connect:trace2] [pid 26472:tid 140206004721408] mod_proxy_connect.c(569): [client ::1:33400] @END: [client: POLLIN: 1 POLLOUT: 0 POLLHUP: 0 data_in_filters: 0] [backend: POLLIN: 1 POLLOUT: 0 POLLHUP: 0 data_in_filters: 0]\n\n\nTwo things are peculiar:\n- we no longer request APR_POLLHUP events. Is that intentional?\n- It seems like when ap_proxy_transfer_between_connections returns, \"data_in_output_filters\" is always true on the outgoing connection and we're always entering the \"write wait\" path.", "id": 201576, "time": "2017-10-22T19:41:39Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-10-22T19:41:39Z", "is_private": false, "attachment_id": null}, {"count": 27, "attachment_id": null, "creator": "ylavic.dev@gmail.com", "text": "The logs from comment 21 and comment 26 look normal, no timeout triggered, proxy_connect_handler() seems to be waiting for sockets to be ready to do its job.\n\n(In reply to Carsten Wolff from comment #26)\n> I added this just at the end of the poll() loop:\n>         ap_log_rerror(APLOG_MARK, APLOG_TRACE2, 0, r,\n>             \"@END: \"\n>             \"[%s: POLLIN: %d POLLOUT: %d POLLHUP: %d data_in_filters: %d] \"\n>             \"[%s: POLLIN: %d POLLOUT: %d POLLHUP: %d data_in_filters: %d]\",\n>             conns[0].name,\n>             conns[0].pfd.reqevents & APR_POLLIN && 1,\n>             conns[0].pfd.reqevents & APR_POLLOUT && 1,\n>             conns[0].pfd.reqevents & APR_POLLHUP && 1,\n>             conns[0].c->data_in_output_filters,\n>             conns[1].name,\n>             conns[1].pfd.reqevents & APR_POLLIN && 1,\n>             conns[1].pfd.reqevents & APR_POLLOUT && 1,\n>             conns[1].pfd.reqevents & APR_POLLHUP && 1,\n>             conns[1].c->data_in_output_filters\n>         );\n> \n> \n> The whole long log looks like this sample:\n\nEvents are fine here.\n\n> \n> Two things are peculiar:\n> - we no longer request APR_POLLHUP events. Is that intentional?\n\nPOLLHUP is a returned event only (ignored when requested usually), though APR might maybe require it when pollset is implemented with select() (will loook at this), it shouldn't be the case on Linux hence for your testing...\n\n\n> - It seems like when ap_proxy_transfer_between_connections returns,\n> \"data_in_output_filters\" is always true on the outgoing connection and we're\n> always entering the \"write wait\" path.\n\nYes, ap_proxy_transfer_between_connections() used to forward everything it reads until EAGAIN (non-blocking on read only), that's why the core output filter ended up blocking when the outgoing socket didn't follow the incoming rate.\nI changed this in attachment 35446 with a flag (AP_PROXY_TRANSFER_UNBUFFERED) to tell ap_proxy_transfer_between_connections() to stop when buffering happens in ouput too (i.e. data_in_output_filters), such that in this case mod_proxy_connect can use POLLOUT before trying again.", "id": 201590, "time": "2017-10-23T12:40:09Z", "bug_id": 61616, "creation_time": "2017-10-23T12:40:09Z", "tags": [], "is_private": false}, {"count": 28, "tags": [], "creator": "carsten@wolffcarsten.de", "text": "Thanks Yann for your patience and all your work! I have some last observations to share:\n\n- When combining https://bz.apache.org/bugzilla/attachment.cgi?id=35427 and https://bz.apache.org/bugzilla/attachment.cgi?id=35446, I still get frequent stalls at the end of the transfer. Also, I observed a higher CPU usage by apache compared to e.g. apache-2.4.27 release. During transfer, the apache process used ~50% of one core in my test setup. For reference, I also ran the same test with squid3 as a connect proxy and it used ~9% of one core.\n\n- When combining https://bz.apache.org/bugzilla/attachment.cgi?id=35447 and https://bz.apache.org/bugzilla/attachment.cgi?id=35446, I can no longer produce any stall during or at the end of the transfer. Also, the CPU usage by apache is reduced to ~14%\n\n- When combining https://bz.apache.org/bugzilla/attachment.cgi?id=35447 and https://bz.apache.org/bugzilla/attachment.cgi?id=35446 and applying the settings you suggested in comment#24, everything still works and the CPU usage drops to ~8%.\n\nI will now have to run tests on my target platform, which is a bit more complicated, so it will take a few days, but I suppose we can assume this bug is fixed now. Thanks again!", "id": 201643, "time": "2017-10-24T18:37:12Z", "bug_id": 61616, "creation_time": "2017-10-24T18:37:12Z", "is_private": false, "attachment_id": null}, {"count": 29, "tags": [], "bug_id": 61616, "attachment_id": null, "text": "Hi, unfortunately, using patch#35446 and patch#35447, there is another problem, when I change the scenario by making the client connection through SSL. In that case, the transfer stalls very quickly within the proxy_connect poll loop:\n\n\n[Thu Nov 09 20:34:28.029653 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(246): [client 127.0.0.1:56996] AH01019: connecting localhost:873 to localhost:873\n[Thu Nov 09 20:34:28.029745 2017] [proxy_connect:trace1] [pid 19603:tid 139754421921536] mod_proxy_connect.c(272): [client 127.0.0.1:56996] connecting to remote proxy localhost on port 873\n[Thu Nov 09 20:34:28.029776 2017] [proxy:trace2] [pid 19603:tid 139754421921536] proxy_util.c(2080): [client 127.0.0.1:56996] CONNECT: fam 10 socket created to connect to localhost\n[Thu Nov 09 20:34:28.029877 2017] [proxy_connect:trace3] [pid 19603:tid 139754421921536] mod_proxy_connect.c(355): [client 127.0.0.1:56996] connection complete to [::1]:873 (localhost)\n[Thu Nov 09 20:34:28.029905 2017] [proxy_connect:trace1] [pid 19603:tid 139754421921536] mod_proxy_connect.c(377): [client 127.0.0.1:56996] Returning 200 OK\n[Thu Nov 09 20:34:28.029921 2017] [ssl:trace6] [pid 19603:tid 139754421921536] ssl_engine_io.c(859): [client 127.0.0.1:56996] ssl_filter_write: 76 bytes\n[Thu Nov 09 20:34:28.029946 2017] [ssl:trace6] [pid 19603:tid 139754421921536] ssl_engine_io.c(219): [client 127.0.0.1:56996] bio_filter_out_write: 105 bytes\n[Thu Nov 09 20:34:28.029967 2017] [ssl:trace4] [pid 19603:tid 139754421921536] ssl_engine_io.c(2220): [client 127.0.0.1:56996] OpenSSL: write 105/105 bytes to BIO#7f1b040021b0 [mem: 7f1b040025a3] (BIO dump follows)\n[Thu Nov 09 20:34:28.029982 2017] [core:trace6] [pid 19603:tid 139754421921536] core_filters.c(458): [client 127.0.0.1:56996] core_output_filter: flushing because of FLUSH bucket\n[Thu Nov 09 20:34:28.030012 2017] [core:trace6] [pid 19603:tid 139754421921536] core_filters.c(857): [client 127.0.0.1:56996] writev_nonblocking: 105/105 bytes\n[Thu Nov 09 20:34:28.030027 2017] [proxy_connect:trace2] [pid 19603:tid 139754421921536] mod_proxy_connect.c(399): [client 127.0.0.1:56996] setting up poll()\n[Thu Nov 09 20:34:28.032931 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(454): [client 127.0.0.1:56996] AH01024: woke from poll(), i=1\n[Thu Nov 09 20:34:28.032971 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(485): [client 127.0.0.1:56996] AH01025: client was readable\n[Thu Nov 09 20:34:28.033024 2017] [ssl:trace4] [pid 19603:tid 139754421921536] ssl_engine_io.c(2220): [client 127.0.0.1:56996] OpenSSL: read 5/5 bytes from BIO#7f1b04002270 [mem: 7f1b040025a3] (BIO dump follows)\n[Thu Nov 09 20:34:28.033041 2017] [ssl:trace4] [pid 19603:tid 139754421921536] ssl_engine_io.c(2220): [client 127.0.0.1:56996] OpenSSL: read 38/38 bytes from BIO#7f1b04002270 [mem: 7f1b040025a8] (BIO dump follows)\n[Thu Nov 09 20:34:28.033060 2017] [proxy:debug] [pid 19603:tid 139754421921536] proxy_util.c(3803): [client 127.0.0.1:56996] AH03306: ap_proxy_transfer_between_connections: read 14 bytes from client\n[Thu Nov 09 20:34:28.033089 2017] [proxy:trace2] [pid 19603:tid 139754421921536] proxy_util.c(3847): [client 127.0.0.1:56996] ap_proxy_transfer_between_connections complete\n[Thu Nov 09 20:34:28.033098 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(519): [client 127.0.0.1:56996] backend wait writable\n[Thu Nov 09 20:34:28.033112 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(454): [client 127.0.0.1:56996] AH01024: woke from poll(), i=1\n[Thu Nov 09 20:34:28.033120 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(530): [client 127.0.0.1:56996] backend was writable\n[Thu Nov 09 20:34:28.033159 2017] [core:trace6] [pid 19603:tid 139754421921536] core_filters.c(857): [remote ::1:873] writev_nonblocking: 14/14 bytes\n[Thu Nov 09 20:34:28.033755 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(454): [client 127.0.0.1:56996] AH01024: woke from poll(), i=1\n[Thu Nov 09 20:34:28.033782 2017] [proxy_connect:debug] [pid 19603:tid 139754421921536] mod_proxy_connect.c(485): [client 127.0.0.1:56996] AH01025: backend was readable\n[Thu Nov 09 20:34:28.033817 2017] [proxy:debug] [pid 19603:tid 139754421921536] proxy_util.c(3803): [client 127.0.0.1:56996] AH03306: ap_proxy_transfer_between_connections: read 14 bytes from backend\n[Thu Nov 09 20:34:28.033835 2017] [ssl:trace4] [pid 19603:tid 139754421921536] ssl_engine_io.c(1674): [client 127.0.0.1:56996] coalesce: have 0 bytes, adding 14 more\n[Thu Nov 09 20:34:28.033848 2017] [proxy:trace2] [pid 19603:tid 139754421921536] proxy_util.c(3847): [client 127.0.0.1:56996] ap_proxy_transfer_between_connections complete\n[Thu Nov 09 20:34:28.582512 2017] [proxy:debug] [pid 19638:tid 139754523858112] proxy_util.c(1781): AH00925: initializing worker proxy:forward shared\n[Thu Nov 09 20:34:28.582666 2017] [proxy:debug] [pid 19638:tid 139754523858112] proxy_util.c(1823): AH00927: initializing worker proxy:forward local\n[Thu Nov 09 20:34:28.582706 2017] [proxy:debug] [pid 19638:tid 139754523858112] proxy_util.c(1858): AH00930: initialized pool in child 19638 for (*) min=0 max=25 smax=25\n[Thu Nov 09 20:34:28.582732 2017] [proxy:debug] [pid 19638:tid 139754523858112] proxy_util.c(1781): AH00925: initializing worker proxy:forward shared\n[Thu Nov 09 20:34:28.582746 2017] [proxy:debug] [pid 19638:tid 139754523858112] proxy_util.c(1823): AH00927: initializing worker proxy:forward local\n[Thu Nov 09 20:34:28.582772 2017] [proxy:debug] [pid 19638:tid 139754523858112] proxy_util.c(1858): AH00930: initialized pool in child 19638 for (*) min=0 max=25 smax=25\n[Thu Nov 09 20:39:28.134117 2017] [proxy_connect:error] [pid 19603:tid 139754421921536] (70007)The timeout specified has expired: [client 127.0.0.1:56996] AH01023: polling\n[Thu Nov 09 20:39:28.134322 2017] [proxy_connect:trace2] [pid 19603:tid 139754421921536] mod_proxy_connect.c(560): [client 127.0.0.1:56996] finished with poll() - cleaning up\n[Thu Nov 09 20:39:28.134870 2017] [ssl:trace4] [pid 19603:tid 139754421921536] ssl_engine_io.c(1734): [client 127.0.0.1:56996] coalesce: passing on 14 bytes", "id": 202027, "time": "2017-11-09T19:45:50Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-11-09T19:45:50Z", "is_private": false}, {"count": 30, "tags": [], "bug_id": 61616, "attachment_id": null, "text": "(In reply to Carsten Wolff from comment #29)\n> Hi, unfortunately, using patch#35446 and patch#35447, there is another\n> problem, when I change the scenario by making the client connection through\n> SSL. In that case, the transfer stalls very quickly within the proxy_connect\n> poll loop\n\nWhen I comment the coalescing filter in ssl_io_filter_init(), it works again. So it seems we need a way for mod_proxy_connect to cause mod_ssl to not use that coalescing filter. Is there an accepted way for passing flags from module to module or would this need to go through the core code?", "id": 202030, "time": "2017-11-10T06:43:23Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-11-10T06:43:23Z", "is_private": false}, {"count": 31, "tags": [], "bug_id": 61616, "attachment_id": 35512, "text": "Created attachment 35512\nHandle POLLOUT in proxy_connect (v7)\n\nGood catch, it makes no sense to try to coalesce TCP packets!\nThis v7 simply removes the mod_ssl coalescing filter from the chain before entering the mod_proxy_connect's loop.\n\nThe change between v6 and v7 lokks like this:\n\n+     /* we are now acting as a tunnel - the input/output filter stacks should\n+-     * not contain any non-connection filters.\n++     * not contain any non-connection or coalescing filters.\n+      */\n++    ap_remove_output_filter_byhandle(c->output_filters,\n++                                     \"SSL/TLS Coalescing Filter\");\n+     r->output_filters = c->output_filters;\n\nWe'll probably want to use a helper from mod_ssl for this in the final patch, and avoid the magic name, but for now it should work..", "id": 202031, "time": "2017-11-10T10:01:20Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-11-10T10:01:20Z", "is_private": false}, {"count": 32, "tags": [], "creator": "ylavic.dev@gmail.com", "text": "Created attachment 35513\nHandle POLLOUT in proxy_connect (v8)\n\nWell, same for the backend connection I guess, hence this v8.", "id": 202032, "time": "2017-11-10T10:26:45Z", "bug_id": 61616, "creation_time": "2017-11-10T10:26:45Z", "is_private": false, "attachment_id": 35513}, {"count": 33, "tags": [], "bug_id": 61616, "attachment_id": null, "text": "Tests with v8 look good with SSL. Thanks! :-)", "id": 202039, "time": "2017-11-10T15:49:39Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-11-10T15:49:39Z", "is_private": false}, {"count": 34, "tags": [], "bug_id": 61616, "attachment_id": null, "text": "Hi Yann,\n\nI have hit another problem with these patches, they do break mod_jk (ajp13). The ajp code in mod_jk uses the ap_rwrite() function from server/protocol.c, which now sometimes breaks it's semantics and doesn't finish the write of the whole buffer passed to it. The only other user of ap_rwrite() I could find is mod_lua, which I haven't tested. Any idea?", "id": 202620, "time": "2017-12-07T12:31:57Z", "creator": "carsten.wolff@dsa.de", "creation_time": "2017-12-07T12:31:57Z", "is_private": false}, {"count": 35, "tags": [], "creator": "carsten@wolffcarsten.de", "attachment_id": null, "id": 202787, "time": "2017-12-14T22:39:53Z", "bug_id": 61616, "creation_time": "2017-12-14T22:39:53Z", "is_private": false, "text": "Hum. It seems to be even worse. Now that proxy_connect works for my case with these patches, everything else seems to be broken, even the normal case of GET for static files produces incomplete files.\nTo test this, I just put a handfull of files of 500k into the document root and linked them from index.html. When I now do \"wget -r\", it recognizes partial content at connection close by the server:\n\n--2017-12-14 23:37:48--  http://192.168.122.11/1.html\nConnecting to 192.168.122.11:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 512000 (500K) [text/html]\nSaving to: \u2018192.168.122.11/1.html\u2019\n\n192.168.122.11/1.html                                   16%[===================>                                                                                                       ]  84,57K  --.-KB/s    in 0s      \n\n2017-12-14 23:37:48 (316 MB/s) - Connection closed at byte 86604. Retrying.\n\n--2017-12-14 23:37:49--  (try: 2)  http://192.168.122.11/1.html\nConnecting to 192.168.122.11:80... connected.\nHTTP request sent, awaiting response... 206 Partial Content\nLength: 512000 (500K), 425396 (415K) remaining [text/html]\nSaving to: \u2018192.168.122.11/1.html\u2019\n\n192.168.122.11/1.html                                   33%[++++++++++++++++++++====================>                                                                                  ] 169,09K  --.-KB/s    in 0,001s  \n\n2017-12-14 23:37:49 (99,2 MB/s) - Connection closed at byte 173153. Retrying."}, {"count": 36, "tags": [], "bug_id": 61616, "text": "Sorry for the delay Carsten, just to clarify, you are running httpd with attachment 35513 [Handle POLLOUT in proxy_connect (v8)] and attachment 35447 [ Read buffer size and flush threshold tuning], no other patch right?\n\nWhile attachment 35513 alone should work for the proxy_connect case, perfomances are better with tunings from attachment 35447, still right?\n\nIf this is the case, I'd suggest using attachment 35513 only for now, that's less performances (than expected?) but the same as the original proxy_connect ones.\n\nI'm looking at what could cause regressions in attachment 35447, but this patch is work in progress anyway and not really related to this PR I think.", "id": 202839, "time": "2017-12-19T11:46:05Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-12-19T11:46:05Z", "is_private": false, "attachment_id": null}, {"count": 37, "attachment_id": null, "creator": "carsten@wolffcarsten.de", "text": "(In reply to Yann Ylavic from comment #36)\n> Sorry for the delay Carsten, just to clarify, you are running httpd with\n> attachment 35513 [details] [Handle POLLOUT in proxy_connect (v8)] and\n> attachment 35447 [details] [ Read buffer size and flush threshold tuning],\n> no other patch right?\n\nAlmost. I added these two patches to packages from debian sid (https://packages.debian.org/sid/apache2) and to the SuSE package from obs (https://build.opensuse.org/package/show/Apache/apache2), so there are some distribution patches in there, but nothing essential at this time.\n\n> While attachment 35513 [details] alone should work for the proxy_connect\n> case, perfomances are better with tunings from attachment 35447 [details],\n> still right?\n\nYes, the single-transfer test runs for about the same time, but cpu load is less with attachment 35447, which promises better performance with concurrency.\n\n> If this is the case, I'd suggest using attachment 35513 [details] only for\n> now, that's less performances (than expected?) but the same as the original\n> proxy_connect ones.\n\nI did a quick test without attachment 35447 and it seems you're right, the regression seems to be in that patch. I will do more thorough testing tomorrow.\n\n> I'm looking at what could cause regressions in attachment 35447 [details],\n> but this patch is work in progress anyway and not really related to this PR\n> I think.\n\nRunning the client (wget) on localhost, I cannot reproduce the error. Running it from another machine, it's there 100% of the time. The logs of both cases look exactly the same, except for the last two lines:\n\nGOOD:\n[Mon Dec 18 13:36:59.042282 2017] [core:trace6] [pid 3507:tid 140042357954304] core_filters.c(449): [client 192.168.122.11:43236] core_output_filter: flushing because of max threshold\n[Mon Dec 18 13:36:59.042875 2017] [core:trace6] [pid 3507:tid 140042357954304] core_filters.c(857): [client 192.168.122.11:43236] writev_nonblocking: 512276/512276 bytes\n[Mon Dec 18 13:36:59.042929 2017] [core:trace6] [pid 3507:tid 140042357954304] core_filters.c(458): [client 192.168.122.11:43236] core_output_filter: flushing because of FLUSH bucket\n\nBAD:\n[Mon Dec 18 13:37:21.164185 2017] [core:trace6] [pid 3537:tid 140042248849152] core_filters.c(449): [client 192.168.122.12:51428] core_output_filter: flushing because of max threshold\n[Mon Dec 18 13:37:21.164499 2017] [core:trace6] [pid 3537:tid 140042248849152] core_filters.c(857): (11)Resource temporarily unavailable: [client 192.168.122.12:51428] writev_nonblocking: 86880/512276 bytes\n[Mon Dec 18 13:37:21.164548 2017] [core:trace6] [pid 3537:tid 140042248849152] core_filters.c(449): [client 192.168.122.12:51428] core_output_filter: flushing because of max threshold\n[Mon Dec 18 13:37:21.164566 2017] [core:trace1] [pid 3537:tid 140042248849152] core_filters.c(571): (70007)The timeout specified has expired: [client 192.168.122.12:51428] core_output_filter: writing data to the network", "id": 202847, "time": "2017-12-19T23:00:51Z", "bug_id": 61616, "creation_time": "2017-12-19T23:00:51Z", "tags": [], "is_private": false}, {"count": 38, "tags": [], "bug_id": 61616, "text": "(In reply to Carsten Wolff from comment #37)\n> (In reply to Yann Ylavic from comment #36)\n> > If this is the case, I'd suggest using attachment 35513 [details] only for\n> > now, that's less performances (than expected?) but the same as the original\n> > proxy_connect ones.\n> \n> I did a quick test without attachment 35447 [details] and it seems you're\n> right, the regression seems to be in that patch. I will do more thorough\n> testing tomorrow.\n\nStill, with just attachment 35513, the rsync transfer through proxy_connect often stalls at the very end of the transfer. This disappeared when I also used attachment 35447 and set FlushMinThreshold 1. That's the main reason why I used both patches.\n\nThe end of the log of such a stall:\n\n[Wed Dec 20 00:23:54.200810 2017] [ssl:trace6] [pid 24662:tid 140235413370624] ssl_engine_io.c(219): [client 192.168.122.12:33064] bio_filter_out_write: 1317 bytes\n[Wed Dec 20 00:23:54.200815 2017] [ssl:trace4] [pid 24662:tid 140235413370624] ssl_engine_io.c(2220): [client 192.168.122.12:33064] OpenSSL: write 1317/1317 bytes to BIO#7f8b080021b0 [mem: 7f8b080025a3] (BIO dump follows)\n[Wed Dec 20 00:23:54.200820 2017] [proxy:trace2] [pid 24662:tid 140235413370624] proxy_util.c(3845): [client 192.168.122.12:33064] ap_proxy_transfer_between_connections complete\n[Wed Dec 20 00:23:54.200835 2017] [core:trace6] [pid 24662:tid 140235413370624] core_filters.c(857): [client 192.168.122.12:33064] writev_nonblocking: 1317/1317 bytes\n[Wed Dec 20 00:28:54.301391 2017] [proxy_connect:error] [pid 24662:tid 140235413370624] (70007)The timeout specified has expired: [client 192.168.122.12:33064] AH01023: polling\n[Wed Dec 20 00:28:54.301651 2017] [proxy_connect:trace2] [pid 24662:tid 140235413370624] mod_proxy_connect.c(562): [client 192.168.122.12:33064] finished with poll() - cleaning up", "id": 202848, "time": "2017-12-19T23:35:14Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2017-12-19T23:35:14Z", "is_private": false, "attachment_id": null}, {"count": 39, "tags": [], "creator": "ylavic.dev@gmail.com", "text": "Created attachment 35620\nRead buffer size and flush threshold tuning (v2)\n\nThanks for the details.\n\nI was able to reproduce and the issue came from the socket timeout set to zero in ap_core_output_filter(), and later used for apr_poll() in send_brigade_blocking() (i.e. poll() was asked for immediate POLLOUT availability and timeouted otherwise, e.g. not on localhost...).\n\nFixed in this v2 by passing the (original) timeout to send_brigade_blocking().", "id": 202860, "time": "2017-12-20T13:57:47Z", "bug_id": 61616, "creation_time": "2017-12-20T13:57:47Z", "is_private": false, "attachment_id": 35620}, {"count": 40, "tags": [], "creator": "ylavic.dev@gmail.com", "text": "Created attachment 35621\nHandle POLLOUT in proxy_connect (v9)\n\nWhile at it, I also updated this patch for a better ap_proxy_transfer_between_connections() interface regarding the flags parameter.\n\nWould you mind testing this one with your update rather than v8?\nThanks!", "id": 202861, "time": "2017-12-20T14:01:28Z", "bug_id": 61616, "creation_time": "2017-12-20T14:01:28Z", "is_private": false, "attachment_id": 35621}, {"count": 41, "tags": [], "bug_id": 61616, "text": "Created attachment 35623\nHandle POLLOUT in proxy_connect (v10)\n\nArgh, new v10 since v9 changed semantics of ap_proxy_transfer_between_connections() regarding EAGAIN (which it can't since it's public API).", "id": 202865, "time": "2017-12-20T14:18:51Z", "creator": "ylavic.dev@gmail.com", "creation_time": "2017-12-20T14:18:51Z", "is_private": false, "attachment_id": 35623}, {"count": 42, "tags": [], "bug_id": 61616, "text": "Created attachment 35639\npcaps of a stall in rsync handshake (Test4)\n\nHello Yann, I wish you a happy new year!\n\nI did some more testing on different combinations of these patches. Here are the results (I will call patch 35620 \"v2\" and patch 35623 \"v10\"):\n\nTest1 (only v10 applied):\n    - wget for static contents: OK\n    - rsync through SSL+proxy_connect: sometimes still stalls at the very end of the transfer (just like in comment#38)\n\nTest2 (v10 and v2 applied):\n    - wget for static contents: OK\n    - rsync through SSL+proxy_connect: often stalls at the beginning of the transfer (during the rsync protocol handshake)\n\nTest3 (v10 and v2 applied, configuration from comment#24 added):\n    - wget for static contents: OK\n    - rsync through SSL+proxy_connect: OK (can't reproduce in the scenario I used so far ... but ...)\n\nBecause I was under the impression that timing plays a role in that code, I decided to repeat Test3 under different network conditions and used netem(1) to change the characteristics of the virtual network:\n\nTest4 (v10 and v2 applied, configuration from comment#24 added, netem):\n    - wget for static contents: OK\n    - rsync through SSL+proxy_connect: often stalls(2)(3) at the beginning of the transfer (during the rsync protocol handshake)\n\nSo I think it's safe to say that the bug in attachment 35447 has been fixed by \"v2\". But, unfortunately, stalls in SSL+proxy_connect can still be provoked under certain conditions. Next, I tried without SSL:\n\nTest5 (v10 and v2 applied, configuration from comment#24 added, netem, no SSL): \n    - rsync through proxy_connect: OK (cannot seem to reproduce the stalls)\n\nI have a feeling I should repeat Tests 1 and 2 without SSL to see if SSL is necessary for all the stalls to be reproduced. I'll let you know.\n\n(1)\ntc qdisc add dev vnet0 root handle 1: netem delay 100ms 50ms 25%\ntc qdisc add dev vnet0 parent 1:1 pfifo limit 1000\n\n(2) log of stall in Test4\n[Thu Dec 28 09:06:01.752182 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(503): [client 192.168.122.12:33950] client wait writable\n[Thu Dec 28 09:06:01.752210 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(456): [client 192.168.122.12:33950] AH01024: woke from poll(), i=1\n[Thu Dec 28 09:06:01.752227 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(531): [client 192.168.122.12:33950] client was writable\n[Thu Dec 28 09:06:01.752316 2017] [core:trace6] [pid 9674:tid 139811025307392] core_filters.c(740): [client 192.168.122.12:33950] writev_nonblocking: 69/69 bytes\n[Thu Dec 28 09:06:01.894674 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(456): [client 192.168.122.12:33950] AH01024: woke from poll(), i=1\n[Thu Dec 28 09:06:01.894840 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(487): [client 192.168.122.12:33950] AH01025: client was readable\n[Thu Dec 28 09:06:01.895319 2017] [ssl:trace4] [pid 9674:tid 139811025307392] ssl_engine_io.c(2220): [client 192.168.122.12:33950] OpenSSL: read 5/5 bytes from BIO#7f282c002270 [mem: 7f282c002bc3] (BIO dump follows)\n[Thu Dec 28 09:06:01.895393 2017] [ssl:trace4] [pid 9674:tid 139811025307392] ssl_engine_io.c(2220): [client 192.168.122.12:33950] OpenSSL: read 64/64 bytes from BIO#7f282c002270 [mem: 7f282c002bc8] (BIO dump follows)\n[Thu Dec 28 09:06:01.895495 2017] [proxy:trace2] [pid 9674:tid 139811025307392] proxy_util.c(3845): [client 192.168.122.12:33950] ap_proxy_transfer_between_connections: output filters full\n[Thu Dec 28 09:06:01.895531 2017] [proxy:trace2] [pid 9674:tid 139811025307392] proxy_util.c(3866): (70008)Partial results are valid but processing is incomplete: [client 192.168.122.12:33950] ap_proxy_transfer_between_connections finished\n[Thu Dec 28 09:06:01.895557 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(503): [client 192.168.122.12:33950] backend wait writable\n[Thu Dec 28 09:06:01.895737 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(456): [client 192.168.122.12:33950] AH01024: woke from poll(), i=1\n[Thu Dec 28 09:06:01.895785 2017] [proxy_connect:debug] [pid 9674:tid 139811025307392] mod_proxy_connect.c(531): [client 192.168.122.12:33950] backend was writable\n[Thu Dec 28 09:06:01.895915 2017] [core:trace6] [pid 9674:tid 139811025307392] core_filters.c(740): [remote ::1:873] writev_nonblocking: 9/9 bytes\n\n(3) bt of stall in Test4\nThread 3 (Thread 0x7f284bfff700 (LWP 9827)):\n#0  0x00007f2855f3f0f3 in epoll_wait () at ../sysdeps/unix/syscall-template.S:84\n#1  0x00007f2856435201 in impl_pollset_poll (pollset=0x7f2856b1e028, timeout=<optimized out>, num=0x7f284bffca34, descriptors=0x7f284bffca40) at ./poll/unix/epoll.c:266\n#2  0x00007f2852d4c4b5 in proxy_connect_handler (r=0x7f2856b1c0a0, worker=<optimized out>, conf=<optimized out>, url=<optimized out>, proxyname=<optimized out>, proxyport=<optimized out>) at mod_proxy_connect.c:446\n#3  0x00007f2852f5b3fc in proxy_run_scheme_handler (r=r@entry=0x7f2856b1c0a0, worker=0x7f2856c638d8, conf=conf@entry=0x7f2856c29320, url=0x7f2856b1d706 \"localhost:873\", proxyhost=proxyhost@entry=0x0, \n    proxyport=proxyport@entry=0) at mod_proxy.c:2888\n#4  0x00007f2852f5c371 in proxy_handler (r=0x7f2856b1c0a0) at mod_proxy.c:1230", "id": 203023, "time": "2018-01-02T22:12:30Z", "creator": "carsten@wolffcarsten.de", "creation_time": "2018-01-02T22:12:30Z", "is_private": false, "attachment_id": 35639}, {"count": 43, "tags": [], "bug_id": 61616, "attachment_id": null, "text": "(In reply to Carsten Wolff from comment #42)\n> I have a feeling I should repeat Tests 1 and 2 without SSL to see if SSL is\n> necessary for all the stalls to be reproduced. I'll let you know.\n\nSo I did:\n\nTest5 (only v10 applied, no SSL):\n    - rsync through proxy_connect: OK\n\nTest6 (only v10 applied, no SSL, netem):\n    - rsync through proxy_connect: OK\n\nTest7 (v10 and v2 applied, no SSL):\n    - rsync through proxy_connect: OK\n\nTest8 (v10 and v2 applied, no SSL, netem):\n    - rsync through proxy_connect: OK\n\nSo it definitely seems to be necessary that the frontend connection uses SSL for the stalls to be triggered.", "id": 203151, "time": "2018-01-08T12:25:57Z", "creator": "carsten.wolff@dsa.de", "creation_time": "2018-01-08T12:25:57Z", "is_private": false}]