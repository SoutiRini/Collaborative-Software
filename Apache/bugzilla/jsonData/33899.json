[{"count": 0, "tags": [], "text": "From some time my apache is consuming a lot of memory. It happens not very often \nbut, when it does, it take place for about 2 days (or more). Restarting apache \nhelps only for a while. After consumption of 60% memory, my load is growing up \nto 60, and the machine is almost down. I doesn't notice more than normal amount \nof http requests.\n\nI noticed problem on apache 2.0.52, 2.0.53, working on worker and prefork", "is_private": false, "id": 71961, "creator": "andree@ds5.agh.edu.pl", "time": "2005-03-08T12:35:50Z", "bug_id": 33899, "creation_time": "2005-03-08T12:35:50Z", "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 33899, "attachment_id": null, "is_private": false, "id": 71971, "time": "2005-03-08T16:05:27Z", "creator": "chip@force-elite.com", "creation_time": "2005-03-08T16:05:27Z", "text": "Without more details on your configuration, it ias hard to track down these types of problems.\n\nCan you try using the patch in Bug #33382?\n"}, {"count": 2, "tags": [], "text": "(In reply to comment #1)\n> Without more details on your configuration, it ias hard to track down these \ntypes of problems.\nI know. I will do my best to find something specyfic.\nfor now I can say that I'm using modules:\napache-mod_actions\napache-mod_auth_digest\napache-mod_deflate\napache-mod_ssl\napache-mod_auth\napache-mod_auth_dbm\napache-mod_autoindex\napache-mod_headers\napache-mod_dir\napache-mod_rewrite\napache-mod_vhost_alias\napache-mod_suphp-0.5.2\napache-apxs\napache-mod_auth_anon\napache-mod_dav\napache-mod_expires\nphp-5.0.2, kernel-2.6.8\n\n> Can you try using the patch in Bug #33382?\nI'm compiling it now.\n", "is_private": false, "id": 71979, "creator": "andree@ds5.agh.edu.pl", "time": "2005-03-08T17:01:42Z", "bug_id": 33899, "creation_time": "2005-03-08T17:01:42Z", "attachment_id": null}, {"count": 3, "tags": [], "bug_id": 33899, "is_private": false, "id": 72433, "attachment_id": null, "creator": "andree@ds5.agh.edu.pl", "creation_time": "2005-03-15T12:41:52Z", "time": "2005-03-15T12:41:52Z", "text": "I tried to use patch from #33382, and it fixed memory consumption.\nBut high load still exist i.e\nUSER       PID %CPU %MEM   VSZ  RSS TTY      STAT START   TIME COMMAND\nhttp     26055  0.0  1.6 98668 6504 ?        SN   01:07   0:00 httpd.worker -f /\netc/httpd/httpd.conf\nhttp     26057  0.0  1.6 86300 6456 ?        SN   01:07   0:00 httpd.worker -f /\netc/httpd/httpd.conf\nhttp      4623  0.0  2.5 80852 9916 ?        SN   01:17   0:00 httpd.worker -f /\netc/httpd/httpd.conf\nhttp      4768  0.0  2.4 77272 9568 ?        SN   01:18   0:00 httpd.worker -f /\netc/httpd/httpd.conf\nhttp      4828  0.0  2.8 81768 10916 ?       SN   01:18   0:00 httpd.worker -f /\netc/httpd/httpd.conf\nhttp      4836  0.0  2.5 76140 9816 ?        SN   01:18   0:00 httpd.worker -f /\netc/httpd/httpd.conf\nhttp      5090  0.0  2.2 80876 8736 ?        SN   01:19   0:00 httpd.worker -f /\netc/httpd/httpd.conf\nhttp      5097  0.0  2.4 80956 9264 ?        SN   01:19   0:00 httpd.worker -f /\netc/httpd/httpd.conf\n\nand load ~79\nafter apache restart, load is normal"}, {"count": 4, "tags": [], "text": "'High Load' is too generic.  We need more information about what is causing this\n'high load'.  Can you try getting a GDB backtrace or profiling the processes?", "is_private": false, "id": 75825, "creator": "chip@force-elite.com", "time": "2005-06-02T23:48:37Z", "bug_id": 33899, "creation_time": "2005-06-02T23:48:37Z", "attachment_id": null}, {"count": 5, "tags": [], "creator": "mhaerry@c3com.ch", "attachment_id": null, "text": "(In reply to comment #3)\n> I tried to use patch from #33382, and it fixed memory consumption.\n> But high load still exist i.e\n> USER       PID %CPU %MEM   VSZ  RSS TTY      STAT START   TIME COMMAND\n> [...]\n> and load ~79\n> after apache restart, load is normal\n\nas i'm using 2.0.54 and the patch from #33382 is included in this version i\nreassign this bug here.\n\ni'm using rhel3 with kernel 2.4.21-15.0.2.ELsmp and the apache2 is self compiled\ndownloaded from the apache.org site.\n\nit has sometimes a very high load and memory consumption, but only for about\nhalf an hour. then it is running normal again. but during the whole last day the\nload average was very high and also memory consumption. it's always just one\nhttpd process but sometimes its consumption is 86% of the memory and the CPU% is\n99%.\n\ni could fix the MemoryConsumption by adding MaxFreeMem 200 to the prefork Module\nsettings. but still sometimes one process is very high now:\n\ni.e.\n\nPID   USER     PRI  NI  SIZE  RSS SHARE STAT %CPU %MEM   TIME CPU COMMAND\n22589 nobody    25   0 25476  18M 14480 R    99.7  1.8  23:52   1 httpd\n24064 nobody    15   0 20732  15M  9636 S     0.5  1.5   0:00   3 httpd\n23298 nobody    15   0 30136  23M 18200 S     0.1  2.3   0:00   2 httpd\n\n\n./httpd -l gives me:\n\n  core.c\n  mod_access.c\n  mod_auth.c\n  mod_auth_digest.c\n  mod_include.c\n  mod_deflate.c\n  mod_log_config.c\n  mod_env.c\n  mod_headers.c\n  mod_setenvif.c\n  mod_ssl.c\n  prefork.c\n  http_core.c\n  mod_mime.c\n  mod_status.c\n  mod_autoindex.c\n  mod_asis.c\n  mod_cgi.c\n  mod_negotiation.c\n  mod_dir.c\n  mod_imap.c\n  mod_actions.c\n  mod_userdir.c\n  mod_alias.c\n  mod_rewrite.c\n  mod_so.c\n\nand i configured the source with:\n\n./configure --prefix=/appl/apache \\\n  --enable-so \\\n  --enable-auth-digest \\\n  --enable-rewrite \\\n  --enable-setenvif \\\n  --enable-mime \\\n  --enable-deflate \\\n  --enable-ssl \\\n  --with-ssl=/usr/local \\\n  --enable-headers\n\ni have logged the process-ids serving the requests and then when a process was\nhigh loaded i tried to figure out if there was a special request or so. but\nnothing like that.\n\nerror log isn't showing something special now (nothing)...\n\nas memory consumption is low now the load average isn't as high as before, it's\nbetter now. but i think this with MaxMemFree is just a nasty work around.\n\nanyway the sites are serving well now but i don't think this is ok like that.\n\nit seems that i was having this behaviour before updating to 2.0.54 but only\nwith big short peaks (load average for half an hour of 16 or so) but not a whole\nday long like today.", "id": 80262, "time": "2005-09-21T12:50:22Z", "bug_id": 33899, "creation_time": "2005-09-21T12:50:22Z", "is_private": false}, {"count": 6, "tags": [], "creator": "jorton@redhat.com", "is_private": false, "text": "When you see a process consuming CPU like, please run as root:\n\n  # strace -p 22589\n\nfor a while, to see what it is doing, and post the output of the strace command\nhere.  If there is no output from the strace command, use:\n\n  # gdb /path/to/httpd 22589\n  ...\n  (gdb) backtrace\n\nand post that information here instead.\n", "id": 80264, "time": "2005-09-21T12:56:57Z", "bug_id": 33899, "creation_time": "2005-09-21T12:56:57Z", "attachment_id": null}, {"count": 7, "tags": [], "text": "> When you see a process consuming CPU like, please run as root:\n> \n>   # strace -p 22589\n> \n> for a while, to see what it is doing, and post the output of the strace command\n> here.  If there is no output from the strace command, use:\n> \n>   # gdb /path/to/httpd 22589\n>   ...\n>   (gdb) backtrace\n> \n> and post that information here instead.\n\nit was hard to catch such a thread as i have to be logged on when it appears and\nstart the strace. otherwise it's very hard to work on the machine when it has a\nhuge load. anyway here is the strace output:\n\n--- strace -p 30610\n[...unforutnately it is not from begin on my screen buffer wasn't that huge...]\nbrk(0x35520000)                         = 0x35520000\nbrk(0)                                  = 0x35520000\nbrk(0x35541000)                         = 0x35541000\nbrk(0)                                  = 0x35541000\nbrk(0x35562000)                         = 0x35562000\nbrk(0)                                  = 0x35562000\nbrk(0x35583000)                         = 0x35583000\nbrk(0)                                  = 0x35583000\nbrk(0x355a4000)                         = 0x355a4000\nbrk(0)                                  = 0x355a4000\nbrk(0x355c5000)                         = 0x355c5000\nbrk(0)                                  = 0x355c5000\nbrk(0x355e6000)                         = 0x355e6000\nbrk(0)                                  = 0x355e6000\nbrk(0x35607000)                         = 0x35607000\nbrk(0)                                  = 0x35607000\n[...and so on many lines...]\nbrk(0)                                  = 0x395f7000\nbrk(0x39618000)                         = 0x39618000\nbrk(0)                                  = 0x39618000\nbrk(0x39639000)                         = 0x39639000\nbrk(0)                                  = 0x39639000\nbrk(0x3965a000)                         = 0x3965a000\nbrk(0)                                  = 0x3965a000\nbrk(0x3967b000)                         = 0x3967b000\nbrk(0)                                  = 0x3967b000\nbrk(0x3969c000)                         = 0x3969c000\nbrk(0)                                  = 0x3969c000\nbrk(0x396bd000)                         = 0x396bd000\nbrk(0)                                  = 0x396bd000\nbrk(0x396de000)                         = 0x396de000\nbrk(0)                                  = 0x396de000\nbrk(0x396ff000)                         = 0x396ff000\nProcess 30610 detached\n---\n\nto start a gdb i didn't get because the process after i detached from it didn't\nrun anymore...", "attachment_id": null, "id": 80529, "creator": "mhaerry@c3com.ch", "time": "2005-09-28T09:32:50Z", "bug_id": 33899, "creation_time": "2005-09-28T09:32:50Z", "is_private": false}, {"count": 8, "tags": [], "text": "A useful trick is that once you see a process eating RAM, run:\n\n   kill -STOP <pid>\n\nto immediately freeze the thread; it will then stop making your machine thrash\nand you can connect gdb to it at will.", "attachment_id": null, "id": 80539, "creator": "jorton@redhat.com", "time": "2005-09-28T10:28:34Z", "bug_id": 33899, "creation_time": "2005-09-28T10:28:34Z", "is_private": false}, {"count": 9, "tags": [], "creator": "mhaerry@c3com.ch", "attachment_id": null, "text": "(In reply to comment #8)\n> A useful trick is that once you see a process eating RAM, run:\n> \n>    kill -STOP <pid>\n> \n> to immediately freeze the thread; it will then stop making your machine thrash\n> and you can connect gdb to it at will.\n\nit seems like a buggy perl script did this highload. and it seems that it has\nbeen fixed.\n\nthanks anyway for your help\n\n", "id": 81692, "time": "2005-10-25T12:02:07Z", "bug_id": 33899, "creation_time": "2005-10-25T12:02:07Z", "is_private": false}, {"count": 10, "tags": [], "creator": "jorton@redhat.com", "attachment_id": null, "text": "OK, thanks for reporting back.", "id": 81694, "time": "2005-10-25T12:05:28Z", "bug_id": 33899, "creation_time": "2005-10-25T12:05:28Z", "is_private": false}]