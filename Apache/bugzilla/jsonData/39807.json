[{"attachment_id": null, "tags": [], "creator": "tyler@yi.org", "text": "I recently had some filesystem corruption that resulted in the file size of a\n.GIF  to be reported as 65,536 terabytes, even though it's only 782 bytes:\n\n-rw-r--r-- 1 kirin kirin               656 Mar 21  2005 btn_profile_on.gif\n-rw-r--r-- 1 kirin kirin               866 Mar 21  2005 btn_register.gif\n-rw-r--r-- 1 kirin kirin               772 Mar 21  2005 btn_register_on.gif\n-rw-r--r-- 1 kirin kirin 72057594037928718 Mar 21  2005 btn_search.gif\n-rw-r--r-- 1 kirin kirin               709 Mar 21  2005 btn_search_on.gif\n-rw-r--r-- 1 kirin kirin              1045 Mar 21  2005 btn_users.gif\n-rw-r--r-- 1 kirin kirin               915 Mar 21  2005 btn_users_on.gif\n\n# cp btn_search.gif btn_search2.gif\n# ls -l btn_search2.gif\n\n-rw-r--r-- 1 root root 782 Jun 13 15:19 btn_search2.gif\n\n\nWhen apache tries to serve the corrupt file, it enters into a loop in\ndefault_handler that causes it to use up all available memory.\n\n) bt\n#0  apr_bucket_alloc (size=52, list=0x8210390)\n    at buckets/apr_buckets_alloc.c:136\n#1  0xa7f8c790 in apr_bucket_simple_copy (a=0x8210670, b=0xaf9e12f0)\n    at buckets/apr_buckets_simple.c:22\n#2  0xa7f8a36c in apr_bucket_shared_copy (a=0x8210670, b=0xaf9e12f0)\n    at buckets/apr_buckets_refcount.c:38\n#3  0x0806d811 in default_handler (r=0x8320320) at core.c:3678\n#4  0x080740f7 in ap_run_handler (r=0x8320320) at config.c:157\n#5  0x080771e1 in ap_invoke_handler (r=0x8320320) at config.c:371\n#6  0x08081c48 in ap_process_request (r=0x8320320) at http_request.c:258\n#7  0x0807eeee in ap_process_http_connection (c=0x820c550) at http_core.c:172\n#8  0x0807ae77 in ap_run_process_connection (c=0x820c550) at connection.c:43\n#9  0x08085c24 in child_main (child_num_arg=<value optimized out>)\n    at prefork.c:640\n#10 0x08085f1a in make_child (s=<value optimized out>, slot=0) at prefork.c:736\n#11 0x08085fda in startup_children (number_to_start=5) at prefork.c:754\n#12 0x08086a44 in ap_mpm_run (_pconf=0x80a70a8, plog=0x80d5160, s=0x80a8f48)\n    at prefork.c:975\n#13 0x08061dcf in main (argc=134893856, argv=0x8153390) at main.c:717\n\nI'm not sure if httpd would go into such a tailspin if the file *was* actually\nthat huge or not, but it would be nice to see it be more resilient to large\nfiles and filesystem corruption.", "count": 0, "id": 90161, "time": "2006-06-13T22:22:12Z", "bug_id": 39807, "creation_time": "2006-06-13T22:22:12Z", "is_private": false}, {"count": 1, "tags": [], "text": "The out of memory is from trying to split the huge file into buckets of\nAP_MAX_SENDFILE size to sendfile each section of the file.  I don't believe\nthere is anything we can do in this case, except to call the OOM abort function\nin APR, which has already been done in trunk.\n\nIf you had a 64bit machine/OS/httpd (or where sizeof(apr_off_t) <=\nsizeof(apr_size_t)), I believe it would work since it would attempt to\nsendfile() it all in one bucket, rather than millions AP_MAX_SENDFILE size buckets.", "attachment_id": null, "id": 90182, "creator": "chip@force-elite.com", "time": "2006-06-14T05:46:30Z", "bug_id": 39807, "creation_time": "2006-06-14T05:46:30Z", "is_private": false}, {"count": 2, "tags": [], "bug_id": 39807, "attachment_id": null, "id": 90190, "time": "2006-06-14T09:19:13Z", "creator": "jorton@redhat.com", "creation_time": "2006-06-14T09:19:13Z", "is_private": false, "text": "I don't think it's unreasonable to expect that the server should either handle\nthis situation correctly or at least fail gracefully without attempting to eat\nall your RAM.\n\nIt would be possible to handle this quite correctly by storing an \"insanely\nlarge file\" in a bucket with unknown length.  This could be done either as a\nmodification to the file bucket or, I suppose, as a new bucket type.  httpd\ncould handle such a bucket without problems; apr_brigade_insert_file could do\nwhatever is necessary to insert it.\n\nBut I'm not entirely convinced it would be worth all the effort to fix this\nproperly unless someone really wants to serve insanely large files with 32-bit\nhttpd binaries.  By my calculations apr_brigade_file_insert() will be using 36Mb\nof RAM in bucket structures per petabyte of file."}, {"count": 3, "tags": [], "bug_id": 39807, "attachment_id": null, "id": 90192, "time": "2006-06-14T10:05:09Z", "creator": "nick@webthing.com", "creation_time": "2006-06-14T10:05:09Z", "is_private": false, "text": "The server has effectively DOSed itself.  A 500 response would be a sensible \nalternative.\n\nHow to detect when to bail out?  Maybe a LimitResponseSize directive (with a \ndefault value of AP_MAX_SENDFILE) would make sense as a sanity check?"}, {"count": 4, "tags": [], "bug_id": 39807, "attachment_id": 18464, "id": 90205, "time": "2006-06-14T16:41:42Z", "creator": "tyler@yi.org", "creation_time": "2006-06-14T16:41:42Z", "is_private": false, "text": "Created attachment 18464\nAdd a \"LimitResponseFileSize\" directive for default_handler to use.\n\nThis patch implements Nick's suggestion to have a config directive that limits\nthe maximum size of a file we will send.\n\nI named the directive \"LimitResponseFileSize\" instead of \"LimitResponseSize\",\nbecause the directive only affects default_handler; other handlers could still\nsend more information than this in a response."}, {"count": 5, "tags": [], "bug_id": 39807, "text": "Might not be obvious, we have a keyword rather than changing subject ;-)", "id": 90211, "time": "2006-06-14T17:29:44Z", "creator": "wrowe@apache.org", "creation_time": "2006-06-14T17:29:44Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 39807, "attachment_id": null, "id": 90212, "time": "2006-06-14T17:30:59Z", "creator": "tyler@yi.org", "creation_time": "2006-06-14T17:30:59Z", "is_private": false, "text": "ahh, didn't catch that. thanks. :-)\n"}, {"count": 7, "tags": [], "bug_id": 39807, "text": "Please let's not add yet-another-config-directive just to paper over a bug.\n\n1) this doesn't catch all the places where such files are added to brigades\n2) there is more code added there than would be necessary to actually correctly\nhandle such files\n3) enforcing a 16Mb default response size is... unwise ;)", "id": 90240, "time": "2006-06-15T10:43:03Z", "creator": "jorton@redhat.com", "creation_time": "2006-06-15T10:43:03Z", "is_private": false, "attachment_id": null}, {"count": 8, "tags": [], "bug_id": 39807, "attachment_id": null, "id": 90304, "time": "2006-06-16T16:23:32Z", "creator": "tyler@yi.org", "creation_time": "2006-06-16T16:23:32Z", "is_private": false, "text": "I didn't think it would be that easy either... nothing's that easy. :)\n\nYou're suggesting a \"bucket with unknown length\"... I'm not really sure what\nthis means or how it'd work but it *sounds* interesting...\n\n> 1) this doesn't catch all the places where such files are added to brigades\n\nFair enough\n\n> 2) there is more code added there than would be necessary to actually\ncorrectly handle such files\n\nWell that patch took me 10 minutes to whip up... so I guess that means you can\nsupply a patch to fix this properly in 2 minutes? :)\n\n> 3) enforcing a 16Mb default response size is... unwise ;)\n\nPeople shouldn't have filed larger than that on their webservers! HTTP is not a\nfile transfer protocol!\n\n... kidding... I actually had no idea the limit was set that low. But that means\nthat httpd is creating a bunch of these structures whenever it sends out any\nlarge file, not just an insanely large one... hmm...\n\nI'm going to stop thinking about this now.\n\n"}, {"count": 9, "tags": [], "bug_id": 39807, "attachment_id": null, "id": 90359, "time": "2006-06-19T14:02:41Z", "creator": "jorton@redhat.com", "creation_time": "2006-06-19T14:02:41Z", "is_private": false, "text": "Well, OK :) I *thought* this would be a 2 minute patch but modifying the FILE\nbucket to do this actually isn't possible, so my point (2) is out of the window.\n I still think that adding config directives for this is the wrong approach,\nhowever.\n\nIt's actually annoyingly difficult to fix this in APR-util because\napr_brigade_insert_file() cannot assume that the *whole* passed-in file must be\ninserted; so a new function with those semantics would be needed."}]