[{"attachment_id": 33531, "tags": [], "creator": "slash@aceslash.net", "text": "Created attachment 33531\nGraph of network connection status during the crash of the connector\n\n==============================\nEnvironment:\nDebian 8\nTomcat 8.0.30\nJava Oracle JDK 1.8.0_72\nUsing connector NIO, current connector configuration:\n    <Connector port=\"8001\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\"\n        connectionTimeout=\"20000\"\n        acceptorThreadCount=\"4\"\n        maxThreads=\"200\"\n        maxConnections=\"1000\"\n        maxKeepAliveRequests=\"5000\" />\nHardware: different servers, Intel Xeon CPU with a total of 16 core (32 thread) memory per tomcat around 30GB, using G1GC.\n==============================\nWhat is happening:\nBefore the update, with Tomcat version 8.0.27, we didn't have any issue with the NIO connector, it was working fine and websocket too.\nSince the update, the connector just \"crash\" after several hours of work: no request are then processed (websocket or http), trying to access any application from http://ip:8001/ just hangs. Looking at the state of the network socket, it is clearly not working (graph attached).\n\nThe http/NIO connector is used almost exclusively for websocket connections (the only connection that are not websocket are from our internal connector checker).\n\nThere is also an AJP/APR connector that is working fine during that time, even when the NIO/http connector crash.\n\nI don't see anything in the catalina.out nor in the system log... \n\nI know this is difficult to debug with so little information, I only see this issue in production myself when there is a large number of connections, never in test.\n\nThe tomcat is behind an apache httpd 2.4 proxy, relevant configuration:\nJkMount /APPNAME* server_tomcat1\nProxyPass /APPNAME/realtime/ ws://server.example.net:8001/APPNAME/realtime/\nProxyPassReverse /APPNAME/realtime/ ws://server.example.net:8001/APPNAME/realtime/", "count": 0, "id": 188227, "time": "2016-02-04T17:18:07Z", "bug_id": 58970, "creation_time": "2016-02-04T17:18:07Z", "is_private": false}, {"attachment_id": null, "tags": [], "bug_id": 58970, "text": "Thread dump when the problem occurs and logs leading up to the problem please.\n\nBest guess at this point in that the Poller thread stopped but without information that is nothing more than a wild guess.", "count": 1, "id": 188237, "time": "2016-02-05T09:36:40Z", "creator": "markt@apache.org", "creation_time": "2016-02-05T09:36:40Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "slash@aceslash.net", "is_private": false, "count": 2, "id": 188239, "time": "2016-02-05T10:30:41Z", "bug_id": 58970, "creation_time": "2016-02-05T10:30:41Z", "text": "I know it's difficult to debug like this, unfortunately I had to rollback the production to 8.0.27 for now to restore our websocket services.\n\nI'll see what I can do to give you relevant logs/thread dump."}, {"count": 3, "tags": [], "text": "Created attachment 33732\nThread dump of a tomcat 8.0.30 with http connector frozen\n\nHello, \n\nPlease find the required thread dump in attachment.\nThread dump of a tomcat 8.0.30 with a frozen http nio connector.\n\nRegards", "is_private": false, "bug_id": 58970, "id": 190017, "time": "2016-04-06T13:43:41Z", "creator": "reda.housnialaoui@gmail.com", "creation_time": "2016-04-06T13:43:41Z", "attachment_id": 33732}, {"count": 4, "tags": [], "bug_id": 58970, "text": "The dump looks slightly weird (lots of APR AJP, this seems more active to me than the NIO connector). However, the NIO connector is indeed stuck on its max connections which probably have been leaked due to the Atmosphere use, which may or may not be doing bad things.\n\nmaxConnections is 10000 and often does not make sense (I disabled it by default for the NIO2 connector).\n\nSo I'll switch it back to need info since there's no proof this is valid (or the same issue that was originally reported, although I'd say it's likely).", "id": 190023, "time": "2016-04-06T16:32:56Z", "creator": "remm@apache.org", "creation_time": "2016-04-06T16:32:56Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "text": "I am sorry, I wasn't clear enough.\nSlash and me are working in the same company, so I can assure you that the uploaded thread dump is about this issue.\n\nWe have a lot of trafic on AJP and less on http NIO because all non websocket traffic is going through httpd modjk and then AJP connector.\nSince modjk can't deal with websocket connections, http NIO connector is here to only manage websocket traffic.\n\nHere is what we do to systematically reproduce the issue:\n- From a nodejs application we try to establish 20 000 atmosphere connections using websocket transport to the app running in tomcat 8.0.30\n- Once we hit the max connection, we wait about 1 minute\n- Then we kill violently the node application and relaunch it to establish 20 000 new atmosphere connections\n- If the http connector is still alive, we repeat the whole operation\n\nIt takes about 3 attemps to crash the http connector.\nIn the end, the node app is totally stopped, there is no more connection to the tomcat http nio connector and yet the connector is totally frozen.\n\nFrom what I have seen, comparing healty tomcat tdump and tomcat with frozen connector tdump, I can see that when connector is frozen, all http nio acceptors thread are in PARKING status.", "is_private": false, "id": 190039, "creator": "reda.housnialaoui@gmail.com", "time": "2016-04-07T07:49:45Z", "bug_id": 58970, "creation_time": "2016-04-07T07:49:45Z", "attachment_id": null}, {"count": 6, "tags": [], "creator": "reda.housnialaoui@gmail.com", "text": "I don't know if you can see this in the tdump but we are using the JSR356 websocket implementation.", "id": 190040, "time": "2016-04-07T07:58:42Z", "bug_id": 58970, "creation_time": "2016-04-07T07:58:42Z", "is_private": false, "attachment_id": null}, {"count": 7, "attachment_id": null, "bug_id": 58970, "is_private": false, "id": 190198, "time": "2016-04-12T21:25:49Z", "creator": "markt@apache.org", "creation_time": "2016-04-12T21:25:49Z", "tags": [], "text": "The problem is with the current connection count tracking. There are code paths where this isn't being decremented when a connection closes in error. I'm currently looking for a reliable way to track the open connection count."}, {"count": 8, "tags": [], "creator": "markt@apache.org", "attachment_id": null, "text": "I (think I) found the root cause. This has been fixed in:\n- 9.0.x for 9.0.0.M5\n- 8.5.x for 8.5.1\n- 8.0.x for 8.0.34\n- 7.0.x for 7.0.70", "id": 190218, "time": "2016-04-13T19:14:04Z", "bug_id": 58970, "creation_time": "2016-04-13T19:14:04Z", "is_private": false}, {"count": 9, "tags": [], "text": "Thank you for the fix.\nWhen can we expect the 8.0.34 release?\nWould it be wise to use the current 8.0.34 snapshot in production?", "is_private": false, "id": 190288, "creator": "reda.housnialaoui@gmail.com", "time": "2016-04-15T12:37:47Z", "bug_id": 58970, "creation_time": "2016-04-15T12:37:47Z", "attachment_id": null}, {"count": 10, "tags": [], "bug_id": 58970, "attachment_id": null, "is_private": false, "id": 190289, "time": "2016-04-15T12:43:37Z", "creator": "remm@apache.org", "creation_time": "2016-04-15T12:43:37Z", "text": "Simply set maxConnections to unlimited (-1) in your configuration and you're done."}, {"count": 11, "tags": [], "text": "Hello,\n\nWe still have the issue on tomcat 8.0.37 and 8.0.38 with the same configuration.\nNew jstack attached.", "is_private": false, "id": 194886, "creator": "reda.housnialaoui@gmail.com", "time": "2016-11-09T09:37:37Z", "bug_id": 58970, "creation_time": "2016-11-09T09:37:37Z", "attachment_id": null}, {"count": 12, "tags": [], "text": "The dump is too big to be attached.\n\nHere is a link to download it: \nhttp://s000.tinyupload.com/index.php?file_id=00903516386387493654", "attachment_id": null, "bug_id": 58970, "id": 194888, "time": "2016-11-09T10:10:06Z", "creator": "reda.housnialaoui@gmail.com", "creation_time": "2016-11-09T10:10:06Z", "is_private": false}, {"count": 13, "tags": [], "creator": "reda.housnialaoui@gmail.com", "text": "The dump comes from a tomcat 8.0.38 with crashed http connector.", "id": 194889, "time": "2016-11-09T10:11:57Z", "bug_id": 58970, "creation_time": "2016-11-09T10:11:57Z", "is_private": false, "attachment_id": null}, {"count": 14, "tags": [], "bug_id": 58970, "text": "Do the same reproduction steps still create the issue?\n\nCan you provide a (simple as possible) web application and client we can use to recreate this problem?", "id": 194890, "time": "2016-11-09T11:40:39Z", "creator": "markt@apache.org", "creation_time": "2016-11-09T11:40:39Z", "is_private": false, "attachment_id": null}, {"count": 15, "tags": [], "bug_id": 58970, "attachment_id": null, "id": 194891, "time": "2016-11-09T12:22:54Z", "creator": "remm@apache.org", "creation_time": "2016-11-09T12:22:54Z", "is_private": false, "text": "I still don't understand if this is caused by maxConnections or not. Can the unlimited setting be tried and/or the connection count be monitored ?\n\nUsually unplugging a network cable is the worst test since the network connection may never be actually noticed by the other server as being dead. However, the server connectionTimeout should work, but it doesn't necessarily apply in all cases (websockets, etc, and precisely that's the scenario here)."}, {"count": 16, "tags": [], "bug_id": 58970, "is_private": false, "text": "No further response from OP, no info on how to reproduce this and no similar reports from other users.\n\nIf you believe you are experiencing this issue or one similar, please open a new issue with the steps to reproduce the issue on clean install of the latest 7.0.x, 8.0.x, 8.5.x or 9.0.x release.", "id": 198156, "time": "2017-04-04T15:38:59Z", "creator": "markt@apache.org", "creation_time": "2017-04-04T15:38:59Z", "attachment_id": null}]