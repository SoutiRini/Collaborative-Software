[{"count": 0, "tags": [], "text": "Created attachment 22554\nexcel and sourcecode to reproduce situation\n\nCloning sheets having several columns with autofilter leads to HUGE output files.\n\nOpening the file into excel and saving again reduces size dramatically.\n\nCloning same sheets without autofilters produces much smaller file.\n\n- without autofilter     320k\n- with autofilter      13080k\n- same back from excel   580k\n \nSize-effect is the same with both svn r696683 (working autofilters, see fix 45720) as with 3.0.2-FINAL (broken autofilters).\n\nSourcecode to create examples attached.", "attachment_id": 22554, "id": 120533, "creation_time": "2008-09-11T05:14:47Z", "time": "2008-09-11T05:14:47Z", "creator": "antti.koskimaki@joinex.com", "bug_id": 45778, "is_private": false}, {"count": 1, "tags": [], "bug_id": 45778, "attachment_id": null, "text": "fixed in svn r707450\n\nJunit added.\n\nObjRecord was not reading sub-record ftLbsData properly.  ftLbsData does not encode its length in the second ushort field.  This field often has a value like 0x1FFEE.  Due to another bug in RecordInputStream that allows buffer read-overruns, the ftLbsData was being interpreted as being nearly 8KB instead of 20 bytes.  \n\n\nI re-ran your test (cloning 50 sheets from a 22KB input workbook).\nHere are the resulting file sizes:\nbefore fix: 13,101KB\nafter fix:     588KB\n\nYour test spreadsheet had 30 (autofilter) combo-boxes, which get cloned 50 times.  Assuming 8KB extra per combo-box, this would account for roughly 12MB extra allocation.  I didn't attempt the same test (manually) with Excel, but the remaining difference from the 580KB that you got could be explained by POI's expansion of MULRK records.  Preserving MULRK, MULBLANK and SHARREDFMLA records is on the to-do list but currently not a high priority.", "id": 121836, "time": "2008-10-23T12:29:45Z", "creator": "josh@apache.org", "creation_time": "2008-10-23T12:29:45Z", "is_private": false}, {"count": 2, "tags": [], "creator": "antti.koskimaki@joinex.com", "attachment_id": null, "id": 121842, "time": "2008-10-24T00:05:28Z", "bug_id": 45778, "creation_time": "2008-10-24T00:05:28Z", "is_private": false, "text": "> I re-ran your test (cloning 50 sheets from a 22KB input workbook).\n> Here are the resulting file sizes:\n> before fix: 13,101KB\n> after fix:     588KB\n\nThanks, verified that this is OK.\n\nBut, when I tried to stress-test the fix, I noticed that by adding more cloning-iterations the workbook got corrupted at some point (limit=83, to be exact :=). Workbook without auto-filters seems not to corrupt, not even with 1000 clones.\n\nWhen I tried to open the workbook I got several \"File error, data may have been lost\" pop-ups in row, one for each (!) cloned sheet. Besides fatal, also very annoying :=)\n\nNot sure how closely relates to this bug or fix, seems to re-produce with 3.2-FINAL too. But you mentioning buffer-overruns and me using the same test-case to reproduce, plus auto-filters having something to do with it, I decided to re-open this bug instead of reporting new one.\n\n \n\n"}, {"attachment_id": null, "tags": [], "bug_id": 45778, "is_private": false, "count": 3, "id": 124449, "time": "2009-01-28T18:40:31Z", "creator": "josh@apache.org", "creation_time": "2009-01-28T18:40:31Z", "text": "(In reply to comment #2)\n\nBTW - the buffer overrun problem in RecordInputStream (that helped keep this bug silent) was fixed in svn r707778 .\n\nIt would be better if we close off this bugzilla to help track things more clearly.  I am pretty sure that the original bug is fixed.  The new bug you describe has different characteristics and the fix will probably be unrelated to svn r707450 .\n\nPlease open a new bugzilla for your second bug.  I suspect that this new bug will be of a lower priority since most POI users won't clone anywhere near 83 sheets.  However, if you need a fix urgently, you can speed things up by providing a patch or at least identifying the exact cause of the bug.\n\n\n \n\n"}]