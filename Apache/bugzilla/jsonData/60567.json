[{"count": 0, "tags": [], "creator": "dejian.tu@oracle.com", "text": "My project is using POI library to read excel file in HDFS. The API I used is as below:\n==================\n// inputStream is generated from a HDFS path, because OPCPackage could\n// not recognize HDFS path directly.\nXSSFReader xssfReader = new XSSFReader(OPCPackage.open(inputStream));\n==================\n\nThe excel file has around 1,000,000 rows of simple data (columns like name, id, address, etc.), and the file size is around 140MB. When I run my project, the process consumes about 3.25GB memory, which is much bigger than the excel file size.\n\nAFAIK, reading from a String path or File uses much less memory than reading from inputStream for XSSFReader. But for my case, because the excel file is in HDFS file system, we could not pass the HDFS path to XSSFReader directly.\n\nCould you please help to fix the issue that XSSFReader uses much more memory when reading from inputStream?\n\nThank you.", "id": 195945, "time": "2017-01-10T00:20:06Z", "bug_id": 60567, "creation_time": "2017-01-10T00:20:06Z", "is_private": false, "attachment_id": null}, {"attachment_id": null, "tags": [], "bug_id": 60567, "is_private": false, "count": 1, "id": 196189, "time": "2017-01-19T08:30:43Z", "creator": "onealj@apache.org", "creation_time": "2017-01-19T08:30:43Z", "text": "1,000,000 rows is massive. That's nearly the maximum number of rows allowed per the file format specification.\n\n140 MB file size is massive. Keep in mind that this is zipped XML files, and I would expect 90-95% compression for these files. Unzip this on your hard drive to see how much disk space is consumed when you expand it. It should be in the neighborhood of 1-3 GB.\n\nYou're also opening the file via an input stream, which has some memory overhead.\n\nTherefore, 3.25 GB of memory consumption is reasonable in this case, considering input stream overhead, memory alignment, garbage collection, temporary files for unzipping, maintaining references to files in the unzipped directory structure, creating XML trees for the minimum files needed for XSSFReader.\n\nIf you have any suggestions and could contribute a patch towards lowering XSSFReader's memory footprint, we'd greatly appreciate the help."}, {"count": 2, "tags": [], "creator": "onealj@apache.org", "text": "(In reply to Dejian Tu from comment #0)\n> AFAIK, reading from a String path or File uses much less memory than reading\n> from inputStream for XSSFReader. But for my case, because the excel file is\n> in HDFS file system, we could not pass the HDFS path to XSSFReader directly.\n\nThis sounds like a point for discussion on the mailing list (and perhaps Stack Overflow or other community to get suggestions on how to write a program that can deal with data stored on a distributed file system), and is not a bug without convincing evidence and a patch.", "id": 196190, "time": "2017-01-19T08:37:53Z", "bug_id": 60567, "creation_time": "2017-01-19T08:37:53Z", "is_private": false, "attachment_id": null}, {"attachment_id": null, "tags": [], "bug_id": 60567, "is_private": false, "count": 3, "id": 198809, "time": "2017-05-17T09:11:16Z", "creator": "chenchanghan1@huawei.com", "creation_time": "2017-05-17T09:11:16Z", "text": "I encounter the same question. 700,000 rows, size:139M\nI find use OPCPackage.open(inputStream) will get OutOfMemoryError,But use OPCPackage.open(filePath) run very well."}]