[{"count": 0, "tags": [], "bug_id": 47740, "attachment_id": 24167, "text": "Created attachment 24167\nthread dump\n\nWe were using log4j 1.2.8. We were getting deadlocks similar to following.\n\nhttps://issues.apache.org/bugzilla/show_bug.cgi?id=38137 : Curt, we have also\nexperienced this exact problem in our production system...We are \ncurrently using log4j 1.2.8...Please retry using log4j 1.2.15\n\nThen we upgraded to log4j 1.2.15. But we still got following deadlock. Please\nsee attachment for more details.\n\n\"RMI TCP Connection(5392)-154.4.160.174\" daemon prio=1 tid=0x0000002af80e9110\nnid=0x1e26 waiting for monitor entry [0x0000000043182000..0x0000000043183bb0]\n    at java.util.Hashtable.get(Hashtable.java:335)\n    - waiting to lock <0x0000002aa16b5fb8> (a java.util.Hashtable)\n    at org.apache.log4j.NDC.getCurrentStack(NDC.java:135)\n    at org.apache.log4j.NDC.get(NDC.java:220)\n    at org.apache.log4j.spi.LoggingEvent.getNDC(LoggingEvent.java:303)\n    at org.apache.log4j.AsyncAppender.append(AsyncAppender.java:155)\n    at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n    - locked <0x0000002aa16bf418> (a org.apache.log4j.AsyncAppender)\n    at\norg.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n    at org.apache.log4j.Category.callAppenders(Category.java:206)\n    - locked <0x0000002aa16c6040> (a org.apache.log4j.spi.RootLogger)\n    at org.apache.log4j.Category.forcedLog(Category.java:391)\n    at org.apache.log4j.Category.info(Category.java:666)\n    at\ncom.abc.dataExtractor.SLMTDSLFDDataExtractor.extract(SLMTDSLFDDataExtractor.java:192)\n    at com.abc.pdfGenerator.PdfGeneratorImpl.start(PdfGeneratorImpl.java:379)\n    at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:585)\n    at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:294)\n    at sun.rmi.transport.Transport$1.run(Transport.java:153)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at sun.rmi.transport.Transport.serviceCall(Transport.java:149)\n    at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:466)\n    at\nsun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:707)\n    at java.lang.Thread.run(Thread.java:595)", "id": 129943, "time": "2009-08-26T03:48:08Z", "creator": "ashutoshpunhani@yahoo.com", "creation_time": "2009-08-26T03:48:08Z", "is_private": false}, {"count": 1, "tags": [], "bug_id": 47740, "attachment_id": null, "text": "Looked at the stack trace and looks like there is contention on Hashtable 0x0000002aa16b5fb8, but I've got no obvious reason why anything would be deadlocking on it.\n\nHowever, there is a weird mix of internal and external synchronization on the ht variable.  Note the initial comment that synchronized is not used in this class, but then it appears in lazyRemove, but lazyRemove does not appear in your thread dumps.\n\nI'd look for any reports on synchronization issues with HashTable on your JVM.\n\nIf you are willing to experiment and know how to reproduce them problem, rewrite NDC using externally synchronized HashMap's.  If that fixes the problem, we may migrate to that approach.", "id": 131015, "time": "2009-10-08T22:12:28Z", "creator": "carnold@apache.org", "creation_time": "2009-10-08T22:12:28Z", "is_private": false}, {"count": 2, "tags": [], "creator": "ashutoshpunhani@yahoo.com", "text": ">> If you are willing to experiment and know how to reproduce them problem\n\nI'm afraid I won't be able to spend time on that. Also it is very difficult to reproduce the problem in a deterministic way.", "id": 133655, "time": "2010-01-17T03:22:37Z", "bug_id": 47740, "creation_time": "2010-01-17T03:22:37Z", "is_private": false, "attachment_id": null}, {"count": 3, "tags": [], "creator": "hostalp@post.cz", "attachment_id": null, "id": 135379, "time": "2010-03-16T11:50:39Z", "bug_id": 47740, "creation_time": "2010-03-16T11:50:39Z", "is_private": false, "text": "I looked at the thread dump provided and actually I don't see any classic deadlock there.\n\nWhat can be seen there is that some native thread holds a monitor lock on that hashtable so all those threads are waiting for it. It could be because of garbage collection run for example.\n\nI've seen the similar behavior on HP JVMs (but on HP-UX) many times before and it was almost always caused by garbage collections running repeatedly.\nI noticed that in many cases (full heap, can't be GC'd) the HP JVM doesn't throw OutOfMemoryError but instead ends up in endless garbage collection cycle. This happens especially (maybe only - I'm not sure) if parallel GC is used which is the default for multi CPU systems.\n\nWhat can you do is to run the JVM with verbose GC (on HP JVM it's like -Xverbosegc:file=/tmp/java-something.vgc ) and when you notice such behavior again have a look at the vgc file with HPjmeter for example. This will tell you if your JVM ends up in that GC loop."}]