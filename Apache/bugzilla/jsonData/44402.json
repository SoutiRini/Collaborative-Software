[{"count": 0, "tags": [], "bug_id": 44402, "attachment_id": null, "id": 113672, "time": "2008-02-12T16:47:39Z", "creator": "basant.kukreja@sun.com", "creation_time": "2008-02-12T16:47:39Z", "is_private": false, "text": "I am running specweb99 static content workload with httpd 2.2.6 on Solaris\nnevada (snv_79).  I am seeing several crashes. Typically crash do reproduce in\n10 minutes. Here are the details :\nApache version : httpd-2.2.6\nSimultaneous connection : 1000\nHardware : X4100 Server (4 core 2.8 GHz)\nCPU : Only single core is enabled\nArchitecture : x86_64\n\n\nhttpd.conf contains :\n<IfModule worker.c>\nListenBackLog     50000\nStartServers         2\nThreadLimit        500\nThreadsPerChild    500\nMinSpareThreads    100\nMaxSpareThreads    100\nThreadsPerChild    500\nMaxClients        1000\nMaxRequestsPerChild  0\n</IfModule>\n\nListen 192.168.21.1:80\nListen 192.168.22.1:80\n\nHere is the most common stack trace. \n\nConfigure option :\nCFLAGS=\"-g -mt -m64 -KPIC \" ./configure --prefix=<prefix_path> --with-mpm=worker\n--enable-modules=all --with-ssl=/usr/sfw --enable-mods-shared=all --enable-cgi\n--enable-threads && gmake && gmake install\n\nCrash 1 :\n(dbx) where\ncurrent thread: t@76\n=>[1] allocator_free(allocator = 0x101f870, node = (nil)), line 331 in \"apr_pools.c\"\n  [2] apr_pool_clear(pool = 0x102fb88), line 710 in \"apr_pools.c\"\n  [3] ap_core_output_filter(f = 0x1020550, b = 0x101f9e8), line 899 in\n\"core_filters.c\"\n  [4] ap_pass_brigade(next = 0x1020550, bb = 0x101f9e8), line 526 in \"util_filter.c\"\n  [5] logio_out_filter(f = 0x10204e0, bb = 0x101f9e8), line 135 in \"mod_logio.c\"\n  [6] ap_pass_brigade(next = 0x10204e0, bb = 0x101f9e8), line 526 in \"util_filter.c\"\n  [7] ap_flush_conn(c = 0x101fd00), line 84 in \"connection.c\"\n  [8] ap_lingering_close(c = 0x101fd00), line 123 in \"connection.c\"\n  [9] process_socket(p = 0x101f968, sock = 0x101f9e8, my_child_num = 1,\nmy_thread_num = 227, bucket_alloc = 0x1029a88), line 545 in \"worker.c\"\n  [10] worker_thread(thd = 0x5bed38, dummy = 0x6dbac0), line 894 in \"worker.c\"\n  [11] dummy_worker(opaque = 0x5bed38), line 142 in \"thread.c\"\n  [12] _thr_setup(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5d8f7\n  [13] _lwp_start(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5dba0\n\nCrash 2 :\n(dbx) where\ncurrent thread: t@363\n=>[1] apr_palloc(pool = 0x21680007952225ff, size = 18446744073323675656U), line\n601 in \"apr_pools.c\"\n  [2] apr_sockaddr_ip_get(addr = 0xcda3d0, sockaddr = 0x42d790), line 104 in\n\"sockaddr.c\"\n  [3] core_create_conn(ptrans = 0xcda2d8, server = 0x4bf600, csd = 0xcda358, id\n= 360, sbh = 0xcda378, alloc = 0xd147e8), line 3895 in \"core.c\"\n  [4] ap_run_create_connection(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0x45fe03\n  [5] process_socket(p = 0xcda2d8, sock = 0xcda358, my_child_num = 0,\nmy_thread_num = 360, bucket_alloc = 0xd147e8), line 542 in \"worker.c\"\n  [6] worker_thread(thd = 0x7192f8, dummy = 0x7e45a0), line 894 in \"worker.c\"\n  [7] dummy_worker(opaque = 0x7192f8), line 142 in \"thread.c\"\n  [8] _thr_setup(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5d8f7\n  [9] _lwp_start(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5dba0\n\nI tried httpd-2.2.8 too but got the similar crash :\nCrash 3 (with httpd-2.2.8):\n=>[1] apr_palloc(pool = 0x226800079e7a25ff, size = 18446744073323675656U), line\n630 in \"apr_pools.c\"\n  [2] apr_sockaddr_ip_get(addr = 0xc57060, sockaddr = 0x42dab8), line 104 in\n\"sockaddr.c\"\n  [3] core_create_conn(ptrans = 0xc56f68, server = 0x4c0378, csd = 0xc56fe8, id\n= 951, sbh = 0xc57008, alloc = 0xc58f78), line 3895 in \"core.c\"\n  [4] ap_run_create_connection(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0x4604e3\n  [5] process_socket(p = 0xc56f68, sock = 0xc56fe8, my_child_num = 1,\nmy_thread_num = 451, bucket_alloc = 0xc58f78), line 542 in \"worker.c\"\n  [6] worker_thread(thd = 0x870c88, dummy = 0x7e7e30), line 894 in \"worker.c\"\n  [7] dummy_worker(opaque = 0x870c88), line 142 in \"thread.c\"\n  [8] _thr_setup(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5d8f7\n  [9] _lwp_start(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5dba0\n\nprefork mpm works just fine."}, {"attachment_id": null, "tags": [], "creator": "basant.kukreja@sun.com", "text": "I tried to debug the crash \n=>[1] allocator_free(allocator = 0x101f870, node = (nil)), line 331 in \"apr_pools.c\"\n  [2] apr_pool_clear(pool = 0x102fb88), line 710 in \"apr_pools.c\"\n  [3] ap_core_output_filter(f = 0x1020550, b = 0x101f9e8), line 899 in\n\"core_filters.c\"\n\nIn ap_core_output_filter, crash is happening when apr_pool_clear is called for\ndeferred_write_pool.\n            apr_pool_clear(ctx->deferred_write_pool);\nOn further investigation, I found that for ctx->deferred_write_pool, pool->ref\npoints to pool->next i.e\npool->ref == &pool->next\n\nThus in apr_pool_clear :\n    if (active->next == active)\n        return;\n\n    *active->ref = NULL; // ---> this cause active->next to set to NULL because\n                         // active->ref points to active->next\n    allocator_free(pool->allocator, active->next);\n\nThe situation doesn't arrive on normal single connection situation. This\nhappens only under stress. Under normal connection active->next == active and\nfunction returns from apr_pool_clear (when called for ctx->deferred_write_pool)\n", "count": 1, "id": 113673, "time": "2008-02-12T17:14:03Z", "bug_id": 44402, "creation_time": "2008-02-12T17:14:03Z", "is_private": false}, {"count": 2, "attachment_id": null, "bug_id": 44402, "text": "Crashes are happening on 32 bit apache too therefore changing the summary.", "id": 113674, "time": "2008-02-12T19:15:42Z", "creator": "basant.kukreja@sun.com", "creation_time": "2008-02-12T19:15:42Z", "tags": [], "is_private": false}, {"text": "Here is the crash from 32 bit apache :\n=>[1] allocator_free(allocator = 0x8aae018, node = (nil)), line 331 in \"apr_pools.c\"\n  [2] apr_pool_clear(pool = 0x8b629b8), line 710 in \"apr_pools.c\"\n  [3] ap_core_output_filter(f = 0x8aae870, b = 0x8aae0e0), line 899 in\n\"core_filters.c\"\n  [4] ap_pass_brigade(next = 0x8aae870, bb = 0x8aae0e0), line 526 in \"util_filter.c\"\n  [5] logio_out_filter(f = 0x8aae830, bb = 0x8aae0e0), line 135 in \"mod_logio.c\"\n  [6] ap_pass_brigade(next = 0x8aae830, bb = 0x8aae0e0), line 526 in \"util_filter.c\"\n  [7] ap_flush_conn(c = 0x8aae390), line 84 in \"connection.c\"\n  [8] ap_lingering_close(c = 0x8aae390), line 123 in \"connection.c\"\n  [9] process_socket(p = 0x8aae0a0, sock = 0x8aae0e0, my_child_num = 1,\nmy_thread_num = 249, bucket_alloc = 0x8b5c9a0), line 545 in \"worker.c\"\n  [10] worker_thread(thd = 0x81a6788, dummy = 0x831f5a0), line 894 in \"worker.c\"\n  [11] dummy_worker(opaque = 0x81a6788), line 142 in \"thread.c\"\n  [12] _thr_setup(0xf004d200), at 0xfec6f282\n  [13] _lwp_start(0xfee7ddb9, 0xfee7f55e, 0xfffffff6, 0x0, 0x1, 0xfeea1984), at\n0xfec6f4e0\n", "tags": [], "creator": "basant.kukreja@sun.com", "attachment_id": null, "count": 3, "id": 113675, "time": "2008-02-12T19:17:58Z", "bug_id": 44402, "creation_time": "2008-02-12T19:17:58Z", "is_private": false}, {"count": 4, "tags": [], "creator": "basant.kukreja@sun.com", "attachment_id": null, "text": "Here is the debug information from a crash of 32 bit apache :\n\nt@414 (l@414) terminated by signal SEGV (Segmentation Fault)\nCurrent function is apr_sockaddr_ip_get\n  104       *addr = apr_palloc(sockaddr->pool, sockaddr->addr_str_len);\n(dbx) where\ncurrent thread: t@414\n=>[1] apr_sockaddr_ip_get(addr = 0x974a3d0, sockaddr = (nil)), line 104 in\n\"sockaddr.c\"\n  [2] core_create_conn(ptrans = 0x974a348, server = 0x80d9020, csd = 0x974a388,\nid = 411, sbh = 0x974a398, alloc = 0x9788670), line 3895 in \"core.c\"\n  [3] ap_run_create_connection(0x974a348, 0x80d9020, 0x974a388, 0x19b,\n0x974a398, 0x9788670), at 0x8090ae8\n  [4] process_socket(p = 0x974a348, sock = 0x974a388, my_child_num = 0,\nmy_thread_num = 411, bucket_alloc = 0x9788670), line 542 in \"worker.c\"\n  [5] worker_thread(thd = 0x83a6ff8, dummy = 0x8125e80), line 894 in \"worker.c\"\n  [6] dummy_worker(opaque = 0x83a6ff8), line 142 in \"thread.c\"\n  [7] _thr_setup(0xf008e200), at 0xfec6f282\n  [8] _lwp_start(0x0, 0xfee8410c, 0xe451bef8, 0xe451bef8, 0x8081a83, 0x974a3d0),\nat 0xfec6f4e0\n(dbx) p sockaddr\nsockaddr = (nil)\n(dbx) where\ncurrent thread: t@414\n=>[1] apr_sockaddr_ip_get(addr = 0x974a3d0, sockaddr = (nil)), line 104 in\n\"sockaddr.c\"\n  [2] core_create_conn(ptrans = 0x974a348, server = 0x80d9020, csd = 0x974a388,\nid = 411, sbh = 0x974a398, alloc = 0x9788670), line 3895 in \"core.c\"\n  [3] ap_run_create_connection(0x974a348, 0x80d9020, 0x974a388, 0x19b,\n0x974a398, 0x9788670), at 0x8090ae8\n  [4] process_socket(p = 0x974a348, sock = 0x974a388, my_child_num = 0,\nmy_thread_num = 411, bucket_alloc = 0x9788670), line 542 in \"worker.c\"\n  [5] worker_thread(thd = 0x83a6ff8, dummy = 0x8125e80), line 894 in \"worker.c\"\n  [6] dummy_worker(opaque = 0x83a6ff8), line 142 in \"thread.c\"\n  [7] _thr_setup(0xf008e200), at 0xfec6f282\n  [8] _lwp_start(0x0, 0xfee8410c, 0xe451bef8, 0xe451bef8, 0x8081a83, 0x974a3d0),\nat 0xfec6f4e0\n(dbx) up\nCurrent function is core_create_conn\n 3895       apr_sockaddr_ip_get(&c->local_ip, c->local_addr);\n(dbx) p *c\n*c = {\n    pool                  = 0x974a348\n    base_server           = (nil)\n    vhost_lookup_data     = (nil)\n    local_addr            = (nil)\n    remote_addr           = (nil)\n    remote_ip             = (nil)\n    remote_host           = (nil)\n    remote_logname        = (nil)\n    aborted               = 0\n    keepalive             = AP_CONN_UNKNOWN\n    double_reverse        = 0\n    keepalives            = 0\n    local_ip              = (nil)\n    local_host            = (nil)\n    id                    = 0\n    conn_config           = 0x974a400\n    notes                 = 0x974a6a0\n    input_filters         = (nil)\n    output_filters        = (nil)\n    sbh                   = 0x974a398\n    bucket_alloc          = (nil)\n    cs                    = (nil)\n    data_in_input_filters = 0\n}\n(dbx) dump\nalloc = 0x9788670\nrv = 0\nptrans = 0x974a348\nserver = 0x80d9020\nsbh = 0x974a398\nc = 0x974a3a0\nid = 411\ncsd = 0x974a388\n(dbx) _arch_networkio.h`struct apr_socket_t*)csd                              <\n*((struct apr_socket_t *) csd) = {\n    pool                    = (nil)\n    socketdes               = 158893680\n    type                    = -17726080\n    protocol                = 134660748\n    local_addr              = (nil)\n    remote_addr             = 0x19b\n    timeout                 = 158638920LL\n    local_port_unknown      = 0\n    local_interface_unknown = 0\n    remote_addr_unknown     = 0\n    options                 = 0\n    inherit                 = 0\n    userdata                = (nil)\n}\n\nPlease let me know if any other information is need.\n", "id": 113676, "time": "2008-02-12T19:43:51Z", "bug_id": 44402, "creation_time": "2008-02-12T19:43:51Z", "is_private": false}, {"count": 5, "tags": [], "bug_id": 44402, "attachment_id": null, "id": 113725, "time": "2008-02-13T13:03:06Z", "creator": "rpluem@apache.org", "creation_time": "2008-02-13T13:03:06Z", "is_private": false, "text": "First guess for your last crash in comment #4 (all line numbers 2.2.8):\n\nlr->accept_func(&csd, lr, ptrans); (line 742 in worker.c) fails with\nrv != APR_SUCCESS, but with a non NULL value for csd.\nIn contrast to the code in prefork we don't check this situation:\n\nLines 621 - 631 of prefork.c:\n\n        status = lr->accept_func(&csd, lr, ptrans);\n\n        SAFE_ACCEPT(accept_mutex_off());      /* unlock after \"accept\" */\n\n        if (status == APR_EGENERAL) {\n            /* resource shortage or should-not-occur occured */\n            clean_child_exit(1);\n        }\n        else if (status != APR_SUCCESS) {\n            continue;\n        }\n\nMaybe we need to do a continue in the worker case as well or we need to do\nsomething like the following:\n\nIndex: server/mpm/worker/worker.c\n===================================================================\n--- server/mpm/worker/worker.c  (Revision 627576)\n+++ server/mpm/worker/worker.c  (Arbeitskopie)\n@@ -743,6 +743,9 @@\n             /* later we trash rv and rely on csd to indicate success/failure */\n             AP_DEBUG_ASSERT(rv == APR_SUCCESS || !csd);\n\n+            if (rv != APR_SUCCESS) {\n+                csd = NULL;\n+            }\n             if (rv == APR_EGENERAL) {\n                 /* E[NM]FILE, ENOMEM, etc */\n                 resource_shortage = 1;\n\n\n\n"}, {"count": 6, "tags": [], "creator": "basant.kukreja@sun.com", "attachment_id": null, "text": "I did the stress test with the patch you suggested. After your patch, I\nstill got the 1st crash. If it crashed in second stack trace then I will update\nthe bug.\n\nHere are some more information about 1st crash.\n* I am able to reproduce the crash on Solaris 10 update 1 (on a different\n  machine) too. It took around 4 hours of stress before I got the crash on\n  Solaris 10 while it takes around < 30 minutes to reproduce on Solaris nevada.\n  It was crash 1 (allocator_free with node = null) (without your patch).\n\nHere is more information of the crash 1 :\n(dbx) where\ncurrent thread: t@21\n=>[1] allocator_free(allocator = 0x8afe2e0, node = (nil)), line 331 in \"apr_pools.c\"\n  [2] apr_pool_clear(pool = 0xa0d01c0), line 710 in \"apr_pools.c\"\n  [3] ap_core_output_filter(f = 0xa0b28c8, b = 0xa0b2a08), line 899 in\n\"core_filters.c\"\n  [4] ap_pass_brigade(next = 0xa0b28c8, bb = 0xa0b2a08), line 526 in \"util_filter.c\"\n  [5] logio_out_filter(f = 0xa0b2888, bb = 0xa0b2a08), line 135 in \"mod_logio.c\"\n  [6] ap_pass_brigade(next = 0xa0b2888, bb = 0xa0b2a08), line 526 in \"util_filter.c\"\n  [7] ap_flush_conn(c = 0xa0b23e8), line 84 in \"connection.c\"\n  [8] ap_lingering_close(c = 0xa0b23e8), line 123 in \"connection.c\"\n  [9] process_socket(p = 0x8afe368, sock = 0x8aff660, my_child_num = 1,\nmy_thread_num = 18, bucket_alloc = 0xa0be178), line 545 in \"worker.c\"\n  [10] worker_thread(thd = 0x81487d8, dummy = 0x8117b30), line 894 in \"worker.c\"\n  [11] dummy_worker(opaque = 0x81487d8), line 142 in \"thread.c\"\n  [12] _thr_setup(0xfe244800), at 0xfeccf92e\n  [13] _lwp_start(), at 0xfeccfc10\n(dbx) up\nCurrent function is apr_pool_clear\n  710       allocator_free(pool->allocator, active->next);\n(dbx) p *active\n*active = {\n    next        = (nil)\n    ref         = 0xa0d01a8\n    index       = 1U\n    free_index  = 0\n    first_avail = 0xa0d01f8 \"\\xc0^A^M\\n\\xfc^A^M\\n\\xfc^A^M\\nx\\xe1^K\\n\"\n    endp        = 0xa0d21a8 \"^A \"\n}\n(dbx) up\nCurrent function is ap_core_output_filter\n  899               apr_pool_clear(ctx->deferred_write_pool);\n(dbx) p *ctx\n*ctx = {\n    b                   = (nil)\n    deferred_write_pool = 0xa0d01c0\n}\n(dbx) p *ctx->deferred_write_pool\n*ctx->deferred_write_pool = {\n    parent           = 0x8afe368\n    child            = (nil)\n    sibling          = 0xa0c6198\n    ref              = 0x8afe36c\n    cleanups         = (nil)\n    free_cleanups    = (nil)\n    allocator        = 0x8afe2e0\n    subprocesses     = (nil)\n    abort_fn         = (nil)\n    user_data        = (nil)\n    tag              = 0x80bfd1c \"deferred_write\"\n    active           = 0xa0d01a8\n    self             = 0xa0d01a8\n    self_first_avail = 0xa0d01f8 \"\\xc0^A^M\\n\\xfc^A^M\\n\\xfc^A^M\\nx\\xe1^K\\n\"\n}\n(dbx) p *c\n*c = {\n    pool                  = 0x8afe368\n    base_server           = 0x80e6bf8\n    vhost_lookup_data     = (nil)\n    local_addr            = 0x8aff698\n    remote_addr           = 0x8aff7c0\n    remote_ip             = 0xa0b2850 \"192.168.11.1\"\n    remote_host           = (nil)\n    remote_logname        = (nil)\n    aborted               = 0\n    keepalive             = AP_CONN_KEEPALIVE\n    double_reverse        = 0\n    keepalives            = 1\n    local_ip              = 0xa0b2840 \"192.168.11.2\"\n    local_host            = (nil)\n    id                    = 518\n    conn_config           = 0xa0b2448\n    notes                 = 0xa0b26e8\n    input_filters         = 0xa0b2870\n    output_filters        = 0xa0b2888\n    sbh                   = 0xa0b23e0\n    bucket_alloc          = 0xa0be178\n    cs                    = (nil)\n    data_in_input_filters = 0\n}\n\nOne putting some printfs I figured out the following :\n\nIn apr_pool_clear (when invoked for deferred_write_pool)\n    ...\n    active = pool->active = pool->self;\n    active->first_avail = pool->self_first_avail;\n\n    if (active->next == active)\n        return;\n\nactive->next should typically be s circular link list. What is happenning some\ncases is that active->next points to some thing else and active->ref still\npoints to active->next.  I put a printf of active->next before it is set to\nNULL. For a particular crash, here is my debugging session. I found that\nactive->next\nwas set to 0x20e8810 before it was set to NULL.\n\n(dbx) up\nCurrent function is apr_pool_clear\n  774       allocator_free(pool->allocator, active->next);\n(dbx) up\nCurrent function is ap_core_output_filter\n  923               apr_pool_clear(ctx->deferred_write_pool);\n(dbx) p (struct apr_memnode_t*)0x20e8810 -----> This was active->next before set\nto NULL.\n(struct apr_memnode_t *) 0x20e8810 = 0x20e8810\n(dbx) p *(struct apr_memnode_t*)0x20e8810\n*((struct apr_memnode_t *) 0x20e8810) = {\n    next        = 0x288c5b0\n    ref         = 0x20e8810\n    index       = 1U\n    free_index  = 0\n    first_avail = 0x20e9eb0 \"GET /file_set/dir00104/class1_3 HTTP/1.0\"\n    endp        = 0x20ea810 \"^A \"\n}\n(dbx) down\nCurrent function is apr_pool_clear\n  774       allocator_free(pool->allocator, active->next);\n(dbx) p active\nactive = 0x20e27e0\n(dbx) p *((struct apr_memnode_t*)0x20e8810)->next\n*((struct apr_memnode_t *) 0x20e8810)->next = {\n    next        = 0x20e07d0\n    ref         = 0x20e07d0\n    index       = 1U\n    free_index  = 0\n    first_avail = 0x288d008 \"\"\n    endp        = 0x288e5b0 \"^A \"\n}\n(dbx) p active\nactive = 0x20e27e0\n(dbx) p *(((struct apr_memnode_t*)0x20e8810)->next)->next\n*((struct apr_memnode_t *) 0x20e8810)->next->next = {\n    next        = 0x28905d0\n    ref         = 0x288c5b0\n    index       = 1U\n    free_index  = 0\n    first_avail = 0x20e2738 \"\"\n    endp        = 0x20e27d0 \"^A \"\n}\n(dbx) p *((((struct apr_memnode_t*)0x20e8810)->next)->next)->next\n*((struct apr_memnode_t *) 0x20e8810)->next->next->next = {\n    next        = 0x288e5c0\n    ref         = 0x28905d0\n    index       = 1U\n    free_index  = 0\n    first_avail = 0x2890668 \"\\xf8^E\\x89^B\"\n    endp        = 0x28925d0 \"^Q^P\"\n}\n(dbx) p *(((((struct apr_memnode_t*)0x20e8810)->next)->next)->next)->next\n*((struct apr_memnode_t *) 0x20e8810)->next->next->next->next = {\n    next        = (nil)\n    ref         = (nil)\n    index       = 1U\n    free_index  = 0\n    first_avail = 0x288e5e8 \"`^_\"\n    endp        = 0x28905c0 \"^A \"\n}\n\nOn further debugging, I figured out that typically ap_core_output_filter is\ncalled 4 times for a request. The crash always happen in 4th invocation. It\nseems to me that it gets corrupted somewhere after the 3rd invocation (after it\nreturns from ap_core_output_filter) and before it enters into\nap_core_output_filter 4th time (when ap_lingering_close is in call stack). Also\nconn->keepalives was always set to 1.\n", "id": 113734, "time": "2008-02-13T22:56:02Z", "bug_id": 44402, "creation_time": "2008-02-13T22:56:02Z", "is_private": false}, {"count": 7, "tags": [], "bug_id": 44402, "text": "With Ruediger patch, I still got the crash (#2). Here is the debug information :\n\nt@314 (l@314) terminated by signal SEGV (Segmentation Fault)\nCurrent function is apr_sockaddr_ip_get\n  104       *addr = apr_palloc(sockaddr->pool, sockaddr->addr_str_len);\n(dbx) where\ncurrent thread: t@314\n=>[1] apr_sockaddr_ip_get(addr = 0x1ebb4b0, sockaddr = (nil)), line 104 in\n\"sockaddr.c\"\n  [2] core_create_conn(ptrans = 0x1ebb3b8, server = 0x4c0200, csd = 0x1ebb728,\nid = 311, sbh = 0x1ebb458, alloc = 0x2128b58), line 3895 in \"core.c\"\n  [3] ap_run_create_connection(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0x4602c3\n  [4] process_socket(p = 0x1ebb3b8, sock = 0x1ebb728, my_child_num = 0,\nmy_thread_num = 311, bucket_alloc = 0x2128b58), line 566 in \"worker.c\"\n  [5] worker_thread(thd = 0x7195c8, dummy = 0x6e2310), line 923 in \"worker.c\"\n  [6] dummy_worker(opaque = 0x7195c8), line 142 in \"thread.c\"\n  [7] _thr_setup(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5d8f7\n  [8] _lwp_start(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5dba0\n(dbx) p *addr\n*addr = (nil)\n(dbx) up\nCurrent function is core_create_conn\n 3895       apr_sockaddr_ip_get(&c->local_ip, c->local_addr);\n(dbx) p *c\n*c = {\n    pool                  = 0x1ebb3b8\n    base_server           = (nil)\n    vhost_lookup_data     = (nil)\n    local_addr            = (nil)\n    remote_addr           = (nil)\n    remote_ip             = (nil)\n    remote_host           = (nil)\n    remote_logname        = (nil)\n    aborted               = 0\n    keepalive             = AP_CONN_UNKNOWN\n    double_reverse        = 0\n    keepalives            = 0\n    local_ip              = (nil)\n    local_host            = (nil)\n    id                    = 0\n    conn_config           = 0x1ebb508\n    notes                 = 0x1ebba48\n    input_filters         = (nil)\n    output_filters        = (nil)\n    sbh                   = 0x1ebb458\n    bucket_alloc          = (nil)\n    cs                    = (nil)\n    data_in_input_filters = 0\n}\n(dbx) dump\nalloc = 0x2128b58\nrv = 0\nptrans = 0x1ebb3b8\nserver = 0x4c0200\nsbh = 0x1ebb458\nc = 0x1ebb460\nid = 311\ncsd = 0x1ebb728\n(dbx) p csd\ncsd = 0x1ebb728\n(dbx) p *(struct apr_socket_t*) csd\n*((struct apr_socket_t *) csd) = {\n    pool                    = (nil)\n    socketdes               = 0\n    type                    = 0\n    protocol                = 0\n    local_addr              = (nil)\n    remote_addr             = (nil)\n    timeout                 = 0\n    local_port_unknown      = 0\n    local_interface_unknown = 0\n    remote_addr_unknown     = 0\n    options                 = 0\n    inherit                 = 0\n    userdata                = (nil)\n}\n \n", "id": 113735, "time": "2008-02-14T00:03:50Z", "creator": "basant.kukreja@sun.com", "creation_time": "2008-02-14T00:03:50Z", "is_private": false, "attachment_id": null}, {"count": 8, "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "text": "I assume that the ptrans pool somehow gets corrupted. I guess it is used by two\nthreads in parallel which could lead to a corruption since pools as such are not\nthread safe. So I think a good starting point for further investigations would be \n\nap_queue_info_wait_for_idler in mpm/worker/fdqueue.c\n\nor the lines 731 - 740 in worker.c:\n\n            if (ptrans == NULL) {\n                /* we can't use a recycled transaction pool this time.\n                 * create a new transaction pool */\n                apr_allocator_t *allocator;\n\n                apr_allocator_create(&allocator);\n                apr_allocator_max_free_set(allocator, ap_max_mem_free);\n                apr_pool_create_ex(&ptrans, pconf, NULL, allocator);\n                apr_allocator_owner_set(allocator, ptrans);\n            }\n\n", "id": 113758, "time": "2008-02-14T12:25:52Z", "bug_id": 44402, "creation_time": "2008-02-14T12:25:52Z", "is_private": false}, {"count": 9, "tags": [], "text": "Thanks Ruediger for your suggestion. I will try to explore based on your\nsuggestion.\n\nMeanwhile here is the 3rd type of crash (with your patch).\n\nt@13 (l@13) terminated by signal SEGV (Segmentation Fault)\nCurrent function is apr_pool_cleanup_kill\n 2045       c = p->cleanups;\n(dbx) where\ncurrent thread: t@13\n=>[1] apr_pool_cleanup_kill(p = 0xa0, data = 0x195b888, cleanup_fn =\n0xfffffd7fff223540 = &`libapr-1.so.0.2.11`sockets.c`socket_cleanup(void *sock)),\nline 2045 in \"apr_pools.c\"\n  [2] apr_pool_cleanup_run(p = 0xa0, data = 0x195b888, cleanup_fn =\n0xfffffd7fff223540 = &`libapr-1.so.0.2.11`sockets.c`socket_cleanup(void *sock)),\nline 2088 in \"apr_pools.c\"\n  [3] apr_socket_close(thesocket = 0x195b888), line 149 in \"sockets.c\"\n  [4] ap_lingering_close(c = 0x17407f0), line 135 in \"connection.c\"\n  [5] process_socket(p = 0x1740748, sock = 0x195b888, my_child_num = 1,\nmy_thread_num = 10, bucket_alloc = 0x195b728), line 569 in \"worker.c\"\n  [6] worker_thread(thd = 0x52bb48, dummy = 0x4f3480), line 951 in \"worker.c\"\n  [7] dummy_worker(opaque = 0x52bb48), line 142 in \"thread.c\"\n  [8] _thr_setup(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5d8f7\n  [9] _lwp_start(0x0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xfffffd7ffef5dba0\n(dbx) up\nCurrent function is apr_pool_cleanup_run\n 2088       apr_pool_cleanup_kill(p, data, cleanup_fn);\n(dbx) up\nCurrent function is apr_socket_close\n  149       return apr_pool_cleanup_run(thesocket->pool, thesocket, socket_cleanup);\n(dbx) p *thesocket\n*thesocket = {\n    pool                    = 0xa0\n    socketdes               = 26588968\n    type                    = 0\n    protocol                = 26588928\n    local_addr              = 0x195b7e8\n    remote_addr             = 0x1741158\n    timeout                 = 24383832\n    local_port_unknown      = 4895848\n    local_interface_unknown = 0\n    remote_addr_unknown     = 0\n    options                 = 0\n    inherit                 = 0\n    userdata                = (nil)\n}\n(dbx) dump\nthesocket = 0x195b888\n(dbx) up\nCurrent function is ap_lingering_close\n  135           apr_socket_close(csd);\n(dbx) dump\ntimeup = 0\ndummybuf = \"\"\nc = 0x17407f0\nnbytes = 4294967296U\ncsd = 0x195b888\n(dbx) p *c\n*c = {\n    pool                  = 0x1740748\n    base_server           = 0x4c0300\n    vhost_lookup_data     = (nil)\n    local_addr            = 0x195b8d8\n    remote_addr           = 0x195ba18\n    remote_ip             = 0x1740f88 \"192.168.22.2\"\n    remote_host           = (nil)\n    remote_logname        = (nil)\n    aborted               = 0\n    keepalive             = AP_CONN_UNKNOWN\n    double_reverse        = 0\n    keepalives            = 0\n    local_ip              = 0x1740f78 \"192.168.22.1\"\n    local_host            = (nil)\n    id                    = 510\n    conn_config           = 0x1740898\n    notes                 = 0x1740dd8\n    input_filters         = 0x1740fa8\n    output_filters        = 0x1740fd0\n    sbh                   = 0x17407e8\n    bucket_alloc          = 0x195b728\n    cs                    = (nil)\n    data_in_input_filters = 0\n}\n(dbx) _arch_networkio.h`struct apr_socket_t*)csd                              <\n*((struct apr_socket_t *) csd) = {\n    pool                    = 0xa0\n    socketdes               = 26588968\n    type                    = 0\n    protocol                = 26588928\n    local_addr              = 0x195b7e8\n    remote_addr             = 0x1741158\n    timeout                 = 24383832\n    local_port_unknown      = 4895848\n    local_interface_unknown = 0\n    remote_addr_unknown     = 0\n    options                 = 0\n    inherit                 = 0\n    userdata                = (nil)\n}\n(dbx) etworkio.h`struct apr_socket_t*)csd->local_addr                         <\ndbx: can't find field \"local_addr\" in \"*(csd)\"\n(dbx) p ((`srclib/apr/include/arch/unix/apr_arch_networkio.h`struct apr_socke >\n((struct apr_socket_t *) csd)->local_addr = 0x195b7e8\n(dbx) p *((`srclib/apr/include/arch/unix/apr_arch_networkio.h`struct apr_sock >\n*((struct apr_socket_t *) csd)->local_addr = {\n    pool         = 0xa0\n    hostname     = 0x195b728 \"H^Gt^A\"\n    servname     = 0x195b700 \"\"\n    port         = 0\n    family       = 0\n    salen        = 24383744U\n    ipaddr_len   = 0\n    addr_str_len = 24383744\n    ipaddr_ptr   = 0xfffffd7fff2e8010\n    next         = (nil)\n    sa           = {\n        sin  = {\n            sin_family = 0\n            sin_port   = 0\n            sin_addr   = {\n                S_un = {\n                    S_un_b = {\n                        s_b1 = '\\0'\n                        s_b2 = '\\0'\n                        s_b3 = '\\0'\n                        s_b4 = '\\0'\n                    }\n                    S_un_w = {\n                        s_w1 = 0\n                        s_w2 = 0\n                    }\n                    S_addr = 0\n                }\n            }\n            sin_zero   = \"\"\n        }\n        sin6 = {\n            sin6_family   = 0\n            sin6_port     = 0\n            sin6_flowinfo = 0\n            sin6_addr     = {\n                _S6_un = {\n                    _S6_u8     = \"\"\n                    _S6_u32    = (0, 0, 4373928U, 0)\n                    __S6_align = 0\n                }\n            }\n            sin6_scope_id = 26588968U\n            __sin6_src_id = 0\n        }\n        sas  = {\n            ss_family = 0\n            _ss_pad1  = \"\"\n            _ss_align = 0.0\n            _ss_pad2  = \"xxB\"\n        }\n    }\n}\n", "attachment_id": null, "bug_id": 44402, "id": 113763, "time": "2008-02-14T14:15:27Z", "creator": "basant.kukreja@sun.com", "creation_time": "2008-02-14T14:15:27Z", "is_private": false}, {"count": 10, "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "text": "As you wrote Solaris on Sun I suppose you mean on SPARC. Have you checked if the\ncrashes happen with the same Solaris version on x86? Background of the question:\nap_queue_info_wait_for_idler uses atomics whose implementation depends on the\nhardware architecture. Non functional atomics could be a source for concurrency\nproblems under load.", "id": 113765, "time": "2008-02-14T14:44:20Z", "bug_id": 44402, "creation_time": "2008-02-14T14:44:20Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "rpluem@apache.org", "text": "(In reply to comment #10)\n> As you wrote Solaris on Sun I suppose you mean on SPARC. Have you checked if the\n> crashes happen with the same Solaris version on x86? Background of the question:\n\nOops my fault: You already said that you are using x86. Nevertheless does the\nsame happen on SPARC with the same Solaris version or if you compile with \n--enable-nonportable-atomics=no ?\n", "count": 11, "id": 113766, "time": "2008-02-14T14:55:43Z", "bug_id": 44402, "creation_time": "2008-02-14T14:55:43Z", "is_private": false}, {"text": "Thanks Ruediger for your pointer. It was really useful.\n\nRegarding the function : ap_queue_info_wait_for_idler (lines 188-196)\n188:        struct recycled_pool *first_pool = queue_info->recycled_pools;\n189:        if (first_pool == NULL) {\n190:            break;\n191:        }\n192:        if (apr_atomic_casptr((volatile\nvoid**)&(queue_info->recycled_pools), first_pool->next,\n193:                              first_pool) == first_pool) {\n194:            *recycled_pool = first_pool->pool;\n195:            break;\n196:        }\n\nI will represent queue_info->receycled_pools as qu->rp to make it make it\nshort.  Inside apr_atomic_casptr we acquire a mutex. So I will write 3 steps :\n1. Calcualte first_pool.\n2. Calculate first_pool->next and invoke apr_atomic_caspptr\n3. Acquire lock on qi->rp (inside apr_atomic_casptr)\n\nThere is a very clear race condition between the two statement (line 188 and\nline 193) and between step 2 & 3. Though I agree that\n&queue_info->recycled_pool is protected and it is atomically correct but the\nnext pointer (first_pool and first_pool->next) are not protected correctly.\nThere is a very clear race condition between the two. To prove my point, here\nis an example :\n\nSuppose at a particular moment recycled_pool pool list is \n1 --> 2 ---> 3 . Where 1,2,3 are the pool nodes. qi->rp = 1. Now consider the\nfollowing situation :\nThread 1 :\n    first_pool = 1;\n    first_pool->next = 2.\n    \nNow before step 3 is executed that is before we acquire a lock on qi->rp,\ncontext switch happens.\n\nThread 2 :\n    Thread 2 pops a node (1) from the list and hence list becomes 2->3.\n\nThread 3 :\n    Thread 3 pops another node (2) from the list and hence list becomes 3.\n\nThread 2 :\n    push the node back and now list becomes 1->3.\n\nThread 1:\n    first->pool->next = 2.  qi->rp is still 1.  Thread acquires a lock &1 and\natomically compare and swap with 2. It succeeded because qi->rp was 1 but\nqi->rp->next was not 3, it becomes 2 and hence queue becomes 2 (or 2-->3).\n        \nI believe, I can prove my point with a sample standalone application. So far I\nused a separate mutex and protected both qi->rp and qi->rp->next both.  I tried\nwith the attached patch. With this patch, I am able to run the stress for more\nthan 10 hour without any crash. Without this patch, crash used to happen in\nless than 30 minutes. Here is the patch which I tried :\n---------------------------------------------------------------------------\n\n--- orghttpd-2.2.6/server/mpm/worker/fdqueue.c\tWed Jul 25 06:13:49 2007\n+++ httpd-2.2.6/server/mpm/worker/fdqueue.c\tFri Feb 15 10:57:42 2008\n@@ -25,6 +25,7 @@\n struct fd_queue_info_t {\n     apr_uint32_t idlers;\n     apr_thread_mutex_t *idlers_mutex;\n+    apr_thread_mutex_t *queue_mutex;\n     apr_thread_cond_t *wait_for_idler;\n     int terminated;\n     int max_idlers;\n@@ -36,6 +37,7 @@\n     fd_queue_info_t *qi = data_;\n     apr_thread_cond_destroy(qi->wait_for_idler);\n     apr_thread_mutex_destroy(qi->idlers_mutex);\n+    apr_thread_mutex_destroy(qi->queue_mutex);\n \n     /* Clean up any pools in the recycled list */\n     for (;;) {\n@@ -65,6 +67,11 @@\n     if (rv != APR_SUCCESS) {\n         return rv;\n     }\n+    rv = apr_thread_mutex_create(&qi->queue_mutex, APR_THREAD_MUTEX_DEFAULT,\n+                                 pool);\n+    if (rv != APR_SUCCESS) {\n+        return rv;\n+    }\n     rv = apr_thread_cond_create(&qi->wait_for_idler, pool);\n     if (rv != APR_SUCCESS) {\n         return rv;\n@@ -93,14 +100,14 @@\n         new_recycle = (struct recycled_pool *)apr_palloc(pool_to_recycle,\n                                                          sizeof(*new_recycle));\n         new_recycle->pool = pool_to_recycle;\n-        for (;;) {\n-            new_recycle->next = queue_info->recycled_pools;\n-            if (apr_atomic_casptr((volatile void**)&(queue_info->recycled_pools),\n-                                  new_recycle, new_recycle->next) ==\n-                new_recycle->next) {\n-                break;\n-            }\n-        }\n+        rv = apr_thread_mutex_lock(queue_info->queue_mutex);\n+        if (rv != APR_SUCCESS)\n+            return rv;\n+        new_recycle->next = queue_info->recycled_pools;\n+        queue_info->recycled_pools = new_recycle;\n+        rv = apr_thread_mutex_unlock(queue_info->queue_mutex);\n+        if (rv != APR_SUCCESS)\n+            return rv;\n     }\n \n     /* Atomically increment the count of idle workers */\n@@ -182,19 +189,18 @@\n \n     /* Atomically decrement the idle worker count */\n     apr_atomic_dec32(&(queue_info->idlers));\n-\n-    /* Atomically pop a pool from the recycled list */\n-    for (;;) {\n+    rv = apr_thread_mutex_lock(queue_info->queue_mutex);\n+    if (rv != APR_SUCCESS)\n+        return rv;\n+    if (queue_info->recycled_pools) {\n         struct recycled_pool *first_pool = queue_info->recycled_pools;\n-        if (first_pool == NULL) {\n-            break;\n-        }\n-        if (apr_atomic_casptr((volatile void**)&(queue_info->recycled_pools),\nfirst_pool->next,\n-                              first_pool) == first_pool) {\n-            *recycled_pool = first_pool->pool;\n-            break;\n-        }\n+        queue_info->recycled_pools = first_pool->next;\n+        *recycled_pool = first_pool->pool;\n+        first_pool->next = NULL;\n     }\n+    rv = apr_thread_mutex_unlock(queue_info->queue_mutex);\n+    if (rv != APR_SUCCESS)\n+        return rv;\n \n     if (queue_info->terminated) {\n         return APR_EOF;\n---------------------------------------------------------------------------\n\nIf you agree that there is a clear race condition then to correct this, I have\nfollowing suggestion :\n(a) Use a dedicated pool for each worker thread, this will avoid any locking.\nIt will perform better but may require little more memory in those situations\nwhen worker threads are not fully used.\n(b) Use some other technique other than a recycled pool list which avoids race\nconditions.\n\nI am in favour of option (a) until some good idea for (b) comes to my mind.\nIf you agree with (a) then I can work and generate a patch.\n\nNote : Also I believe that the crash will happen in linux too. I never ran more\nthan 1 hour in linux. I will try that tonight.\n", "tags": [], "creator": "basant.kukreja@sun.com", "attachment_id": null, "count": 12, "id": 113800, "time": "2008-02-15T11:48:52Z", "bug_id": 44402, "creation_time": "2008-02-15T11:48:52Z", "is_private": false}, {"count": 13, "tags": [], "bug_id": 44402, "attachment_id": null, "id": 113823, "time": "2008-02-16T00:25:03Z", "creator": "rpluem@apache.org", "creation_time": "2008-02-16T00:25:03Z", "is_private": false, "text": "(In reply to comment #12)\n\n> If you agree that there is a clear race condition then to correct this, I have\n> following suggestion :\n> (a) Use a dedicated pool for each worker thread, this will avoid any locking.\n> It will perform better but may require little more memory in those situations\n> when worker threads are not fully used.\n> (b) Use some other technique other than a recycled pool list which avoids race\n> conditions.\n> \n> I am in favour of option (a) until some good idea for (b) comes to my mind.\n> If you agree with (a) then I can work and generate a patch.\n> \n> Note : Also I believe that the crash will happen in linux too. I never ran more\n> than 1 hour in linux. I will try that tonight.\n> \n\nThank you for your thorough investigation. I agree with you that we have the\ndescribed race conditions here. We have a similar race in the event MPM.\nNext steps:\n\n1. Bring your patch above into trunk. Currently I see no significant performance \n   loss over the current code as we are using a mutex there as well. We only\n   increase the time during which we lock the resource. I don't know right\n   now when I find the cycles to apply the patch to trunk, but if you could\n   attach a trunk version of your patch to this report it would be a big help.\n\n2. Move the further discussion regarding options a) or b) to \n   dev@httpd.apache.org and lets wait for its results to decide how to move \n   along and improve the situation here in the long run.\n \n"}, {"count": 14, "tags": [], "bug_id": 44402, "attachment_id": null, "id": 113828, "time": "2008-02-16T14:23:39Z", "creator": "rpluem@apache.org", "creation_time": "2008-02-16T14:23:39Z", "is_private": false, "text": "I think I have to correct myself in two points.\n\n1. On APR trunk there are better implementations for apr_atomic_casptr which no \n   longer use a mutex, but native platform processor / OS features. So in \n   contrast to my first assumption there could be a performance degradation by\n   your patch on trunk, which would be bad.\n\n2. The race scenario you described cannot happen in this way, because it assumes\n   that multiple threads pop pools from the list in parallel. This is not the \n   case as only the listener thread does this. What happens in parallel are:\n\n   - Multiple pushes to the list\n   - (Multiple) pushes to the list and a pop\n\nOTOH I still believe that there is some kind of race scenario as your patch\nshowed that the error goes away if the locking / syncing is changed here.\nSo maybe its only a different scenario (that I haven't figured out so far) or\nthere is a bug in apr_atomic_casptr.\nDo the same crashes happen with trunk?\n   "}, {"count": 15, "tags": [], "creator": "basant.kukreja@sun.com", "attachment_id": null, "text": "Regarding the example given in comments # 12, I need to correct myself. I\nagree with you that the example is not valid for worker implementation because\nthere is single thread which pop the nodes and multiple threads which pushes\nthe node.  ( ap_queue_info_wait_for_idler is not thread safe but it is not\ncalled by multiple threads. It is only invoked by single listener_thread. )\n\nI could not yet think of any race condition in which single popping thread\nand several pushing thread cause recycle_pool list corruption.\n\nI am still working on it to find the real cause of the crashes.\n\n\n", "id": 113861, "time": "2008-02-18T11:33:55Z", "bug_id": 44402, "creation_time": "2008-02-18T11:33:55Z", "is_private": false}, {"count": 16, "tags": [], "bug_id": 44402, "attachment_id": null, "id": 113999, "time": "2008-02-21T18:24:53Z", "creator": "basant.kukreja@sun.com", "creation_time": "2008-02-21T18:24:53Z", "is_private": false, "text": "Few more updates : \n* Probably these crashes also exist on Linux (64 bit). But I can't say for\nsure. I saw 3 crashes so far. Out of 3, I get core dump only once and stack\ntrace from that core dump didn't seem much sense to me so I can't say for sure\nthat the bug reproduces on Linux or not. (Linux is 64 bit Fedora 8 with 64 bit\napache). \n\nOn Solaris, I tried the following things :\n* Replaced apr_atomic_casptr with solaris's atomic_casptr. But the result\nremained the same. I still saw the crashes. This means that this may not\nbe the apr bug.\n* If I replace apr_atomic_casptr code but keep the for loop then  the crashes\ndisappear.\n---------------------------------- ap_queue_info_set_idle-------------\n            if (apr_atomic_casptr((volatile void**)&(queue_info->recycled_pools),\n                                  new_recycle, new_recycle->next) ==\n                new_recycle->next) {\n                break;\n            }\n---------------------------------- replace with -----------------------\n            rv = apr_thread_mutex_lock(queue_info->queue_mutex);\n            if (queue_info->recycled_pools == new_recycle->next) {\n                queue_info->recycled_pools = new_recycle;\n                success = 1;\n            }\n            rv = apr_thread_mutex_unlock(queue_info->queue_mutex);\n\n\n---------------------------------- ap_queue_info_wait_for_idler --------------\n        if (apr_atomic_casptr((volatile void**)&(queue_info->recycled_pools),\nfirst_pool->next,\n                              first_pool) == first_pool) {\n            *recycled_pool = first_pool->pool;\n            break;\n        }\n---------------------------------- replace with ---------------------------\n        rv = apr_thread_mutex_lock(queue_info->queue_mutex);\n        if (queue_info->recycled_pools == first_pool) {\n            queue_info->recycled_pools = next;\n            success = 1;\n        }\n        rv = apr_thread_mutex_unlock(queue_info->queue_mutex);\n----------------------------------\n"}, {"text": "Created attachment 21581\nPatch for httpd-2.2.8\n\nEventually I figured out where is the real race condition.\n\nLines fdqueue.c:96-102 (in httpd-2.2.8)\n\tfor (;;) {\n\t    new_recycle->next = queue_info->recycled_pools;\n\t    if (apr_atomic_casptr((volatile\nvoid**)&(queue_info->recycled_pools),\n\t\t\t\t  new_recycle, new_recycle->next) ==\n\t\tnew_recycle->next) {\n\t\tbreak;\n\t    }\n\t}\n\nThe race condition is between return of apr_atomic_casptr and calculating\nnew_recycle->next.\n\nLet us write the look into three steps (qi->rp is queue_info->recycled_pools):\n1. Set new_recycle->next to qi->rp\n2. atomically compare and swap qi->rp with new_recycle if matches with\n    new_recycle->next.\n3. Calculate new_recycle->next again.\n4. Determine the call apr_atomic_casptr is successful based on the return value\nand\n   result of step 3.\n\nThe race condition is in between step2 and step3. If apr_atomic_casptr was\nsuccessful (it means it successfully swapped the value) and If there is a\ncontext switch between 2 and 3 then new_recycle->next can point to something\nelse and can also be corrupted. The result of which is that if condition will\nfail.\n\nI saved the new_recycle->next in a local variable and then used the local\nvariable as shown in the patch and the issue got resolved.\n\nHere is the example how new_recycle->next can be changed by a race condition :\nSuppose our list is 1-->2-->3, where 1,2,3 are list nodes. Now suppose worker\nthread 1 wants to add a node 4 to it's head. Here is how it goes :\n\n-----------------------------------------------------------\nWorker thread 1 :\n   new_recycle = 4;\n   qi->rp = 1;\n   new_recycle->next = 1;\n   apr_atomic_casptr successfully compare and swap it with qi->rp that means\n   qi->rp = 4; \n   (list now becomes 4-->1-->2-->3)\n\n   Now context switch happens :\n\nListener_thread : \n   qi->rp is 4 and hence it pops the node 4 and gives it to worker thread 2.\n   The list becomes 1-->2-->3.\n\n   Listener thread pops another node and give it worker thread 3 and now list\n   becomes 2-->3.\n\nWorker thread 2 :\n   Returns the node 4 into the list and list becomes 4-->2-->3.\n\nWorker thread 1 :\n   new_recycle->next now becomes 2 and it compares with 1 and hence\ncomparision fails.\n-----------------------------------------------------------\n\n   Real situations can be little different than what I described because before\n\nworker thread returns node 4 to the list, pool is cleared (line 897 of\nworker.c,\nin worker_thread function )\n\tapr_pool_clear(ptrans);\n\tlast_ptrans = ptrans;\n\nwhich means new_recycle->next will be corrupted and point to a deleted value.\n\nHow I figured out this is that if you put a assert statement like :\n\t    struct recycled_pool *next = queue_info->recycled_pools;\n\t    new_recycle->next = next;\n\t    if (apr_atomic_casptr((volatile\nvoid**)&(queue_info->recycled_pools),\n\t\t\t\t  new_recycle, new_recycle->next) == next) {\n\t\tap_assert(next == new_recycle->next);\n\t\tbreak;\n\t    }\nthen assertion fails under stress situations.\n\nThe bug also exist in event mpm too (server/mpm/experimental/event/fdqueue.c).\n\nRuediger, can you review the patch? The patch is against 2.2.8. Should I submit\n\npatch against trunk?", "tags": [], "creator": "basant.kukreja@sun.com", "attachment_id": 21581, "count": 17, "id": 113321, "time": "2008-02-22T12:28:17Z", "bug_id": 44402, "creation_time": "2008-02-22T12:28:17Z", "is_private": false}, {"count": 18, "tags": [], "bug_id": 44402, "attachment_id": null, "id": 111084, "time": "2008-02-22T12:34:29Z", "creator": "basant.kukreja@sun.com", "creation_time": "2008-02-22T12:34:29Z", "is_private": false, "text": "Since the crash happens on linux too so I am changing the summary."}, {"count": 19, "tags": [], "text": "Created attachment 21582\nRevised patch\n\nMade small correction in comments of patch.", "attachment_id": 21582, "id": 111085, "creator": "basant.kukreja@sun.com", "time": "2008-02-22T12:41:11Z", "bug_id": 44402, "creation_time": "2008-02-22T12:41:11Z", "is_private": false}, {"count": 20, "tags": [], "bug_id": 44402, "text": "Fixed in trunk in r630335.", "id": 111086, "time": "2008-02-22T14:17:59Z", "creator": "nick@webthing.com", "creation_time": "2008-02-22T14:17:59Z", "is_private": false, "attachment_id": null}, {"text": "Great work Basant. Nick beat me to committing your patch, but in the meantime I\napplied your patch to the event MPM on trunk as well (r630348).", "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "count": 21, "id": 109956, "time": "2008-02-22T15:00:31Z", "bug_id": 44402, "creation_time": "2008-02-22T15:00:31Z", "is_private": false}, {"count": 22, "tags": [], "bug_id": 44402, "text": "*** Bug 44474 has been marked as a duplicate of this bug. ***", "id": 108932, "time": "2008-02-22T17:35:12Z", "creator": "nick@webthing.com", "creation_time": "2008-02-22T17:35:12Z", "is_private": false, "attachment_id": null}, {"count": 23, "tags": [], "creator": "rpluem@apache.org", "attachment_id": null, "text": "Backported to 2.2.x as r631362 (http://svn.apache.org/viewvc?rev=631362&view=rev).", "id": 97593, "time": "2008-02-26T12:37:54Z", "bug_id": 44402, "creation_time": "2008-02-26T12:37:54Z", "is_private": false}, {"attachment_id": null, "tags": [], "creator": "nick@webthing.com", "text": "*** Bug 42086 has been marked as a duplicate of this bug. ***", "count": 24, "id": 114082, "time": "2008-02-27T15:27:49Z", "bug_id": 44402, "creation_time": "2008-02-27T15:27:49Z", "is_private": false}, {"count": 25, "tags": [], "creator": "wrowe@apache.org", "attachment_id": null, "text": "*** Bug 41030 has been marked as a duplicate of this bug. ***", "id": 140527, "time": "2010-10-06T19:06:04Z", "bug_id": 44402, "creation_time": "2010-10-06T19:06:04Z", "is_private": false}]