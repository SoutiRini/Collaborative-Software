[{"count": 0, "tags": [], "bug_id": 59319, "attachment_id": null, "id": 190231, "time": "2016-04-14T10:46:42Z", "creator": "ben.rubson@gmail.com", "creation_time": "2016-04-14T10:46:42Z", "is_private": false, "text": "Hello,\n\nI think I have found a bug around ProxyPass connectiontimeout parameter.\n\nHere is the test config :\n\nRewriteEngine On\nRewriteMap chooseproxy prg:/chooseproxy.pl\nRewriteCond ${chooseproxy:%{THE_REQUEST}} ^([0-9]{4,5})$\nRewriteRule (.*) - [L,E=proxytouse:127.0.0.1:%1]\nProxyPassInterpolateEnv on\nProxyPass \"/api/\" \"https://${proxytouse}/api/\" interpolate connectiontimeout=5\n\nDepending on the request, chooseproxy.pl returns the proxy port to connect to.\nIt works fine, however connectiontimeout is not honored : if port is not reachable, Apache only times-out after timeout seconds (the global timeout parameter).\n\nIf I change my ProxyPass rule removing the ${proxytouse} variable, for example :\nProxyPass \"/api/\" \"https://127.0.0.1:1234/api/\" interpolate connectiontimeout=5\nThen connectiontimeout parameter works correctly.\n\nThank you,\n\nBest regards,\n\nBen"}, {"count": 1, "tags": [], "bug_id": 59319, "attachment_id": null, "id": 190255, "time": "2016-04-14T17:15:21Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2016-04-14T17:15:21Z", "is_private": false, "text": "You scheme/host/port part of the target URI is \"http://${proxytouse}\". Any setting added to this ProxyPass via key=value will be saved under this worker name. Later during request runtime, the \"${proxytouse}\" will first be resolved to a real hostname plus port, say host1 and port1. Then a worker is being looked up under this hostname and port, ie. \"https://host1:port1\". Since no worker like that was defined in the config, the default reverse proxy worker will be used, which doesn't have these key=value settings applied (and you'll also probably not get connection pooling).\n\nIf you need to work with host and port provided by a script, and the list of possible hosts plus port pairs is not too long to maintain, you should configure the workers for those backends, so you get connection pooling an the actual key=value settings you like.\n\nExample: Suppose myhost:myport is one of the values returned by you script, then add\n\n<Proxy \"https://myhost:myport\">\n  ProxySet connectiontimeout=5 timeout=30\n</Proxy>\n\nto your config (and you can remove those key=value parts from you interpolated ProxyPass. I think it's useless there.\n\nYou can get a bit more insight about which workers are used etc. by setting LogLevel to trace8 or at least the LogLevel of mod_proxy to trace8 (\"LogLevel info proxy:trace8\").\n\nIf you don't like repeating those <Proxy> blocks for all you backends, you can define a macro using mod_macro:\n\n<Macro MyWorker $hostAndPort>\n  <Proxy \"https://${hostAndPort}\">\n    ProxySet connectiontimeout=5 timeout=30\n  </Proxy>\n</Macro>\n\nand then use the macro:\n\nUse MyWorker host1:port1\nUse MyWorker host2:port2\n...\n\nThat reduces the redundancy in repeating the communication settings for each Worker. The macro name \"MyWorker\" can be replaced by something more meaningful for you.\n\nIf the number of host/port pairs is really huge, then it is possible that you really don't want pooling, because reusing existing connections before they time out might be rare. In that case using the default reverse proxy worker for all those backend connections should be OK and you wouldn't explicitly define workers. Unfortunately IMHO we currently lack a way of defining worker params (key=value) as default for all workers and/or for the default (reverse) proxy worker. So in that case there's be no way of defining e.g. the connectiontimeout etc.\n\nWhether the fact, that using key=value does not work, if the worker has scheme, host name or port interpolated is a bug or not can be discussed. I'd say from teh current implementation it is not expected to work, but we could maybe log a warning if you try to use that pattern.\n\nLet's see what others comment."}, {"count": 2, "tags": [], "bug_id": 59319, "is_private": false, "id": 190285, "attachment_id": null, "creator": "ben.rubson@gmail.com", "creation_time": "2016-04-15T07:00:06Z", "time": "2016-04-15T07:00:06Z", "text": "Rainer,\n\nFirst of all, thank you very much for your long, detailed, precise answer...\nI really appreciate it !\n\nSo undestood, \"https://${proxytouse}\" will never be targeted by a resolved worker, \"https://host1:port1\" in your example.\nLet's asume it would, some parameters such as connection pooling in a worker like \"https://${proxytouse}\" would not really makes sense, as in reality it would address many different addresses/ports.\nTo work, a new worker (a clone of the default worker with specific user defined parameters applied) would have to be created internally each time the generic one is resolved to a new value.\n\nUnfortunately as you said there is no possibility to enforce default values for the default worker.\nOnce again some values would certainly not make sense for the default worker (connection pooling...), but some of them, such as for example connectiontimeout, timeout... would be worth it.\n\nIn my use case, I have many host/port pairs which vary upon the system's configuration and furthermore change over time.\nSo a uniq configuration with variables is very practical.\n\nI could think about the following : each time a new host/port pair arrives on the server, automatically create its small configuration file containing only the following macro line :\nUse MyWorker hostX:portX\nAnd reload the Apache configuration.\nMacro would be defined in the main configuration.\nThe main configuration would also contain something like :\nInclude /path/to/workers/configurations/worker.*.conf\n\nOf course being able to define default parameters for the default worker (or for its clones) would be even easier / cleaner.\n\nThat's another story but, declaring a worker for each host:port would activate connection pooling for each one of them.\nI think connection pooling would be work it in my use case : each time a user connects to the service, it is forwarded to the same reserve proxy. So connection pooling could be a good thing to have.\nHowever, can connection pooling have an impact in terms of performance if Apache has to manage many workers ?\nCan we tend to somethig like denial of service if the maximum number of connection in pools is reached, and no other connections to other workers can be made ?\nOr is there no limit at all here ?\n\nBeing able to define default worker (or clones of worker) parameters would also improve P flag of mod_rewrite.\n\nThank you again,\n\nBest regards,\n\nBen"}, {"count": 3, "tags": [], "text": "(In reply to Ben RUBSON from comment #2)\n\n> Unfortunately as you said there is no possibility to enforce default values\n> for the default worker.\n> Once again some values would certainly not make sense for the default worker\n> (connection pooling...), but some of them, such as for example\n> connectiontimeout, timeout... would be worth it.\n\nYes and I think I have to fully very this (I checked the code but only reading it) and if it is true, then being able to either define setting for the default worker or being able to set default settings for all worker including the default would be a useful enhacement, probably both of it. Your use case of handling many backends and deciding between them based on policy/convention/table is happening more and more out there.\n\n> I could think about the following : each time a new host/port pair arrives\n> on the server, automatically create its small configuration file containing\n> only the following macro line :\n> Use MyWorker hostX:portX\n> And reload the Apache configuration.\n> Macro would be defined in the main configuration.\n> The main configuration would also contain something like :\n> Include /path/to/workers/configurations/worker.*.conf\n\nI know installations who do it exactly like that.\n\n> That's another story but, declaring a worker for each host:port would\n> activate connection pooling for each one of them.\n> I think connection pooling would be work it in my use case : each time a\n> user connects to the service, it is forwarded to the same reserve proxy. So\n> connection pooling could be a good thing to have.\n\nYes, if there would be reuse, ie. often the next request for the same backend hits before the connection runs into its HTTP keep-alive timeout (especially the one set by the backend!).\n\n> However, can connection pooling have an impact in terms of performance if\n> Apache has to manage many workers ?\n\nI expect no problem in case of say less than 100 workers. I don't remember reports for any numbers, but if you have many more workers, than it might be, that there's not so much relevant experience out there and some testing would be recommended.\n\n> Can we tend to somethig like denial of service if the maximum number of\n> connection in pools is reached, and no other connections to other workers\n> can be made ?\n> Or is there no limit at all here ?\n\nPools are always local in Apache processes. Processes do not share connections to backends between themselves. The limit (maximum number of connections per backend) is equals to the number of threads in the process. Since Apache can't handle more requests in one process at one point in time than it has threads, and we allow the same number of backend connections, you should never run into a connection limitation unless you reduce the limit by configuring a smaller limit. Pooling here is more about reuse than limiting.\n\nIf you would use it to limit the access to the backend via a threshold, the limit would be per process and you can't control which request ends up in which process. So limiting access via the connection pool would not be very precise. There's a \"busy\" counter which is shared between the Apache processes and knows how many requests are in-flight on the backends (more precisely those that come from this Apache) and one could limit using the busy counter to protect the backend, but I think (not 100% sure) we don't yet spport limiting based on the busy value. \n\n> Being able to define default worker (or clones of worker) parameters would\n> also improve P flag of mod_rewrite.\n\nYup. Maybe we sould add another Bugzilla as a feature request for adding a way to configure the default workers (forward, reverse) and to cinfigure defaults for all workers. Both should be per VHost and values would be inherited from global server to VHosts.", "is_private": false, "bug_id": 59319, "id": 190287, "time": "2016-04-15T09:58:22Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2016-04-15T09:58:22Z", "attachment_id": null}, {"count": 4, "tags": [], "creator": "ben.rubson@gmail.com", "text": "(In reply to Rainer Jung from comment #3)\n> \n> (...) being able to either define setting for\n> the default worker or being able to set default settings for all worker\n> including the default would be a useful enhacement, probably both of it.\n\n> Maybe we sould add another Bugzilla as a feature request for adding a\n> way to configure the default workers (forward, reverse) and to cinfigure\n> defaults for all workers. Both should be per VHost and values would be\n> inherited from global server to VHosts.\n\nThis would definitely work for parameters such as connectiontimeout.\nBut do you think it would work for connection pooling, without having to individually declare each target worker (as we need for the moment) ?\nie. would Apache be \"smart\" enough to start a worker and its connection pool for every new proxy it deals with, and if the worker has not been literally declared in the configuration, to apply the default configuration (which could then enable by default or at least contain connection pooling settings) ?\n\n> Pools are always local in Apache processes. Processes do not share\n> connections to backends between themselves. The limit (maximum number of\n> connections per backend) is equals to the number of threads in the process.\n> Since Apache can't handle more requests in one process at one point in time\n> than it has threads, and we allow the same number of backend connections,\n> you should never run into a connection limitation unless you reduce the\n> limit by configuring a smaller limit. Pooling here is more about reuse than\n> limiting.\n\nSo if for any reason prefork MPM is used, connection pool only has one connection :)\nUnderstood, perfectly clear, many thanks Rainer !", "id": 190290, "time": "2016-04-15T12:45:51Z", "bug_id": 59319, "creation_time": "2016-04-15T12:45:51Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 59319, "attachment_id": null, "id": 190401, "time": "2016-04-20T08:36:40Z", "creator": "ben.rubson@gmail.com", "creation_time": "2016-04-20T08:36:40Z", "is_private": false, "text": "In addition Rainer, should we definitely open a new Bugzilla as a feature request ?\nThank you again,\nBen"}, {"count": 6, "tags": [], "text": "Enhancement submitted here :\nhttps://bz.apache.org/bugzilla/show_bug.cgi?id=59373\n\nThank you !\n\n*** This bug has been marked as a duplicate of bug 59373 ***", "is_private": false, "id": 190532, "creation_time": "2016-04-25T09:18:55Z", "time": "2016-04-25T09:18:55Z", "creator": "ben.rubson@gmail.com", "bug_id": 59319, "attachment_id": null}]