[{"count": 0, "tags": [], "bug_id": 59152, "text": "Hi,\n\nThis patch change default Thread Group -> Action to be taken after a Sample Error value from \"Continue\" to \"Start Next thread loop\"\n\nI think it's better to start next thread loop instead of continuing without take into account the error because continue if there is an error makes no sense (it's not realistic)\n\nAntonio", "id": 189298, "time": "2016-03-09T12:19:38Z", "creator": "ra0077@gmail.com", "creation_time": "2016-03-09T12:19:38Z", "is_private": false, "attachment_id": null}, {"count": 1, "tags": [], "bug_id": 59152, "is_private": false, "id": 189299, "attachment_id": null, "creator": "ra0077@gmail.com", "creation_time": "2016-03-09T12:26:14Z", "time": "2016-03-09T12:26:14Z", "text": "PR 161 commited\n\nAntonio"}, {"count": 2, "tags": [], "text": "Date: Wed Mar  9 21:09:24 2016\nNew Revision: 1734313\n\nURL: http://svn.apache.org/viewvc?rev=1734313&view=rev\nLog:\nBug 59152 - Thread Group: Change \"Action to be taken after a Sample Error\" value from \"Continue\" to \"Start Next thread loop\"\n#resolve #161\nhttps://github.com/apache/jmeter/pull/161\nBugzilla Id: 59152\n\nModified:\n    jmeter/trunk/bin/templates/recording-with-think-time.jmx\n    jmeter/trunk/bin/templates/recording.jmx\n    jmeter/trunk/src/core/org/apache/jmeter/threads/gui/AbstractThreadGroupGui.java\n    jmeter/trunk/xdocs/changes.xml", "is_private": false, "id": 189320, "creator": "p.mouawad@ubik-ingenierie.com", "time": "2016-03-09T21:09:43Z", "bug_id": 59152, "creation_time": "2016-03-09T21:09:43Z", "attachment_id": null}, {"count": 3, "tags": [], "bug_id": 59152, "text": "I've just fallen foul of this change.\n\nI set up a test which deliberately had a couple of failing samples, but only one of them was run.\n\nIt took me quite a while to work out what had happened.\n\nI think this change is going to be a nuisance for others as well and should be reverted.", "id": 190709, "time": "2016-05-03T20:56:35Z", "creator": "sebb@apache.org", "creation_time": "2016-05-03T20:56:35Z", "is_private": false, "attachment_id": null}, {"count": 4, "tags": [], "bug_id": 59152, "text": "(In reply to Sebb from comment #3)\n> I've just fallen foul of this change.\n> \n> I set up a test which deliberately had a couple of failing samples, but only\n> one of them was run.\n> \n> It took me quite a while to work out what had happened.\n> \n> I think this change is going to be a nuisance for others as well and should\n> be reverted.\n\nI don't share this opinion and agree with initial motivations of this change.\nIn my experience (webapps testing), I always set the value to what it is after the change.\n\nSince change is mentioned in Incompatible changes, it is not a problem for me.\n\nBut I can be wrong.\nWhat is the Use case of your plan ? \nThanks", "id": 190712, "time": "2016-05-03T21:05:59Z", "creator": "p.mouawad@ubik-ingenierie.com", "creation_time": "2016-05-03T21:05:59Z", "is_private": false, "attachment_id": null}, {"count": 5, "tags": [], "bug_id": 59152, "text": "(In reply to Philippe Mouawad from comment #4)\n> (In reply to Sebb from comment #3)\n> > I've just fallen foul of this change.\n> > \n> > I set up a test which deliberately had a couple of failing samples, but only\n> > one of them was run.\n> > \n> > It took me quite a while to work out what had happened.\n> > \n> > I think this change is going to be a nuisance for others as well and should\n> > be reverted.\n> \n> I don't share this opinion and agree with initial motivations of this change.\n> In my experience (webapps testing), I always set the value to what it is\n> after the change.\n> \n> Since change is mentioned in Incompatible changes, it is not a problem for\n> me.\n\nIt is one line amongst lots. \nEven though I know it's there the consequences are not immediately obvious.\n\n> But I can be wrong.\n> What is the Use case of your plan ? \n\nI was testing the changes for Content-Encoding and wanted to see how error pages were handled.\n\nI quite often want to use invalid URL or failed samples.\n\nAlso when one is developing a test plan, it's easy to make mistakes. With the new default the first mistake stops the test run, so it can take a long time to find all the mistakes. \n\nI think this will penalise people who are new to JMeter or who use it infrequently.", "id": 190713, "time": "2016-05-03T21:16:35Z", "creator": "sebb@apache.org", "creation_time": "2016-05-03T21:16:35Z", "is_private": false, "attachment_id": null}, {"count": 6, "tags": [], "creator": "milamber@apache.org", "attachment_id": null, "is_private": false, "id": 190715, "time": "2016-05-04T06:52:46Z", "bug_id": 59152, "creation_time": "2016-05-04T06:52:46Z", "text": "Hello,\n\nI think that the revert will be a good thing. I made some load tests with this new default option. My experience is this new default mask a bad load test.\nI explain:\nIf you are a plan test like this:\n1/ Login Form\n2/ Login (id/pass)\n3/ Home\n4/ Search\n5/ Results\netc.\n\nWith this default option, if the 2/Login failed (target server failed to login randomly), you see only some Errors (during the load test) on 2/Login, the other pages have 0 error (because return to 1/ after error)\n\nAfter, when you show the graph results : all is good\nIf you show results in the summary listener, you see results like this:\n1/ Login form : 2000 samples\n2/ Login (id/pass) : 2000 samples with 25% errors\n3/ Home : 1500 samples with 0 errors\n4/ Search : 1500 samples with 0 errors\netc.\n\nIt's very easy to conclude that this load test is successful (only 25% errors on 1 page), but in reality, this is a bad load test, because my target load is reduce to 25%, and the target server have been tested with only at 75% of the load.\nI prefer the old option, which force me to stop a load test if the errors increase on all pages after a error on the login page."}, {"count": 7, "tags": [], "text": "Hi,\nOk for me.\nI don't want to delay release for this.\n\nWho reverts it ?\n\nThanks\nRegards\nPhilippe", "is_private": false, "id": 190734, "creator": "p.mouawad@ubik-ingenierie.com", "time": "2016-05-04T20:07:07Z", "bug_id": 59152, "creation_time": "2016-05-04T20:07:07Z", "attachment_id": null}, {"count": 8, "tags": [], "bug_id": 59152, "is_private": false, "id": 190737, "attachment_id": null, "creator": "sebb@apache.org", "creation_time": "2016-05-04T20:33:31Z", "time": "2016-05-04T20:33:31Z", "text": "I will revert it"}, {"count": 9, "tags": [], "bug_id": 59152, "text": "Reverted:\n\nURL: http://svn.apache.org/viewvc?rev=1742333&view=rev\nLog:\nRevert r1734313 so default action remains as 'Continue'\nBugzilla Id: 59152\n\nModified:\n    jmeter/trunk/bin/templates/recording-with-think-time.jmx\n    jmeter/trunk/bin/templates/recording.jmx\n    jmeter/trunk/src/core/org/apache/jmeter/threads/gui/AbstractThreadGroupGui.java\n    jmeter/trunk/xdocs/changes.xml", "id": 190738, "time": "2016-05-04T20:42:08Z", "creator": "sebb@apache.org", "creation_time": "2016-05-04T20:42:08Z", "is_private": false, "attachment_id": null}, {"count": 10, "tags": [], "bug_id": 59152, "attachment_id": null, "id": 191217, "time": "2016-05-24T12:12:37Z", "creator": "sitnikov.vladimir@gmail.com", "creation_time": "2016-05-24T12:12:37Z", "is_private": false, "text": "Milamber>It's very easy to conclude that this load test is successful (only 25% errors on 1 page), but in reality, this is a bad load test, because my target load is reduce to 25%, and the target server have been tested with only at 75% of the load.\n\nTechnically speaking, \"validation\" of a test report should include not only \"% of errors\" validation, but \"planned throughput vs actual throughput\", \"planned response times vs actual response times\".\n\n\nIf a test is hard to setup (e.g. lots of steps), then it might be better to start new iteration to try one's best to achieve \"throughput goal\".\n\nAt the end of the day, real users do restart from scratch in case of failure.\n\n\nNote: high failrate would be misleading, since it will show \"consequence\", while it makes much more sense in knowing \"root cause\". In that case, it would be good to restart after the first failure, and %error would show exactly the step that failed."}, {"count": 11, "tags": [], "bug_id": 59152, "text": "(In reply to Vladimir Sitnikov from comment #10)\n> Milamber>It's very easy to conclude that this load test is successful (only\n> 25% errors on 1 page), but in reality, this is a bad load test, because my\n> target load is reduce to 25%, and the target server have been tested with\n> only at 75% of the load.\n> \n> Technically speaking, \"validation\" of a test report should include not only\n> \"% of errors\" validation, but \"planned throughput vs actual throughput\",\n> \"planned response times vs actual response times\".\n\nIt's also important to know whether any tests were skipped.\n \n> \n> If a test is hard to setup (e.g. lots of steps), then it might be better to\n> start new iteration to try one's best to achieve \"throughput goal\".\n> \n> At the end of the day, real users do restart from scratch in case of failure.\n\nMore likely, they will redo the step that failed.\nAfter a couple of such failures they will go away and try another time.\n\n> \n> Note: high failrate would be misleading, since it will show \"consequence\",\n> while it makes much more sense in knowing \"root cause\". In that case, it\n> would be good to restart after the first failure, and %error would show\n> exactly the step that failed.\n\nThat depends on the exact scenario. \nSome failures may not be fatal to the loop or the test, e.g. image download failure.\n\nOnly the test designer knows what the correct on error behaviour is, and that will vary between plans and parts of test plans.\n\nI think the only sensible default is Continue on Error.\n\nPartly because that is the original setting, and partly to ensure that the full test plan is exercised by default.", "id": 191228, "time": "2016-05-24T14:37:29Z", "creator": "sebb@apache.org", "creation_time": "2016-05-24T14:37:29Z", "is_private": false, "attachment_id": null}]