[{"count": 0, "tags": [], "creator": "dopey@moonteeth.com", "attachment_id": null, "text": "This appears to be the same as:\nhttps://issues.apache.org/bugzilla/show_bug.cgi?id=42366\n\nI'm opening a new one though since that one is so old.  If this is incorrect please let me know.\n\nThe issue is reproducible with any fairly recent mod_jk but I'm testing 1.2.40 right now.  httpd -V output:\nServer version: Apache/2.2.27 (Win32)\nServer built:   May 15 2014 17:04:25\nServer's Module Magic Number: 20051115:33\nServer loaded:  APR 1.5.0, APR-Util 1.5.3\nCompiled using: APR 1.5.0, APR-Util 1.5.3\nArchitecture:   32-bit\nServer MPM:     WinNT\n  threaded:     yes (fixed thread count)\n    forked:     no\nServer compiled with....\n -D APACHE_MPM_DIR=\"server/mpm/winnt\"\n -D APR_HAS_SENDFILE\n -D APR_HAS_MMAP\n -D APR_HAVE_IPV6 (IPv4-mapped addresses disabled)\n -D APR_HAS_OTHER_CHILD\n -D AP_HAVE_RELIABLE_PIPED_LOGS\n -D DYNAMIC_MODULE_LIMIT=128\n -D HTTPD_ROOT=\"/apache\"\n -D DEFAULT_SCOREBOARD=\"logs/apache_runtime_status\"\n -D DEFAULT_ERRORLOG=\"logs/error.log\"\n -D AP_TYPES_CONFIG_FILE=\"conf/mime.types\"\n -D SERVER_CONFIG_FILE=\"conf/httpd.conf\"\u0000\n          \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\nWhat i did was simply created a 600mb file using dd if=/dev/urandom\nplopped it into tomcat's webapps/example directory\n\nconfigured\nJkMount /examples/* ajpWorker\n\nand used wget to download it.  I ran wget in a loop and eventually stopped it when private bytes hit around 300MB.\n\nI can't reliably reproduce it with all workers.properties options.  For example this:\n\nworker.tomcat1.type=ajp13\nworker.tomcat1.host=localhost\nworker.tomcat1.port=8010\nworker.tomcat1.max_packet_size=8192\nworker.tomcat1.connection_pool_size=120\nworker.tomcat1.connection_pool_minsize=8\nworker.tomcat1.connection_pool_timeout=900\ntriggers the problem\nbut\nthis alone:\nworker.tomcat1.type=ajp13\nworker.tomcat1.host=localhost\nworker.tomcat1.port=8010\ndoes not.\n\nI'm in the process of testing to see if the connection_pool configuration may be what's doing it.", "id": 176441, "time": "2014-07-16T22:23:22Z", "bug_id": 56733, "creation_time": "2014-07-16T22:23:22Z", "is_private": false}, {"count": 1, "tags": [], "creator": "dopey@moonteeth.com", "attachment_id": null, "text": "Oh, in case this helps, this is the debugdiag stack for the memory allocations:\nFunction   Destination \nlibapr_1!allocator_alloc+d6   msvcr100!malloc \nlibapr_1!apr_palloc+d9   libapr_1!allocator_alloc \nlibaprutil_1!apr_brigade_create+11   libapr_1!apr_palloc \nlibhttpd!ap_rflush+19   libaprutil_1!apr_brigade_create \nmod_jk!ws_flush+1a   libhttpd!ap_rflush \nmod_jk!ajp_process_callback+586    \nmod_jk!ajp_get_reply+c4   mod_jk!ajp_process_callback \nmod_jk!ajp_service+60a   mod_jk!ajp_get_reply \nmod_jk!service+82f    \nmod_jk!jk_handler+6ea    \nlibhttpd!ap_run_handler+25    \nlibhttpd!ap_invoke_handler+a2   libhttpd!ap_run_handler \nlibhttpd!ap_process_request+3e   libhttpd!ap_invoke_handler \nlibhttpd!ap_process_http_connection+52   libhttpd!ap_process_request \nlibhttpd!ap_run_process_connection+25    \nlibhttpd!ap_process_connection+33   libhttpd!ap_run_process_connection \nlibhttpd!worker_main+a7   libhttpd!ap_process_connection \nmsvcr100!_callthreadstartex+1b    \nmsvcr100!_threadstartex+64    \nkernel32!BaseThreadInitThunk+e    \nntdll!__RtlUserThreadStart+70    \nntdll!_RtlUserThreadStart+1b   ntdll!__RtlUserThreadStart \nmsvcr100!_threadstartex", "id": 176442, "time": "2014-07-16T22:25:22Z", "bug_id": 56733, "creation_time": "2014-07-16T22:25:22Z", "is_private": false}, {"count": 2, "tags": [], "creator": "chris@christopherschultz.net", "attachment_id": null, "is_private": false, "id": 176444, "time": "2014-07-17T00:47:58Z", "bug_id": 56733, "creation_time": "2014-07-17T00:47:58Z", "text": "The max_packet_size is the default (8192) so that's not likely it. The connection_pool sizes are likely the differentiating factor.\n\nDid you build mod_jk yourself, or use a pre-built one? Do you know what version of APR is being used under the hood?\n\nThe call mod_jk makes to ap_rflush looks fairly innocuous. That call is also made in ws_write if the request is for headers only (e.g. a HEAD request). Can you try this with and without +FlushPackets and try a huge number of HEAD requests to see if you can also trigger a memory leak?"}, {"count": 3, "attachment_id": null, "bug_id": 56733, "text": "I was able to reproduce this both with my own mod_jk as well as with the mod_jk binary build from apache.org.\n\nThe APR I used to build mine was the APR that's part of the apache 2.2.27 source tree so it's:\nServer loaded:  APR 1.5.0, APR-Util 1.5.3\nCompiled using: APR 1.5.0, APR-Util 1.5.3\n\nI'll try the HEAD test tomorrow and see what comes up.\n\nDo you think it's worth trying to figure out how the connection pool configuration plays into this by tweaking those a bit more?", "id": 176445, "time": "2014-07-17T03:26:18Z", "creator": "dopey@moonteeth.com", "creation_time": "2014-07-17T03:26:18Z", "tags": [], "is_private": false}, {"count": 4, "attachment_id": null, "bug_id": 56733, "is_private": false, "id": 176457, "time": "2014-07-17T14:25:51Z", "creator": "chris@christopherschultz.net", "creation_time": "2014-07-17T14:25:51Z", "tags": [], "text": "(In reply to Andy Wang from comment #3)\n> I was able to reproduce this both with my own mod_jk as well as with the\n> mod_jk binary build from apache.org.\n> \n> The APR I used to build mine was the APR that's part of the apache 2.2.27\n> source tree so it's:\n> Server loaded:  APR 1.5.0, APR-Util 1.5.3\n> Compiled using: APR 1.5.0, APR-Util 1.5.3\n\nI wonder if this might be a bug in APR... 1.5.0 is only 8 months old and its possible there is a problem with the interaction between mod_jk and apr or something.\n\nTesting against a version of Apache httpd that provides apr 1.4.6 may be instructive.\n\n> I'll try the HEAD test tomorrow and see what comes up.\n> \n> Do you think it's worth trying to figure out how the connection pool\n> configuration plays into this by tweaking those a bit more?\n\nPerhaps. More information is always good."}, {"count": 5, "tags": [], "bug_id": 56733, "is_private": false, "text": "I should have mentioned.  This is reproducible with Apache 2.2.22 with APR 1.4.5\nand 2.2.24 with APR 1.4.6.\n\n\nI don't recall the exact mod_jk versions from those releases and can get those, but the same behavior was definitely recorded for a while now.", "id": 176463, "time": "2014-07-17T19:59:16Z", "creator": "dopey@moonteeth.com", "creation_time": "2014-07-17T19:59:16Z", "attachment_id": null}, {"count": 6, "tags": [], "creator": "jorton@redhat.com", "attachment_id": null, "is_private": false, "id": 176464, "time": "2014-07-17T20:00:57Z", "bug_id": 56733, "creation_time": "2014-07-17T20:00:57Z", "text": "Server version: Apache/2.2.27 (Win32)\n\nthis is a 2.2 bug.  It's fixed in 2.4."}, {"count": 7, "tags": [], "bug_id": 56733, "text": "Is there a particular submission that can be backported?  Or is it something more complicated than that?  It'll be a while before we can get our customers onto 2.4 unfortunately.", "id": 176467, "time": "2014-07-17T20:30:15Z", "creator": "dopey@moonteeth.com", "creation_time": "2014-07-17T20:30:15Z", "is_private": false, "attachment_id": null}, {"count": 8, "attachment_id": null, "bug_id": 56733, "text": "Thanks for sharing Joe.\n\nAre you referring to any of those two?\n\nr821471 | sf | 2009-10-04 09:37:28 +0200 (Sun, 04 Oct 2009) | 3 lines\n\ncore, mod_deflate, mod_sed: Reduce memory usage by reusing bucket\nbrigades in several places\n\nr821477 | sf | 2009-10-04 10:08:50 +0200 (Sun, 04 Oct 2009) | 4 lines\n\nMake sure to not destroy bucket brigades that have been created by earlier\nfilters. Otherwise the pool cleanups would be removed causing potential memory\nleaks later on.\n\nOr something else? I couldn't find any other likely fit in CHANGES for 2.4.\n\nRegards,\n\nRainer", "id": 176472, "time": "2014-07-17T21:31:09Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2014-07-17T21:31:09Z", "tags": [], "is_private": false}, {"count": 9, "tags": [], "creator": "dopey@moonteeth.com", "attachment_id": null, "is_private": false, "id": 176473, "time": "2014-07-17T21:58:40Z", "bug_id": 56733, "creation_time": "2014-07-17T21:58:40Z", "text": "(In reply to Christopher Schultz from comment #2) \n> The call mod_jk makes to ap_rflush looks fairly innocuous. That call is also\n> made in ws_write if the request is for headers only (e.g. a HEAD request).\n> Can you try this with and without +FlushPackets and try a huge number of\n> HEAD requests to see if you can also trigger a memory leak?\n\nI ran ab -c 60 -n 6000 -i about a half dozen times and memory slowly grew to around 38mb, but after that another dozen times and the memory didn't change.\n\nThis is with the following workers.properties config:\nworker.tomcat1.type=ajp13\nworker.tomcat1.host=localhost\nworker.tomcat1.port=8010\nworker.tomcat1.connection_pool_size=120\nworker.tomcat1.connection_pool_minsize=8\nworker.tomcat1.connection_pool_timeout=900\n\nSo I pulled out the packet size option since I agree it's unlikely to be the cause.  I actually only intended to pull in the connection_pool_* ones but my mouse slipped and I cut and pasted 4 lines :)\n\nThis is with +FlushPackets so I'm assuming that since it stopped growing a test without FlushPackets probably wouldn't be meaningful right?  And just to confirm, this still had memory growth with a regular\nab -c 10 -n 6000 (without -i).  I had to drop concurrency to 10 because my poor little VM disk i/o didn't like sending out 60 concurrent 600mb streams.\n\nI'm going to play with the pool_timout and pool_minsize to see if there's something there.  \n\nIs it possible that timing connections out from the pool could \"lose\" the handle to the allocated memory preventing it from being freed?"}, {"count": 10, "tags": [], "creator": "dopey@moonteeth.com", "attachment_id": null, "is_private": false, "id": 176475, "time": "2014-07-17T22:25:06Z", "bug_id": 56733, "creation_time": "2014-07-17T22:25:06Z", "text": "Couple of more datapoints.\n\nCommitted heap stabilized out at 35684k with:\nworker.tomcat1.connection_pool_timeout=0\n\nor with\nworker.tomcat1.connection_pool_timeout=900\nworker.maintain=900\n\nWith just\nworker.tomcat1.connection_pool_timeout=900\nworker.maintain=60\n\nmemory grew to around 196MB before I killed the ab\n\nHere's the thing I don't understand\nwith:\nworker.tomcat1.connection_pool_timeout=900\nworker.maintain=60\n\nThe memory grew to about 196mb in <500s.  So none of connection pool should have been recycled yet.  This seems to imply that the maintenance alone is enough to \"trigger\" the leak.\n\nAt least that's what I'm concluding from the data.\nThoughts?"}, {"count": 11, "tags": [], "creator": "rainer.jung@kippdata.de", "attachment_id": null, "is_private": false, "id": 176476, "time": "2014-07-17T22:35:04Z", "bug_id": 56733, "creation_time": "2014-07-17T22:35:04Z", "text": "@Joe: never mind, based on the dev@apr conversation I can now see it is in r821471."}, {"count": 12, "tags": [], "creator": "rainer.jung@kippdata.de", "attachment_id": null, "is_private": false, "id": 176478, "time": "2014-07-17T22:41:21Z", "bug_id": 56733, "creation_time": "2014-07-17T22:41:21Z", "text": "As far as I understand this is not really a leak, but a high memory use. Memory allocated during a request is not freed but available for use during the next request. If you want to free high memory which might be unlikely to get used quickly again, you can set\n\nMaxMemFree 2048\n\n2.2 has default setting\n\nMaxMemFree 0 (do not free)\n\nand 2.4 has\n\nMaxMemFree 2048 (free if 2MB are reached).\n\nThis will not fix the problem of high memory need during the serving of such a request, but will free memory after the end of the request.\n\n2.4 has optimized code, that makes the serving less memory intensive.\n\nPlease report back, whether \"MaxMemFree 2048\" helps in your case."}, {"count": 13, "tags": [], "creator": "dopey@moonteeth.com", "attachment_id": null, "text": "MaxMemFree does bring memory down slightly every once in a while, but the memory growth overall continues on an upward trend.\n\nI'm calling it a \"leak\" because \na) on a 32-bit process, the process eventually runs out of memory.  It seems that if this was intentionally it shouldn't do that.\nb) if the memory is meant to be used for the next request why does it continually grow?  I can do sequential requests over and over and over time, the memory use will continually grow by .5-1MB per request.", "id": 176482, "time": "2014-07-18T02:00:57Z", "bug_id": 56733, "creation_time": "2014-07-18T02:00:57Z", "is_private": false}, {"count": 14, "attachment_id": null, "bug_id": 56733, "is_private": false, "id": 176483, "time": "2014-07-18T02:45:21Z", "creator": "dopey@moonteeth.com", "creation_time": "2014-07-18T02:45:21Z", "tags": [], "text": "Sorry I didn't mention this in my original bug report, but we didn't simply just see memory growth.  I wouldn't have been worried if it was just that and it stabilized.  The issue is that in a 32-bit environment, httpd actually crashes with memory allocation problems once the 32-bit address space is exhausted.\n\nSo I reconfigured my workers.properties to a minimal config that allows me to reproduce this:\n\nworker.list=tomcat1\nworker.maintain=60\n\nworker.tomcat1.type=ajp13\nworker.tomcat1.host=localhost\nworker.tomcat1.port=8010\nworker.tomcat1.connection_pool_timeout=900\n\nI'm using the default ThreadsPerChild for mpm_winnt (docs say 64)\nJkOptions +FlushPackets is enabled and\nMaxMemFree 2048 is set\n\nI did the following:\n120000 HEAD requests to spool up all the connectors.  I ran this in two 60000 batches.  After each run the committed heap via vmmap.exe was consistently at\n7580kb\n\nI ran \nab -c 10 -n 10\nto download my 600mb file and after that committed heap is at\n29616kb\n\nafter a second run it's at\n51724kb\n\nI'm now running\nab -c 10 -n 1000\n\nAnd memory is still continually growing."}, {"count": 15, "tags": [], "creator": "jorton@redhat.com", "attachment_id": null, "is_private": false, "id": 176490, "time": "2014-07-18T12:47:44Z", "bug_id": 56733, "creation_time": "2014-07-18T12:47:44Z", "text": "Rainer - yes, it is r821471 - I have no idea what subsequent tweaks/fixes were made to that code.\n\nMaxMemFree will not help here.  N calls of ap_rflush(r) has O(N) memory consumption in 2.2, because each call creates a brigade which is never destroyed until r->pool cleanup time."}, {"count": 16, "tags": [], "creator": "chris@christopherschultz.net", "attachment_id": null, "text": "I asked about this on the APR-dev mailing list. Please see http://markmail.org/message/m4vvn4f5elfhnpjj for the response, which seems informative. I must admit, it's a bit over my head.", "id": 176505, "time": "2014-07-18T17:42:13Z", "bug_id": 56733, "creation_time": "2014-07-18T17:42:13Z", "is_private": false}, {"count": 17, "attachment_id": null, "bug_id": 56733, "is_private": false, "id": 176506, "time": "2014-07-18T17:45:36Z", "creator": "chris@christopherschultz.net", "creation_time": "2014-07-18T17:45:36Z", "tags": [], "text": "*eyeroll* Sorry for posting before reading the updates. At least there's a reference to the conversation, now, in BZ ;)"}, {"count": 18, "tags": [], "bug_id": 56733, "text": "So given this, I can stop running tests and posting results?  \n\nI am curious why this seems to occur depending on mod_jk worker maintain and connection pool timeout configuration.\n\nAlso, not sure if this is meaningful, but yesterday I ran a continual test (ab -n 10 -c [some really high number] don't really remember what i picked) and httpd eventually died running out of memory.\n\nwhen I tried it again, I ran it with ab -n 10 -c 1000 which got me to about ~170mb.\n\nthen I stopped testing, came back and tried some more tests, and after 2 runs of\nab -n 10 -c 1000 the memory stopped growing.\n\nSo I think there is something to Rainer's comment that this isn't a simple leak and that memory does get reused but there definitely does appear to be some possibility based on timing of a scalability problem at least (if not a flat out leak).", "id": 176508, "time": "2014-07-18T19:09:15Z", "creator": "dopey@moonteeth.com", "creation_time": "2014-07-18T19:09:15Z", "is_private": false, "attachment_id": null}, {"count": 19, "tags": [], "creator": "dopey@moonteeth.com", "attachment_id": null, "is_private": false, "id": 176509, "time": "2014-07-18T19:11:28Z", "bug_id": 56733, "creation_time": "2014-07-18T19:11:28Z", "text": "\nSorry, this was mis-spoken:\nwhen I tried it again, I ran it with ab -n 10 -c 1000 which got me to about ~170mb.\n\nthen I stopped testing, came back and tried some more tests, and after 2 runs of\nab -n 10 -c 1000 the memory stopped growing.\n\nLet me ammend it:\nwhen I tried it again, [from a freshly restarted httpd] I ran it with ab -n 10 -c 1000 which got me to about ~170mb.\n\nthen I stopped testing, [went to bed overnight and in the morning, against the same running instance as the night before - now at ~170mb committed heap] came back and tried some more tests, and after 2 runs of ab -n 10 -c 1000 the memory never grew from the ~170mb it was at in the morning."}, {"count": 20, "attachment_id": null, "bug_id": 56733, "is_private": false, "id": 176516, "time": "2014-07-18T20:05:16Z", "creator": "rainer.jung@kippdata.de", "creation_time": "2014-07-18T20:05:16Z", "tags": [], "text": "It *should* be like that (for 2.2, 2.4 should be fine):\n\n- to much memory used while requests are in process\n\n- concurrent processing of requests will multiply the needed memory\n\n- MaxMemFree allows to define, how much of that memory will be freed again after each request has finished\n\nI need to check these assumptions (probably next week) and whether I can reproduce.\n\nAs Joe indicated, there might be a workaround possible. Based on the fact, that 2.4 is pretty stable and the issue only shows for huge content with flushpackets set, it is quite possible, that we won't go that way. At least it will depend on how easy the problem is to work around in code.\n\nAs always: patches welcome."}, {"count": 21, "tags": [], "creator": "rainer.jung@kippdata.de", "attachment_id": null, "text": "Based on the fact, that this is a known flushing deficiency in the Apache webserver 2.2, is fixed in 2.4 and als least IMHO it is a non-trivial undertaking to include a 2.4 style fix in mod_jk just for 2.2, I'm closing this as WONTFIX.\n\nFor reference:\n\n- this only happens in combination with Apache web server 2.2, not with 2.4\n- only if JkOptions +FlushPackets in used (non-default)\n- your content is big\n\nIt is not a leak but will result in relatively high memory demand.\n\nThanks for reporting nevertheles and helping to understand this issue. If you disagree with this assessment, feel free to reopen the issue.\n\nRegards,\n\nRainer", "id": 179919, "time": "2014-12-21T18:20:21Z", "bug_id": 56733, "creation_time": "2014-12-21T18:20:21Z", "is_private": false}]