[{"count": 0, "tags": [], "creator": "fhanik@apache.org", "attachment_id": 28704, "text": "Created attachment 28704\nfix missing count down for maxConnections latch\n\nWe've run into a scenario where the JIO Acceptor thread hangs as connections are not counted down properly.\n\n<Executor name=\"tomcatThreadPool\" \n          namePrefix=\"tomcat-8080-\" \n          minSpareThreads=\"50\" \n          maxThreads=\"300\"/>\n\n<Connector port=\"8080\" \n           redirectPort=\"${bio.https.port}\"              \n           protocol=\"org.apache.coyote.http11.Http11Protocol\"\n           maxKeepAliveRequests=\"15\" \n           executor=\"tomcatThreadPool\" \n           connectionTimeout=\"20000\" \n           acceptCount=\"100\"/>\n\nThread dump yields\n\"http-bio-8080-Acceptor-0\" daemon prio=3 tid=0xXXXXXXXX nid=0xXX waiting on condition [0xXXXXXXXX..0xXXXXXXXX]\n   java.lang.Thread.State: WAITING (parking)\n\tat sun.misc.Unsafe.park(Native Method)\n\t- parking to wait for  <0xXXXXXXXX> (a org.apache.tomcat.util.threads.LimitLatch$Sync)\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:905)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1217)\n\tat org.apache.tomcat.util.threads.LimitLatch.countUpOrAwait(LimitLatch.java:99)\n\tat org.apache.tomcat.util.net.AbstractEndpoint.countUpOrAwaitConnection(AbstractEndpoint.java:660)\n\tat org.apache.tomcat.util.net.JIoEndpoint$Acceptor.run(JIoEndpoint.java:210)\n\tat java.lang.Thread.run(Thread.java:619)\n\nThis, as you may imagine, is a fairly hard use case to reproduce into a simple test case. The easiest way to reproduce it is to create the following configuration\n    <Executor name=\"tomcatThreadPool\" \n              namePrefix=\"catalina-exec-\"\n              maxThreads=\"5\" \n              minSpareThreads=\"0\" \n              maxQueueSize=\"15\"/>\n<Connector port=\"8080\" \n           protocol=\"HTTP/1.1\" executor=\"tomcatThreadPool\"\n           connectionTimeout=\"10000\"\n           redirectPort=\"8443\" \n           maxConnections=\"30\"/>\n\nThis reproduces one test case, where the state machine is not taking into account that connections may be rejected by the queue, but it doesn't count down the latch.\nI'm attaching a patch to fix this specific use case, but it may not be a complete fix. As a workaround, the patch also introduces the maxConnections=\"-1\" configuration that disables the usage of maxConnections. The -1 setting is important to give administrator a workaround while the other edge cases are tracked down.\n\n\nI have not been able to reproduce this error with NIO connector.\n\nThere is one more place in the JioEndpoint that requires handling of RejectedExecutionException in the \npublic boolean processSocketAsync(SocketWrapper<Socket> socket,SocketStatus status) \nThis is currently unhandled.", "id": 158711, "time": "2012-05-01T13:54:22Z", "bug_id": 53173, "creation_time": "2012-05-01T13:54:22Z", "is_private": false}, {"count": 1, "tags": [], "creator": "fhanik@apache.org", "is_private": false, "text": "Fixed in trunk r1333114\nFixed in 7.0.x r1333116\nWill be made available in Apache Tomcat 7.0.28 and onwards\n\nAdditional option added\nmaxConnection=-1 to disable connection counting", "id": 158752, "time": "2012-05-02T17:18:42Z", "bug_id": 53173, "creation_time": "2012-05-02T17:18:42Z", "attachment_id": null}, {"count": 2, "tags": [], "bug_id": 53173, "attachment_id": null, "text": "Adding documentation to track cases where this can happen\n\nMay 2, 2012 3:04:03 AM org.apache.tomcat.util.net.NioEndpoint$Acceptor run\nSEVERE: Socket accept failed\njava.io.IOException: Too many open files\n       at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\n       at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)\n       at org.apache.tomcat.util.net.NioEndpoint$Acceptor.run(NioEndpoint.java:784)\n       at java.lang.Thread.run(Thread.java:662)", "id": 158754, "time": "2012-05-02T19:42:36Z", "creator": "fhanik@apache.org", "creation_time": "2012-05-02T19:42:36Z", "is_private": false}, {"count": 3, "tags": [], "bug_id": 53173, "attachment_id": null, "id": 158880, "time": "2012-05-07T12:47:40Z", "creator": "fhanik@apache.org", "creation_time": "2012-05-07T12:47:40Z", "is_private": false, "text": "*** Bug 53186 has been marked as a duplicate of this bug. ***"}, {"count": 4, "tags": [], "bug_id": 53173, "attachment_id": null, "id": 159257, "time": "2012-05-23T15:38:45Z", "creator": "fhanik@apache.org", "creation_time": "2012-05-23T15:38:45Z", "is_private": false, "text": "Just for documentation purposes, found the root cause of this problem, and it all makes sense now\n\nhttp://svn.apache.org/viewvc/tomcat/tc7.0.x/trunk/java/org/apache/tomcat/util/net/NioEndpoint.java?r1=1127961&r2=1127962&\n\nin r1127962\n\nThis change counts up connection before it has been validated to be working. Prior to this change, count up only occurred if a connection was valid and added to the poller."}, {"count": 5, "tags": [], "creator": "brauckmann@dfn-cert.de", "is_private": false, "text": "Hi Filip.\n\nIs it possible that this bugfix did not completely solve the problem?\n\nWhen doing a load test I encountered a stuck tomcat 7.0.29 with all threads in socketRead0 and no thread handling the web application:\n\n   java.lang.Thread.State: RUNNABLE\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.read(SocketInputStream.java:150)\n        at java.net.SocketInputStream.read(SocketInputStream.java:121)\n        at org.apache.coyote.ajp.AjpProcessor.read(AjpProcessor.java:309)\n        at org.apache.coyote.ajp.AjpProcessor.readMessage(AjpProcessor.java:364)\n        at org.apache.coyote.ajp.AjpProcessor.process(AjpProcessor.java:128)\n        at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:585)\n        at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:310)\n        - locked <0x9e2b2098> (a org.apache.tomcat.util.net.SocketWrapper)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n\n\nThe Acceptor thread is stuck:\n\n\"ajp-bio-127.0.0.1-8009-Acceptor-0\" daemon prio=10 tid=0x6eaf2800 nid=0x18ee waiting on condition [0x6e15c000]\n   java.lang.Thread.State: WAITING (parking)\n        at sun.misc.Unsafe.park(Native Method)\n        - parking to wait for  <0xa23cf128> (a org.apache.tomcat.util.threads.LimitLatch$Sync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)\n        at org.apache.tomcat.util.threads.LimitLatch.countUpOrAwait(LimitLatch.java:115)\n        at org.apache.tomcat.util.net.AbstractEndpoint.countUpOrAwaitConnection(AbstractEndpoint.java:718)\n        at org.apache.tomcat.util.net.JIoEndpoint$Acceptor.run(JIoEndpoint.java:210)\n        at java.lang.Thread.run(Thread.java:722)\n\n\nThis looks exactly like the problem that this patch was supposed to fix.\n\nThe configuration: Apache with MaxClients 4000, mod_jk, tomcat 7.0.29 with a very simple Connector configuration (all thread and connection parameters left to default).\n\nTested with two different web applications that have nothing in common. The problem occurs only during heavy load.\n\nThe problem disappears when the Apache and the Tomcat parameters are adjusted so that MaxClients < maxThreads.", "id": 161862, "time": "2012-08-31T08:57:45Z", "bug_id": 53173, "creation_time": "2012-08-31T08:57:45Z", "attachment_id": null}, {"count": 6, "tags": [], "bug_id": 53173, "attachment_id": null, "is_private": false, "id": 161863, "time": "2012-08-31T10:04:34Z", "creator": "knst.kolinko@gmail.com", "creation_time": "2012-08-31T10:04:34Z", "text": "(In reply to comment #5)\n> Hi Filip.\n> \n> Is it possible that this bugfix did not completely solve the problem?\n> \n> When doing a load test I encountered a stuck tomcat 7.0.29 with all threads\n> in socketRead0 and no thread handling the web application:\n> \n>    java.lang.Thread.State: RUNNABLE\n>         at java.net.SocketInputStream.socketRead0(Native Method)\n>         at java.net.SocketInputStream.read(SocketInputStream.java:150)\n>         at java.net.SocketInputStream.read(SocketInputStream.java:121)\n>         at org.apache.coyote.ajp.AjpProcessor.read(AjpProcessor.java:309)\n>         at\n> org.apache.coyote.ajp.AjpProcessor.readMessage(AjpProcessor.java:364)\n\nThis one is busy reading data on an existing connection. (It is between requests, so no web application classes are mentioned in the stack trace).\n\nIt is reading a socket. It cannot serve requests on other sockets. It cannot be used for new requests.\n\n> The Acceptor thread is stuck:\n> \n> \"ajp-bio-127.0.0.1-8009-Acceptor-0\" daemon prio=10 tid=0x6eaf2800 nid=0x18ee\n> waiting on condition [0x6e15c000]\n>    java.lang.Thread.State: WAITING (parking)\n>         at sun.misc.Unsafe.park(Native Method)\n>         - parking to wait for  <0xa23cf128> (a\n> org.apache.tomcat.util.threads.LimitLatch$Sync)\n>         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n\nThis one, yes, is waiting on the counter and would not accept any more request until the counter goes down.\n\n> with a very simple Connector configuration (all thread and connection\n> parameters left to default).\n\n> org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:310)\n\nYou are using the BIO connector implementation.\n\nWhich means (according to the configuration reference for AJP connectors):\nmaxThreads = 200\nmaxConnections = maxThreads\n\nSo if you have 200 working threads and all are busy \"reading\" (waiting for data on existing socket),  then it is by design. You are not able to start 201th thread, so there is no point in accepting the 201th connection.\n\n\nAnyway, as was written earlier,\n> Additional option added\n> maxConnection=-1 to disable connection counting"}, {"count": 7, "tags": [], "bug_id": 53173, "attachment_id": null, "id": 161917, "time": "2012-09-03T07:42:41Z", "creator": "brauckmann@dfn-cert.de", "creation_time": "2012-09-03T07:42:41Z", "is_private": false, "text": "Thanks for explaining."}, {"count": 8, "tags": [], "bug_id": 53173, "attachment_id": null, "is_private": false, "id": 171147, "time": "2013-11-08T18:41:04Z", "creator": "roberthardin@hotmail.com", "creation_time": "2013-11-08T18:41:04Z", "text": "(in reply to comments #5 and #6)\n\nThere still exists a defect in Tomcat 7.0.47 (most likely with the Executor of the connector threads) where, as described in brauckmann's comment #5, all threads in the Tomcat thread pool will begin hanging at peak traffic times. By \"peak\" I'm referring to complete thread pool saturation. The bug is highly recreatable when maxThreads<=maxConnections in config and the threads are executing some logic (jsp, servlet, etc.) with relatively extensive, timely operations (like reading from a database, file I/O, etc). Once the thread pool is exhausted and if the maxConnections continues to allow & queue new requests pending available threads, the Executor seems to start hanging the Tomcat threads themselves (somehow) - it is not the operation that the threads themselves are executing that's hanging the threads either; neither is it the number of connections still available (due to a connection count).\n\nIt was stated in comment #5 that \"The problem disappears when the Apache and the Tomcat parameters are adjusted so that MaxClients < maxThreads\" and even the sample configuration suggested in the very first comment from Filip that allows for recreation of the bug suggests a bug manifestation when the number of socket connections allowed by Tomcat exceed the available number of threads to handle incoming requests:\n\n    <Executor name=\"tomcatThreadPool\" \n              namePrefix=\"catalina-exec-\"\n              maxThreads=\"5\" \n              minSpareThreads=\"0\" \n              maxQueueSize=\"15\"/>\n<Connector port=\"8080\" \n           protocol=\"HTTP/1.1\" executor=\"tomcatThreadPool\"\n           connectionTimeout=\"10000\"\n           redirectPort=\"8443\" \n           maxConnections=\"30\"/>\n\nThis is why adding maxConnections=-1 as an option (effectively putting no upper-limit to accepting client socket connections) actually just makes matters worse. After changing my configuration (where I had previously just specified maxThreads=200) to maxThreads=200 and maxConnections=-1, and by running a simple HTTP grinder against a small, test web application on Tomcat I had brought all 200 connector threads to their knees in a matter of seconds (Tomcat completely unresponsive until Catalina Tomcat was bounced). These threads never recovered and the back end processes that the web app started from the thread were all also hung (until the Tomcat server restart).\n\nTo recreate, simply write a simple webapp containing a jsp to execute system process 'netstat -a' and route the process stdout to the http response. Then from an exerciser client, spin up something like 500 threads each continually submitting HTTP gets against the Tomcat server (web app jsp) every 1/2 second or so.\n\nThe only way I was able to keep Tomcat server thread pool healthy and the server responsive was to make sure maxThreads>maxConnections. Setting maxConnections=-1, or maxConnections>=maxThreads all result in hung connector threads and ultimately an unresponsive Tomcat server on that connector.\n\nBest,\nRobert"}, {"count": 9, "tags": [], "bug_id": 53173, "attachment_id": null, "is_private": false, "id": 171148, "time": "2013-11-08T19:32:16Z", "creator": "fhanik@apache.org", "creation_time": "2013-11-08T19:32:16Z", "text": "A work around for you should be using the NIO connector, as it doesn't use threads in between keep alives, and don't set any max connections\n\n<Connector port=\"8080\" \n           protocol=\"org.apache.coyote.http11.Http11NioProtocol\" ..."}, {"count": 10, "tags": [], "bug_id": 53173, "attachment_id": null, "id": 171149, "time": "2013-11-08T20:25:12Z", "creator": "roberthardin@hotmail.com", "creation_time": "2013-11-08T20:25:12Z", "is_private": false, "text": "Thank you Filip. I'll give it a try.\n\nWould I continue to use the maxThreads parameter to throttle traffic (to prevent client requests from burning up my JVM)? If not, how would I go about configuring an NIO connector to max out at ~200 simultaneous request fulfillments?\n\nAre there any known down-sides to continuing use of BIO with a maxThreads>maxConnections relationship?\n\nThanks"}, {"count": 11, "tags": [], "bug_id": 53173, "attachment_id": null, "text": "Restoring the original Version (7.0.27) and Hardware field values.\n\n(In reply to Robert Hardin from comment #8)\n> the Executor seems to\n> start hanging the Tomcat threads themselves (somehow)\n\nTake a Thread dump. Show it.\n\n> Are there any known down-sides\n\nAsk on the mailing list. Bugzilla is not a support forum.", "id": 171151, "time": "2013-11-09T03:56:51Z", "creator": "knst.kolinko@gmail.com", "creation_time": "2013-11-09T03:56:51Z", "is_private": false}]