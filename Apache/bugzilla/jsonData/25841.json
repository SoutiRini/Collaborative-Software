[{"count": 0, "tags": [], "text": "We run Tomcat 4.1.24 out of JBoss 3.2.1.  We have observed that under load, \ntomcat increasing numbers of tomcat threads are \"stuck\" in a jk conversaton \nwith apache (thread dump below).  As max processors is approached and reached, \nthis evidently results in decreasing performance for users as they have to wait \nfor workers to be available.  Eventually, no response is possible when the \nnumber of threads in this state is reached and the servers have to be re-\nstarted.  \n\nThe doesn't seem to appear except under load - which leaves us to believe that \nits related to max processors.  We have changed max processors from 75 to 100 \nto 150.  It takes increasing amounts of concurrency to cause the problem.  \nIncreasing max processors infinitely is not a solution because we want to \nthrottle requests through the appserver using the acceptcount/maxprocessor \ncombination.\n\nThe thread dump for a typical thread in this condition probably doesn't tell \nyou much because it appears to be a perfectly normal situation (until you \nrealize that all the threads are \"stuck\" in this state:\n\n\"Thread-334\" daemon prio=5 tid=0x19e33c8 nid=0x835 runnable [9ae81000..9ae81994]\n\tat java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:129)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:183)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:222)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:277)\n\t- locked <d170f5f8> (a java.io.BufferedInputStream)\n\tat org.apache.jk.common.ChannelSocket.read(ChannelSocket.java:498)\n\tat org.apache.jk.common.ChannelSocket.receive(ChannelSocket.java:436)\n\tat org.apache.jk.common.ChannelSocket.processConnection\n(ChannelSocket.java:551)\n\tat org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:679)\n\tat org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run\n(ThreadPool.java:619)\n\tat java.lang.Thread.run(Thread.java:536)\n\n\nThe connector configuration is:\n\n            <Connector className=\"org.apache.coyote.tomcat4.CoyoteConnector\"\n               port=\"9011\" minProcessors=\"150\" maxProcessors=\"150\"\n               enableLookups=\"false\"\n               acceptCount=\"50\" debug=\"0\" connectionTimeout=\"60000\"\n               useURIValidationHack=\"false\"\n               protocolHandlerClassName=\"org.apache.jk.server.JkCoyoteHandler\"/>\n\nA typical apache side worker.properties configuration is:\n\nworker.syn1.port=9011\nworker.syn1.host=prodapp01.parago.com\nworker.syn1.type=ajp13\nworker.syn1.cachesize=150\nworker.syn1.cache_timeout=600\nworker.syn1.lbfactor=100", "attachment_id": null, "id": 49909, "creator": "franz@franzzemen.com", "time": "2003-12-31T17:06:05Z", "bug_id": 25841, "creation_time": "2003-12-31T17:06:05Z", "is_private": false}, {"count": 1, "tags": [], "creator": "glenn@apache.org", "attachment_id": null, "id": 50098, "time": "2004-01-05T20:16:25Z", "bug_id": 25841, "creation_time": "2004-01-05T20:16:25Z", "is_private": false, "text": "There is a 1 <-> mapping between apache httpd processes and tomcat Processors.\nOnce an httpd process has had to forward a request to Tomcat the connection\nit establishes to Tomcat is maintained until that httpd process dies, even\nif that httpd process never has to forward another request to Tomcat.\n\nSo if apache is serving alot of normal http request which don't require Tomcat\nyou end up with a large number of idle connections to Tomcat which use Processors.\n\nIf you are running on unix and can use Apache 2 with the threaded worker MPM I\nrecommend upgrading to that.  This configuration makes much more efficient use\nof the connections between Tomcat and apache because multiple request handling\nthreads in apache can share the same pool of connections to Tomcat.\n\nThe stack trace you show just indicates that most of your AJP Processors are\nidle.\n\nThis is not a bug."}]